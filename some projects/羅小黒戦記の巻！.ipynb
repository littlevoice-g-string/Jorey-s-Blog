{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mecab-python3\n",
    "# !pip install unidic-lite\n",
    "# !pip install nltk\n",
    "# !pip install scikit-learn\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install BeautifulSoup4\n",
    "# !pip install neologdn\n",
    "# !pip install stylecloud\n",
    "# !pip install scrapy\n",
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xlwt\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib import request\n",
    "import re\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.cnblogs.com/yoyoketang/p/6557421.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "workbook = xlwt.Workbook()\n",
    "worksheet = workbook.add_sheet('filmarks',cell_overwrite_ok = True)\n",
    "\n",
    "worksheet.write(0,0,label='レビュアー')\n",
    "worksheet.write(0,1,label='時間')\n",
    "worksheet.write(0,2,label='内容')\n",
    "\n",
    "workbook.save('filmarks.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ①クロージング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "browser.get('https://filmarks.com/movies/86613')\n",
    "time.sleep(3)\n",
    "\n",
    "#https://blog.csdn.net/zhangvalue/article/details/102921631\n",
    "links = []\n",
    "\n",
    "for link in browser.find_elements_by_xpath(\"//*[@href]\"):\n",
    "    if \"page\" in str(link.get_attribute('href')):\n",
    "        links.append(link.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'303'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = links[-1].split(\"page=\")[-1]\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "browser.get('https://filmarks.com/movies/86613?page=298')\n",
    "# time.sleep(3)\n",
    "user_list = []\n",
    "condition_list = []\n",
    "for test in browser.find_elements_by_tag_name('img'):\n",
    "    user_list.append(test.get_attribute('alt'))\n",
    "    condition_list .append(test.get_attribute('loading'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = user_list[3:13]\n",
    "conditions = condition_list[3:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "scores = []\n",
    "users = []\n",
    "conditions = []\n",
    "\n",
    "for i in range(1,int(times)+1):\n",
    "    browser.get('https://filmarks.com/movies/86613?page='+str(i))\n",
    "    element_text = browser.find_elements('css selector','.p-mark__review')\n",
    "    element_score = browser.find_elements('css selector','.c-rating__score')\n",
    "    text = [m.text for m in element_text]\n",
    "    score = [l.text for l in element_score]\n",
    "    user = [n.get_attribute('alt') for n in browser.find_elements_by_tag_name('img')]\n",
    "    condition = [c.get_attribute('loading') for c in browser.find_elements_by_tag_name('img')]\n",
    "    texts.append(text)\n",
    "    scores.append(score[1:11])\n",
    "    users.append(user[3:13])\n",
    "    conditions.append(condition[3:13])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3030\n",
      "3030\n",
      "3030\n",
      "3030\n",
      "303\n",
      "303\n",
      "303\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "print(len(sum(texts,[])))\n",
    "print(len(sum(users,[])))\n",
    "print(len(sum(scores,[])))\n",
    "print(len(sum(conditions,[])))\n",
    "print(len(texts))\n",
    "print(len(users))\n",
    "print(len(scores))\n",
    "print(len(conditions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df_datas = pd.DataFrame([]texts,columns =['text'])\n",
    "df_datas = pd.DataFrame([sum(users,[]),sum(texts,[]),sum(scores,[]),sum(conditions,[])],index =['users','texts','scores','conditions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datas.T.to_csv('luoxiaohei_datas.csv',encoding ='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>texts</th>\n",
       "      <th>scores</th>\n",
       "      <th>conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amy</td>\n",
       "      <td>シャオヘイが本当かわいい\\nどのキャラも魅力的\\n\\n全体的に絵が柔らかくて可愛い\\nアクシ...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>lazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>はる</td>\n",
       "      <td>絵が綺麗で、アクションも迫力あった！\\n轟音上映で観れてよかった！\\nそして何よりシャオヘイ...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>lazy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  users                                              texts scores conditions\n",
       "0   Amy  シャオヘイが本当かわいい\\nどのキャラも魅力的\\n\\n全体的に絵が柔らかくて可愛い\\nアクシ...    3.5       lazy\n",
       "1    はる  絵が綺麗で、アクションも迫力あった！\\n轟音上映で観れてよかった！\\nそして何よりシャオヘイ...    4.7       lazy"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_datas.T\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ②前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emojiの除去\n",
    "import neologdn\n",
    "import re\n",
    "def filter(desstr, restr=''):\n",
    "    \"\"\"\n",
    "    filter emoji\n",
    "    desstr: origin str\n",
    "    restr: replace str\n",
    "    \"\"\"\n",
    "    # filter emoji\n",
    "    res = re.compile(u'[\\U00010000-\\U0010ffff]')\n",
    "    res_emoji = res.sub(restr,desstr)\n",
    "    res_linebreak = ''.join(res_emoji.replace('<br/>','').split())\n",
    "    res_uppertolower = res_linebreak.lower()\n",
    "    res_normalization = neologdn.normalize(res_uppertolower)\n",
    "    res_number = re.sub('[①-⑨]', '',res_normalization)\n",
    "    res_url = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+','',res_number)\n",
    "        \n",
    "    return res_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ed = [filter(i) for i in df['texts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>texts</th>\n",
       "      <th>scores</th>\n",
       "      <th>conditions</th>\n",
       "      <th>texts_ed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amy</td>\n",
       "      <td>シャオヘイが本当かわいい\\nどのキャラも魅力的\\n\\n全体的に絵が柔らかくて可愛い\\nアクシ...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>lazy</td>\n",
       "      <td>シャオヘイが本当かわいいどのキャラも魅力的全体的に絵が柔らかくて可愛いアクションシーンはスピ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>はる</td>\n",
       "      <td>絵が綺麗で、アクションも迫力あった！\\n轟音上映で観れてよかった！\\nそして何よりシャオヘイ...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>lazy</td>\n",
       "      <td>絵が綺麗で、アクションも迫力あった!轟音上映で観れてよかった!そして何よりシャオヘイが可愛く...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  users                                              texts scores conditions  \\\n",
       "0   Amy  シャオヘイが本当かわいい\\nどのキャラも魅力的\\n\\n全体的に絵が柔らかくて可愛い\\nアクシ...    3.5       lazy   \n",
       "1    はる  絵が綺麗で、アクションも迫力あった！\\n轟音上映で観れてよかった！\\nそして何よりシャオヘイ...    4.7       lazy   \n",
       "\n",
       "                                            texts_ed  \n",
       "0  シャオヘイが本当かわいいどのキャラも魅力的全体的に絵が柔らかくて可愛いアクションシーンはスピ...  \n",
       "1  絵が綺麗で、アクションも迫力あった!轟音上映で観れてよかった!そして何よりシャオヘイが可愛く...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['texts_ed'] = text_ed\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分かち書き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#名詞、動詞、形容詞の抽出\n",
    "# 参考＜https://qiita.com/chamao/items/7edaba62b120a660657e＞\n",
    "def wakati_by_mecab(text):\n",
    "    tagger = MeCab.Tagger('')\n",
    "    tagger.parse('') \n",
    "    node = tagger.parseToNode(text)\n",
    "    word_list = []\n",
    "    while node:\n",
    "        pos = node.feature.split(\",\")[0]\n",
    "#         if pos in [\"形容詞\"]:   # 対象とする品詞\n",
    "\n",
    "        if pos in [\"名詞\", \"動詞\", \"形容詞\"]:   # 対象とする品詞\n",
    "            word = node.surface\n",
    "            word_list.append(word)\n",
    "        node = node.next\n",
    "    return \" \".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_wakati = [wakati_by_mecab(i) for i in text_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'絵 アクション 迫力 あっ 轟音 上映 観れ よかっ シャオヘイ 可愛く ムゲン かっこ いい'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_wakati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texts_wakati'] =texts_wakati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>texts</th>\n",
       "      <th>scores</th>\n",
       "      <th>conditions</th>\n",
       "      <th>texts_ed</th>\n",
       "      <th>texts_wakati</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amy</td>\n",
       "      <td>シャオヘイが本当かわいい\\nどのキャラも魅力的\\n\\n全体的に絵が柔らかくて可愛い\\nアクシ...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>lazy</td>\n",
       "      <td>シャオヘイが本当かわいいどのキャラも魅力的全体的に絵が柔らかくて可愛いアクションシーンはスピ...</td>\n",
       "      <td>シャオヘイ 本当 かわいい キャラ 魅力 全体 絵 柔らかく 可愛い アクション シーン ス...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>はる</td>\n",
       "      <td>絵が綺麗で、アクションも迫力あった！\\n轟音上映で観れてよかった！\\nそして何よりシャオヘイ...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>lazy</td>\n",
       "      <td>絵が綺麗で、アクションも迫力あった!轟音上映で観れてよかった!そして何よりシャオヘイが可愛く...</td>\n",
       "      <td>絵 アクション 迫力 あっ 轟音 上映 観れ よかっ シャオヘイ 可愛く ムゲン かっこ いい</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  users                                              texts scores conditions  \\\n",
       "0   Amy  シャオヘイが本当かわいい\\nどのキャラも魅力的\\n\\n全体的に絵が柔らかくて可愛い\\nアクシ...    3.5       lazy   \n",
       "1    はる  絵が綺麗で、アクションも迫力あった！\\n轟音上映で観れてよかった！\\nそして何よりシャオヘイ...    4.7       lazy   \n",
       "\n",
       "                                            texts_ed  \\\n",
       "0  シャオヘイが本当かわいいどのキャラも魅力的全体的に絵が柔らかくて可愛いアクションシーンはスピ...   \n",
       "1  絵が綺麗で、アクションも迫力あった!轟音上映で観れてよかった!そして何よりシャオヘイが可愛く...   \n",
       "\n",
       "                                        texts_wakati  \n",
       "0  シャオヘイ 本当 かわいい キャラ 魅力 全体 絵 柔らかく 可愛い アクション シーン ス...  \n",
       "1    絵 アクション 迫力 あっ 轟音 上映 観れ よかっ シャオヘイ 可愛く ムゲン かっこ いい  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 極性辞書の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_= pd.read_table('極性辞書.txt',sep =':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>優れる</th>\n",
       "      <th>すぐれる</th>\n",
       "      <th>動詞</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>良い</td>\n",
       "      <td>よい</td>\n",
       "      <td>形容詞</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>喜ぶ</td>\n",
       "      <td>よろこぶ</td>\n",
       "      <td>動詞</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>褒める</td>\n",
       "      <td>ほめる</td>\n",
       "      <td>動詞</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>めでたい</td>\n",
       "      <td>めでたい</td>\n",
       "      <td>形容詞</td>\n",
       "      <td>0.999645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>賢い</td>\n",
       "      <td>かしこい</td>\n",
       "      <td>形容詞</td>\n",
       "      <td>0.999486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55119</th>\n",
       "      <td>ない</td>\n",
       "      <td>ない</td>\n",
       "      <td>助動詞</td>\n",
       "      <td>-0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55120</th>\n",
       "      <td>酷い</td>\n",
       "      <td>ひどい</td>\n",
       "      <td>形容詞</td>\n",
       "      <td>-0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55121</th>\n",
       "      <td>病気</td>\n",
       "      <td>びょうき</td>\n",
       "      <td>名詞</td>\n",
       "      <td>-0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55122</th>\n",
       "      <td>死ぬ</td>\n",
       "      <td>しぬ</td>\n",
       "      <td>動詞</td>\n",
       "      <td>-0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55123</th>\n",
       "      <td>悪い</td>\n",
       "      <td>わるい</td>\n",
       "      <td>形容詞</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55124 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        優れる  すぐれる   動詞         1\n",
       "0        良い    よい  形容詞  0.999995\n",
       "1        喜ぶ  よろこぶ   動詞  0.999979\n",
       "2       褒める   ほめる   動詞  0.999979\n",
       "3      めでたい  めでたい  形容詞  0.999645\n",
       "4        賢い  かしこい  形容詞  0.999486\n",
       "...     ...   ...  ...       ...\n",
       "55119    ない    ない  助動詞 -0.999997\n",
       "55120    酷い   ひどい  形容詞 -0.999997\n",
       "55121    病気  びょうき   名詞 -0.999998\n",
       "55122    死ぬ    しぬ   動詞 -0.999999\n",
       "55123    悪い   わるい  形容詞 -1.000000\n",
       "\n",
       "[55124 rows x 4 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改行記号などの除去\n",
    "def linebreak(desstr):\n",
    "    return ''.join(desstr.replace('<br/>','').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed = [filter_emoji(i) for i in df['comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed = [linebreak(i) for i in texts_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#大文字を小文字に統一する\n",
    "def uppertolower(desstr):\n",
    "    return desstr.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed = [uppertolower(i) for i in texts_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正規化\n",
    "def normalization(desstr):\n",
    "    return neologdn.normalize(desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neologdn\n",
    "texts_ed = [normalization(i) for i in texts_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#記号系数字の除去\n",
    "def number(desstr):\n",
    "    return re.sub('[①-⑨]', '', desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed = [number(i) for i in texts_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urlの除去\n",
    "def url(desstr):\n",
    "    return re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+','',desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed = [url(i) for i in texts_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed[504]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ご参考まで："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = browser.find_elements('css selector','.p-mark__review') \n",
    "print(type(elements))\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(element[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_page = browser.find_elements('css selector','.c-pagination__prev') \n",
    "print(type(elements_page))\n",
    "print(elements_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elements_page[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n",
    "    対応方法：https://blog.csdn.net/qq_38486203/article/details/82852240\n",
    "    ＜重启也不行，把chromedriver.exe放到python脚本的文件夹下面，再执行试试＞→OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://filmarks.com/movies/86613/no_spoiler?page='\n",
    "# url_num = re.search('(.*?)page=[0-9]',url)\n",
    "html_data = []\n",
    "for i in range(1,175):\n",
    "#     url = get_page(url_basic)\n",
    "    resp = request.urlopen(url+str(i))\n",
    "    data = resp.read().decode('utf-8')\n",
    "    html_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考<https://www.cnblogs.com/cymwill/articles/7574479.html>\n",
    "pmark_review_lists = []\n",
    "pmark_user_lists = []\n",
    "pmark_score_lists = []\n",
    "for m in html_data:\n",
    "    soup = bs(m,'html.parser')\n",
    "    #コメント＜soup＞\n",
    "    pmark_review_list = soup.find_all('div','p-mark__review')\n",
    "    \n",
    "    #ユーザー＜soup＞\n",
    "    pmark_user_list = soup.find_all(lambda tag:tag.has_attr('alt') and tag.has_attr('loading'))\n",
    "    \n",
    "    #スコア＜正規＞\n",
    "    pmark_score_total = soup.find_all('div',class_='c-media__content')\n",
    "    pmark_score_total_list = []\n",
    "    for i in pmark_score_total:\n",
    "        pmark_score_total_list.append(re.findall('<div class=\"c-rating__score\">(.*)</div></div></div>',str(i)))\n",
    "\n",
    "    pmark_review_lists.append(pmark_review_list)\n",
    "    pmark_user_lists.append(pmark_user_list)\n",
    "    pmark_score_lists.append(pmark_score_total_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pmark_score_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考《https://blog.csdn.net/u012611644/article/details/82814377》\n",
    "import re\n",
    "\n",
    "comments = []\n",
    "for item in sum(pmark_review_lists,[]):\n",
    "    item_ed = re.findall('<div class=\"p-mark__review\">(.*)</div>',str(item))\n",
    "    comments.append(item_ed)\n",
    "\n",
    "users =[]\n",
    "for user in sum(pmark_user_lists,[]):\n",
    "    user_ed = re.findall('<img alt=\"(.*)\" height.*',str(user))\n",
    "    users.append(user_ed)\n",
    "    \n",
    "# scores = []\n",
    "# for score in sum(pmark_score_lists,[]):\n",
    "#     score_ed = re.findall('<div class=\"c-rating__score\">(.*)</div>',str(score))\n",
    "#     scores.append(score_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(users))\n",
    "print(len(comments))\n",
    "print(len(sum(pmark_score_lists,[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([sum(users,[]),sum(comments,[]),sum(sum(pmark_score_lists,[]),[])],\n",
    "                 index =['users','comments','scores']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Luoxiaohei.csv',encoding ='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scores_tmp = []\n",
    "for i in df['scores']:\n",
    "    if i != '-':\n",
    "        scores_tmp.append(float(i))\n",
    "<!-- #     else: -->\n",
    "        scores_tmp.append(i.replace('-','0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emojiの除去\n",
    "def filter_emoji(desstr, restr=''):\n",
    "    \"\"\"\n",
    "    filter emoji\n",
    "    desstr: origin str\n",
    "    restr: replace str\n",
    "    \"\"\"\n",
    "    # filter emoji\n",
    "    try:\n",
    "        res = re.compile(u'[\\U00010000-\\U0010ffff]')\n",
    "    except re.error:\n",
    "        res = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "    return res.sub(restr, desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed = [filter_emoji(i) for i in df['comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改行記号などの除去\n",
    "def linebreak(desstr):\n",
    "    return ''.join(desstr.replace('<br/>','').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#顔文字の除去\n",
    "def Emoticons(desstr):\n",
    "    return re.sub('amp|','',desstr)\n",
    "#     return re.sub('φωφ|д|ф|℃|○|〇|&amp|❓|❗','',desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_wakati = [Emoticons(i) for i in texts_wakati]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_wakati[754]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数字の除去\n",
    "def number(desstr):\n",
    "    return re.sub('\\d+', '', desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_wakati = [number(i) for i in texts_wakati]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_wakati[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords\n",
    "\n",
    "stopwords_list = ['あそこ','あたり','あちら','あっち','あと','あな','あなた','あれ','いくつ','いつ','いま','いや','いろいろ','うち','おおまか','おまえ','おれ','がい','かく','かたち','かやの','から','がら','きた','くせ','ここ','こっち','こと','ごと','こちら','ごっちゃ','これ','これら','ごろ','さまざま','さらい','さん','しかた','しよう','すか','ずつ','すね','すべて','ぜんぶ','そう','そこ','そちら','そっち','そで','それ','それぞれ','それなり','たくさん','たち','たび','ため','だめ','ちゃ','ちゃん','てん','とおり','とき','どこ','どこか','ところ','どちら','どっか','どっち','どれ','なか','なかば','なに','など','なん','はじめ','はず','はるか','ひと','ひとつ','ふく','ぶり','べつ','へん','ぺん','ほう','ほか','まさ','まし','まとも','まま','みたい','みつ','みなさん','みんな','もと','もの','もん','やつ','よう','よそ','わけ','わたし','','ハイ','','','上','中','下','字','','','年','月','日','時','分','秒','週','火','水','木','金','土','国','都','道','府','県','市','区','町','村','','','各','第','方','何','的','度','文','者','性','体','人','他','今','部','課','係','外','類','達','気','室','口','誰','用','界','会','首','男','女','別','話','私','屋','店','家','場','等','見','際','観','段','略','例','系','論','形','間','地','員','線','点','書','品','力','法','感','作','元','手','数','彼','彼女','子','内','楽','喜','怒','哀','輪','頃','化','境','俺','奴','高','校','婦','伸','紀','誌','レ','行','列','事','士','台','集','様','所','歴','器','名','情','連','毎','式','簿','','','','','回','匹','個','席','束','歳','目','通','面','円','玉','枚','','前','後','左','右','次','先','','春','夏','秋','冬','','','','一','二','三','四','五','六','七','八','九','十','百','千','万','億','兆','','','下記','上記','時間','今回','前回','場合','一つ','年生','自分','ヶ所','ヵ所','カ所','箇所','ヶ月','ヵ月','カ月','箇月','名前','本当','確か','時点','全部','関係','近く','方法','我々','違い','多く','扱い','新た','その後','半ば','結局','様々','以前','以後','以降','未満','以上','以下','幾つ','毎日','自体','向こう','何人','手段','同じ','感じ']\n",
    "\n",
    "def stopwords(desstr,stopwords):\n",
    "    stoplist_removed_documents = []\n",
    "    for i in desstr.split():\n",
    "        if i not in stopwords:\n",
    "            stoplist_removed_documents.append(i)\n",
    "    return stoplist_removed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwordsを除去\n",
    "texts_wakati_cleaned = [stopwords(i,stopwords_list) for i in texts_wakati]\n",
    "\n",
    "texts_wakati_cleaned_tmp = [' '.join(i) for i in texts_wakati_cleaned] \n",
    "\n",
    "#分かち書き列を追加\n",
    "df['text_wakati'] = texts_wakati_cleaned_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "print(df.isnull().any())\n",
    "\n",
    "df_null = pd.isnull(df)\n",
    "df_null = df[df == True]\n",
    "print(df_null)\n",
    "\n",
    "df = df.fillna('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('羅小黒戦記.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: red; \">感情分析</span>\n",
    "    スコアと内容の乖離とのことがかるかもしれないため\n",
    "    点数が低いというより、そもそも点数つけていないユーザーが多い\n",
    "    \n",
    "    ①極性辞書を元に感情分析を行う\n",
    "    ②機械学習を元に感情分析を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ①機械学習を元に感情分析を行う\n",
    "    参考＜https://blog.csdn.net/u011001084/article/details/78980299＞\n",
    "    <https://www.jianshu.com/p/29aa3ad63f9d>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label付与\n",
    "# def make_label(df):\n",
    "#     df['sentiment'] = df['scores'].apply(lambda x:1 if float(x)>3.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.scores == '-','sentiment'] = 2\n",
    "df.loc[(df.scores <= '3.5')&(df.scores != '-'),'sentiment'] = 0\n",
    "df.loc[df.scores > '3.5','sentiment'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].replace('0',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df.sentiment == 2]))\n",
    "print(len(df[df.sentiment == 1]))\n",
    "print(len(df[df.sentiment == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sentiment == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df.sentiment != 2]\n",
    "df_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[['text_wakati']]\n",
    "Y = df_train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=1/3, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,Y_train.shape,X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = 0.8\n",
    "min_df = 3\n",
    "\n",
    "vect = CountVectorizer(max_df= max_df,\n",
    "                       min_df=min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix = pd.DataFrame(vect.fit_transform(X_train.text_wakati).toarray(), columns=vect.get_feature_names())\n",
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB(alpha =1.0,class_prior = None,fit_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(vect, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(pipe,X_train.text_wakati, Y_train, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train.text_wakati, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pipe.predict(X_test.text_wakati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['pred'] = Y_pred.tolist()\n",
    "X_test[X_test['pred'] ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df[['sentiment','comments','scores']].loc[X_test.index.tolist()]\n",
    "X_test['true'] = df_tmp['sentiment']\n",
    "X_test['comments'] = df_tmp['comments']\n",
    "X_test['scores'] = df_tmp['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test[X_test['pred'] != X_test['true']]))\n",
    "X_test[X_test['pred'] != X_test['true']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "roc_auc = accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_pred)\n",
    "\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC(area = %0.2f)' % (roc_auc))\n",
    "plt.xlabel(\"FPR (False Positive Rate)\")\n",
    "plt.ylabel(\"TPR (True Positive Rate)\")\n",
    "plt.title(\"Receiver Operating Characteristic, ROC(AUC = %0.2f)\"% (roc_auc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(CM,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_michi = df.loc[df.scores == '-'][['text_wakati']]\n",
    "Y_pred_michi = pipe.predict(X_pred_michi.text_wakati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_michi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tmp_2 = df.loc[df.scores == '-']\n",
    "df_tmp_2['pred']=Y_pred_michi.tolist()\n",
    "df_tmp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_good= str(''.join(df[df.scores >'4'].text_wakati))\n",
    "str_bad= str(''.join(df[df.scores <'3'].text_wakati))\n",
    "str_normal= str(''.join(df[(df.scores <='4')&(df.scores>='3')].text_wakati))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "frequency_good = collections.Counter(str_good.split())\n",
    "frequency_good.most_common(200)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_bad = collections.Counter(str_bad.split())\n",
    "frequency_bad.most_common(200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_normal = collections.Counter(str_normal.split())\n",
    "frequency_normal.most_common(200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_good = []\n",
    "for k,v in frequency_good.most_common(200):\n",
    "    words_good.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "font = r'C:\\Users\\jorey\\python\\UDDigiKyokashoN-R.ttc'\n",
    "stopwords =set(['映画','アニメ','シャオヘイ','し','いう','い','ある','いる','ない','さ','いい','する'])\n",
    "\n",
    "wc = WordCloud(background_color = 'black',\n",
    "               font_path =font,\n",
    "               width = 1000,\n",
    "               height = 800,\n",
    "               stopwords=stopwords\n",
    "               ).generate(str(words_good))\n",
    "plt.imshow(wc)\n",
    "wc.to_file('good.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1)すべての単語を特徴にする\n",
    "def bag_of_words(words):  \n",
    "    return dict([(word, True) for word in words]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.scores <= '3.5') & (df.scores !='-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.scores > '3.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_words_good =bag_of_words(' '.join(df[df.scores > '3.5'].text_wakati.values).split())\n",
    "feature_words_bad =bag_of_words(' '.join(df[(df.scores <= '3.5') & (df.scores !='-')].text_wakati.values).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"👍の評価:\" + str(len(feature_words_good)))\n",
    "\n",
    "print(\"========================================================================\")\n",
    "\n",
    "print(\"👎の評価:\" + str(len(feature_words_bad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_words_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2)2つの単語のペア（bigrams)を特徴にする\n",
    "import nltk  \n",
    "from nltk.collocations import BigramCollocationFinder  \n",
    "from nltk.metrics import BigramAssocMeasures  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(words, score_fn=BigramAssocMeasures.chi_sq, n=1000):  \n",
    "    bigram_finder = BigramCollocationFinder.from_words(words) #単語ペアの作成  \n",
    "    bigrams = bigram_finder.nbest(score_fn, n) #カイ二乗、スコア上位1000の単語ペアをセレクト \n",
    "    newBigrams = [u+ ' ' + v for (u,v) in bigrams]\n",
    "    return bag_of_words(newBigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggram_words_good = bigram(feature_words_good)\n",
    "biggram_words_bad = bigram(feature_words_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"👍の評価:\" + str(biggram_words_good))\n",
    "\n",
    "print(\"========================================================================\")\n",
    "\n",
    "print(\"👎の評価:\" + str(biggram_words_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_words(words,score_fn=BigramAssocMeasures.chi_sq,n=1000):\n",
    "    bigram_finder=BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn,n)\n",
    "    newBigrams = [u + ' ' + v for (u,v) in bigrams]\n",
    "    a = bag_of_words(words)\n",
    "    b = bag_of_words(newBigrams)\n",
    "    a.update(b) \n",
    "    return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggram_good = bigram_words(feature_words_good)\n",
    "biggram_bad = bigram_words(feature_words_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"👍の評価:\" + str(biggram_good))\n",
    "\n",
    "print(\"========================================================================\")\n",
    "\n",
    "print(\"👎の評価:\" + str(biggram_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3)コーパス内単語ごとの情報量を計算する\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist  \n",
    "import itertools\n",
    "def create_word_scores():\n",
    "    #辞書の形をリストに整理する\n",
    "    poswords = list(itertools.chain(feature_words_good))\n",
    "    negwords = list(itertools.chain(feature_words_bad))\n",
    "    \n",
    "    #単語の頻度を統計する\n",
    "    word_fd = FreqDist()\n",
    "    cond_word_fd = ConditionalFreqDist()\n",
    "    for word in poswords:\n",
    "        word_fd[word] += 1  \n",
    "        cond_word_fd['pos'][word] += 1\n",
    "    for word in negwords:\n",
    "        word_fd[word] += 1\n",
    "        cond_word_fd['neg'][word] +=1  \n",
    "    \n",
    "    #ポジとネガ単語数のカウント\n",
    "    pos_word_count = cond_word_fd['pos'].N()\n",
    "    neg_word_count = cond_word_fd['neg'].N() \n",
    "    total_word_count = pos_word_count + neg_word_count  \n",
    "    \n",
    "    word_scores = {}  \n",
    "    for word, freq in word_fd.items():  \n",
    "        pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word], (freq, pos_word_count), total_word_count) #计算积极词的卡方统计量，这里也可以计算互信息等其它统计量  \n",
    "        neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word], (freq, neg_word_count), total_word_count) #同理  \n",
    "        word_scores[word] = pos_score + neg_score #一个词的信息量等于积极卡方统计量加上消极卡方统计量  \n",
    "    \n",
    "    return word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_bigram_scores(): \n",
    "    posWords = list(itertools.chain(biggram_words_good))  \n",
    "    negWords = list(itertools.chain(biggram_words_bad))  \n",
    "\n",
    "    bigram_finder = BigramCollocationFinder.from_words(posWords)  \n",
    "    bigram_finder = BigramCollocationFinder.from_words(negWords)  \n",
    "    posBigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 5000)  \n",
    "    negBigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 5000)  \n",
    "\n",
    "    pos = posWords + posBigrams  \n",
    "    neg = negWords + negBigrams\n",
    "\n",
    "    word_fd = FreqDist()  \n",
    "    cond_word_fd = ConditionalFreqDist()  \n",
    "    for word in pos:  \n",
    "        word_fd[word] += 1  \n",
    "        cond_word_fd['pos'][word] += 1 \n",
    "    for word in neg:  \n",
    "        word_fd[word] += 1\n",
    "        cond_word_fd['neg'][word] += 1  \n",
    "\n",
    "    pos_word_count = cond_word_fd['pos'].N()  \n",
    "    neg_word_count = cond_word_fd['neg'].N()  \n",
    "    total_word_count = pos_word_count + neg_word_count  \n",
    "\n",
    "    word_scores = {}  \n",
    "    for word, freq in word_fd.items():  \n",
    "        pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word], (freq, pos_word_count), total_word_count)  \n",
    "        neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word], (freq, neg_word_count), total_word_count)  \n",
    "        word_scores[word] = pos_score + neg_score  \n",
    "\n",
    "    return word_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_word_bigram_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(words_score,index = ['words','socre']).T.to_csv('words_score_xiaohei.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_words(word_scores, number):\n",
    "    best_vals = sorted(word_scores.items(),key = lambda w_s:w_s[1],reverse = True)[:number]\n",
    "#     best_vals = sorted(word_scores.iteritems(), key=lambda (w, s):(w,s), reverse=True)[:number]\n",
    "#     best_vals = sorted(word_scores.items(), key=lambda item:item[1],  reverse=True)[:number]\n",
    "    best_words = set([w for w, s in best_vals])\n",
    "    return best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores_1 = create_word_scores()  \n",
    "word_scores_2 = create_word_bigram_scores() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_words = find_best_words(word_scores_1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_word_features(words):  \n",
    "    #load_data()\n",
    "    #word_scores = create_word_bigram_scores()\n",
    "    global best_words\n",
    "    #best_words = find_best_words(word_scores,7500)\n",
    "    return dict([(word, True) for word in words if word in best_words])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_word_scores_1=best_word_features(word_scores_1)\n",
    "best_word_scores_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_words = find_best_words(word_scores_2,100)\n",
    "best_word_scores_2=best_word_features(word_scores_2)\n",
    "best_word_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_feature(feature_extraction_method):\n",
    "    posFeatures =[]\n",
    "    for i in pos_review:\n",
    "        posWords = [feature_extraction_method(i),'pos']\n",
    "        posFeatures.append(posWords)\n",
    "    return posFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 廃止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmark_dict = dict(zip(sum(users,[]),sum(comments,[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_tmp = pd.DataFrame(pmark_dict,index=['comments']).T\n",
    "\n",
    "df = df_tmp.reset_index().rename(columns={'index':'user'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(scores))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comments'][0] \n",
    "#絵文字\n",
    "#改行など\n",
    "#感情溢れすぎ表現\n",
    "\n",
    "texts_ed[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
