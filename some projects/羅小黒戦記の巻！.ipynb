{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mecab-python3\n",
    "# !pip install unidic-lite\n",
    "# !pip install nltk\n",
    "# !pip install scikit-learn\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install BeautifulSoup4\n",
    "# !pip install neologdn\n",
    "# !pip install stylecloud\n",
    "# !pip install scrapy\n",
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xlwt\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib import request\n",
    "import re\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.cnblogs.com/yoyoketang/p/6557421.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "workbook = xlwt.Workbook()\n",
    "worksheet = workbook.add_sheet('filmarks',cell_overwrite_ok = True)\n",
    "\n",
    "worksheet.write(0,0,label='ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼')\n",
    "worksheet.write(0,1,label='æ™‚é–“')\n",
    "worksheet.write(0,2,label='å†…å®¹')\n",
    "\n",
    "workbook.save('filmarks.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â‘ ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "browser.get('https://filmarks.com/movies/86613')\n",
    "time.sleep(3)\n",
    "\n",
    "#https://blog.csdn.net/zhangvalue/article/details/102921631\n",
    "links = []\n",
    "\n",
    "for link in browser.find_elements_by_xpath(\"//*[@href]\"):\n",
    "    if \"page\" in str(link.get_attribute('href')):\n",
    "        links.append(link.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'303'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = links[-1].split(\"page=\")[-1]\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "browser.get('https://filmarks.com/movies/86613?page=298')\n",
    "# time.sleep(3)\n",
    "user_list = []\n",
    "condition_list = []\n",
    "for test in browser.find_elements_by_tag_name('img'):\n",
    "    user_list.append(test.get_attribute('alt'))\n",
    "    condition_list .append(test.get_attribute('loading'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = user_list[3:13]\n",
    "conditions = condition_list[3:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "scores = []\n",
    "users = []\n",
    "conditions = []\n",
    "\n",
    "for i in range(1,int(times)+1):\n",
    "    browser.get('https://filmarks.com/movies/86613?page='+str(i))\n",
    "    element_text = browser.find_elements('css selector','.p-mark__review')\n",
    "    element_score = browser.find_elements('css selector','.c-rating__score')\n",
    "    text = [m.text for m in element_text]\n",
    "    score = [l.text for l in element_score]\n",
    "    user = [n.get_attribute('alt') for n in browser.find_elements_by_tag_name('img')]\n",
    "    condition = [c.get_attribute('loading') for c in browser.find_elements_by_tag_name('img')]\n",
    "    texts.append(text)\n",
    "    scores.append(score[1:11])\n",
    "    users.append(user[3:13])\n",
    "    conditions.append(condition[3:13])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3030\n",
      "3030\n",
      "3030\n",
      "3030\n",
      "303\n",
      "303\n",
      "303\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "print(len(sum(texts,[])))\n",
    "print(len(sum(users,[])))\n",
    "print(len(sum(scores,[])))\n",
    "print(len(sum(conditions,[])))\n",
    "print(len(texts))\n",
    "print(len(users))\n",
    "print(len(scores))\n",
    "print(len(conditions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df_datas = pd.DataFrame([]texts,columns =['text'])\n",
    "df_datas = pd.DataFrame([sum(users,[]),sum(texts,[]),sum(scores,[]),sum(conditions,[])],index =['users','texts','scores','conditions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datas.T.to_csv('luoxiaohei_datas.csv',encoding ='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>texts</th>\n",
       "      <th>scores</th>\n",
       "      <th>conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amy</td>\n",
       "      <td>ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒæœ¬å½“ã‹ã‚ã„ã„\\nã©ã®ã‚­ãƒ£ãƒ©ã‚‚é­…åŠ›çš„\\n\\nå…¨ä½“çš„ã«çµµãŒæŸ”ã‚‰ã‹ãã¦å¯æ„›ã„\\nã‚¢ã‚¯ã‚·...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>lazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ã¯ã‚‹</td>\n",
       "      <td>çµµãŒç¶ºéº—ã§ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è¿«åŠ›ã‚ã£ãŸï¼\\nè½ŸéŸ³ä¸Šæ˜ ã§è¦³ã‚Œã¦ã‚ˆã‹ã£ãŸï¼\\nãã—ã¦ä½•ã‚ˆã‚Šã‚·ãƒ£ã‚ªãƒ˜ã‚¤...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>lazy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  users                                              texts scores conditions\n",
       "0   Amy  ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒæœ¬å½“ã‹ã‚ã„ã„\\nã©ã®ã‚­ãƒ£ãƒ©ã‚‚é­…åŠ›çš„\\n\\nå…¨ä½“çš„ã«çµµãŒæŸ”ã‚‰ã‹ãã¦å¯æ„›ã„\\nã‚¢ã‚¯ã‚·...    3.5       lazy\n",
       "1    ã¯ã‚‹  çµµãŒç¶ºéº—ã§ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è¿«åŠ›ã‚ã£ãŸï¼\\nè½ŸéŸ³ä¸Šæ˜ ã§è¦³ã‚Œã¦ã‚ˆã‹ã£ãŸï¼\\nãã—ã¦ä½•ã‚ˆã‚Šã‚·ãƒ£ã‚ªãƒ˜ã‚¤...    4.7       lazy"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_datas.T\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â‘¡å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emojiã®é™¤å»\n",
    "import neologdn\n",
    "import re\n",
    "def filter(desstr, restr=''):\n",
    "    \"\"\"\n",
    "    filter emoji\n",
    "    desstr: origin str\n",
    "    restr: replace str\n",
    "    \"\"\"\n",
    "    # filter emoji\n",
    "    res = re.compile(u'[\\U00010000-\\U0010ffff]')\n",
    "    res_emoji = res.sub(restr,desstr)\n",
    "    res_linebreak = ''.join(res_emoji.replace('<br/>','').split())\n",
    "    res_uppertolower = res_linebreak.lower()\n",
    "    res_normalization = neologdn.normalize(res_uppertolower)\n",
    "    res_number = re.sub('[â‘ -â‘¨]', '',res_normalization)\n",
    "    res_url = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+','',res_number)\n",
    "        \n",
    "    return res_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ed = [filter(i) for i in df['texts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>texts</th>\n",
       "      <th>scores</th>\n",
       "      <th>conditions</th>\n",
       "      <th>texts_ed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amy</td>\n",
       "      <td>ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒæœ¬å½“ã‹ã‚ã„ã„\\nã©ã®ã‚­ãƒ£ãƒ©ã‚‚é­…åŠ›çš„\\n\\nå…¨ä½“çš„ã«çµµãŒæŸ”ã‚‰ã‹ãã¦å¯æ„›ã„\\nã‚¢ã‚¯ã‚·...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>lazy</td>\n",
       "      <td>ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒæœ¬å½“ã‹ã‚ã„ã„ã©ã®ã‚­ãƒ£ãƒ©ã‚‚é­…åŠ›çš„å…¨ä½“çš„ã«çµµãŒæŸ”ã‚‰ã‹ãã¦å¯æ„›ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚·ãƒ¼ãƒ³ã¯ã‚¹ãƒ”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ã¯ã‚‹</td>\n",
       "      <td>çµµãŒç¶ºéº—ã§ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è¿«åŠ›ã‚ã£ãŸï¼\\nè½ŸéŸ³ä¸Šæ˜ ã§è¦³ã‚Œã¦ã‚ˆã‹ã£ãŸï¼\\nãã—ã¦ä½•ã‚ˆã‚Šã‚·ãƒ£ã‚ªãƒ˜ã‚¤...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>lazy</td>\n",
       "      <td>çµµãŒç¶ºéº—ã§ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è¿«åŠ›ã‚ã£ãŸ!è½ŸéŸ³ä¸Šæ˜ ã§è¦³ã‚Œã¦ã‚ˆã‹ã£ãŸ!ãã—ã¦ä½•ã‚ˆã‚Šã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒå¯æ„›ã...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  users                                              texts scores conditions  \\\n",
       "0   Amy  ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒæœ¬å½“ã‹ã‚ã„ã„\\nã©ã®ã‚­ãƒ£ãƒ©ã‚‚é­…åŠ›çš„\\n\\nå…¨ä½“çš„ã«çµµãŒæŸ”ã‚‰ã‹ãã¦å¯æ„›ã„\\nã‚¢ã‚¯ã‚·...    3.5       lazy   \n",
       "1    ã¯ã‚‹  çµµãŒç¶ºéº—ã§ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è¿«åŠ›ã‚ã£ãŸï¼\\nè½ŸéŸ³ä¸Šæ˜ ã§è¦³ã‚Œã¦ã‚ˆã‹ã£ãŸï¼\\nãã—ã¦ä½•ã‚ˆã‚Šã‚·ãƒ£ã‚ªãƒ˜ã‚¤...    4.7       lazy   \n",
       "\n",
       "                                            texts_ed  \n",
       "0  ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒæœ¬å½“ã‹ã‚ã„ã„ã©ã®ã‚­ãƒ£ãƒ©ã‚‚é­…åŠ›çš„å…¨ä½“çš„ã«çµµãŒæŸ”ã‚‰ã‹ãã¦å¯æ„›ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚·ãƒ¼ãƒ³ã¯ã‚¹ãƒ”...  \n",
       "1  çµµãŒç¶ºéº—ã§ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è¿«åŠ›ã‚ã£ãŸ!è½ŸéŸ³ä¸Šæ˜ ã§è¦³ã‚Œã¦ã‚ˆã‹ã£ãŸ!ãã—ã¦ä½•ã‚ˆã‚Šã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒå¯æ„›ã...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['texts_ed'] = text_ed\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†ã‹ã¡æ›¸ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åè©ã€å‹•è©ã€å½¢å®¹è©ã®æŠ½å‡º\n",
    "# å‚è€ƒï¼œhttps://qiita.com/chamao/items/7edaba62b120a660657eï¼\n",
    "def wakati_by_mecab(text):\n",
    "    tagger = MeCab.Tagger('')\n",
    "    tagger.parse('') \n",
    "    node = tagger.parseToNode(text)\n",
    "    word_list = []\n",
    "    while node:\n",
    "        pos = node.feature.split(\",\")[0]\n",
    "#         if pos in [\"å½¢å®¹è©\"]:   # å¯¾è±¡ã¨ã™ã‚‹å“è©\n",
    "\n",
    "        if pos in [\"åè©\", \"å‹•è©\", \"å½¢å®¹è©\"]:   # å¯¾è±¡ã¨ã™ã‚‹å“è©\n",
    "            word = node.surface\n",
    "            word_list.append(word)\n",
    "        node = node.next\n",
    "    return \" \".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_wakati = [wakati_by_mecab(i) for i in text_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'çµµ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ è¿«åŠ› ã‚ã£ è½ŸéŸ³ ä¸Šæ˜  è¦³ã‚Œ ã‚ˆã‹ã£ ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ å¯æ„›ã ãƒ ã‚²ãƒ³ ã‹ã£ã“ ã„ã„'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_wakati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texts_wakati'] =texts_wakati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>texts</th>\n",
       "      <th>scores</th>\n",
       "      <th>conditions</th>\n",
       "      <th>texts_ed</th>\n",
       "      <th>texts_wakati</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amy</td>\n",
       "      <td>ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒæœ¬å½“ã‹ã‚ã„ã„\\nã©ã®ã‚­ãƒ£ãƒ©ã‚‚é­…åŠ›çš„\\n\\nå…¨ä½“çš„ã«çµµãŒæŸ”ã‚‰ã‹ãã¦å¯æ„›ã„\\nã‚¢ã‚¯ã‚·...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>lazy</td>\n",
       "      <td>ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒæœ¬å½“ã‹ã‚ã„ã„ã©ã®ã‚­ãƒ£ãƒ©ã‚‚é­…åŠ›çš„å…¨ä½“çš„ã«çµµãŒæŸ”ã‚‰ã‹ãã¦å¯æ„›ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚·ãƒ¼ãƒ³ã¯ã‚¹ãƒ”...</td>\n",
       "      <td>ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ æœ¬å½“ ã‹ã‚ã„ã„ ã‚­ãƒ£ãƒ© é­…åŠ› å…¨ä½“ çµµ æŸ”ã‚‰ã‹ã å¯æ„›ã„ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ã‚·ãƒ¼ãƒ³ ã‚¹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ã¯ã‚‹</td>\n",
       "      <td>çµµãŒç¶ºéº—ã§ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è¿«åŠ›ã‚ã£ãŸï¼\\nè½ŸéŸ³ä¸Šæ˜ ã§è¦³ã‚Œã¦ã‚ˆã‹ã£ãŸï¼\\nãã—ã¦ä½•ã‚ˆã‚Šã‚·ãƒ£ã‚ªãƒ˜ã‚¤...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>lazy</td>\n",
       "      <td>çµµãŒç¶ºéº—ã§ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è¿«åŠ›ã‚ã£ãŸ!è½ŸéŸ³ä¸Šæ˜ ã§è¦³ã‚Œã¦ã‚ˆã‹ã£ãŸ!ãã—ã¦ä½•ã‚ˆã‚Šã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒå¯æ„›ã...</td>\n",
       "      <td>çµµ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ è¿«åŠ› ã‚ã£ è½ŸéŸ³ ä¸Šæ˜  è¦³ã‚Œ ã‚ˆã‹ã£ ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ å¯æ„›ã ãƒ ã‚²ãƒ³ ã‹ã£ã“ ã„ã„</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  users                                              texts scores conditions  \\\n",
       "0   Amy  ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒæœ¬å½“ã‹ã‚ã„ã„\\nã©ã®ã‚­ãƒ£ãƒ©ã‚‚é­…åŠ›çš„\\n\\nå…¨ä½“çš„ã«çµµãŒæŸ”ã‚‰ã‹ãã¦å¯æ„›ã„\\nã‚¢ã‚¯ã‚·...    3.5       lazy   \n",
       "1    ã¯ã‚‹  çµµãŒç¶ºéº—ã§ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è¿«åŠ›ã‚ã£ãŸï¼\\nè½ŸéŸ³ä¸Šæ˜ ã§è¦³ã‚Œã¦ã‚ˆã‹ã£ãŸï¼\\nãã—ã¦ä½•ã‚ˆã‚Šã‚·ãƒ£ã‚ªãƒ˜ã‚¤...    4.7       lazy   \n",
       "\n",
       "                                            texts_ed  \\\n",
       "0  ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒæœ¬å½“ã‹ã‚ã„ã„ã©ã®ã‚­ãƒ£ãƒ©ã‚‚é­…åŠ›çš„å…¨ä½“çš„ã«çµµãŒæŸ”ã‚‰ã‹ãã¦å¯æ„›ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚·ãƒ¼ãƒ³ã¯ã‚¹ãƒ”...   \n",
       "1  çµµãŒç¶ºéº—ã§ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è¿«åŠ›ã‚ã£ãŸ!è½ŸéŸ³ä¸Šæ˜ ã§è¦³ã‚Œã¦ã‚ˆã‹ã£ãŸ!ãã—ã¦ä½•ã‚ˆã‚Šã‚·ãƒ£ã‚ªãƒ˜ã‚¤ãŒå¯æ„›ã...   \n",
       "\n",
       "                                        texts_wakati  \n",
       "0  ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ æœ¬å½“ ã‹ã‚ã„ã„ ã‚­ãƒ£ãƒ© é­…åŠ› å…¨ä½“ çµµ æŸ”ã‚‰ã‹ã å¯æ„›ã„ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ã‚·ãƒ¼ãƒ³ ã‚¹...  \n",
       "1    çµµ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ è¿«åŠ› ã‚ã£ è½ŸéŸ³ ä¸Šæ˜  è¦³ã‚Œ ã‚ˆã‹ã£ ã‚·ãƒ£ã‚ªãƒ˜ã‚¤ å¯æ„›ã ãƒ ã‚²ãƒ³ ã‹ã£ã“ ã„ã„  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¥µæ€§è¾æ›¸ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_= pd.read_table('æ¥µæ€§è¾æ›¸.txt',sep =':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>å„ªã‚Œã‚‹</th>\n",
       "      <th>ã™ãã‚Œã‚‹</th>\n",
       "      <th>å‹•è©</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>è‰¯ã„</td>\n",
       "      <td>ã‚ˆã„</td>\n",
       "      <td>å½¢å®¹è©</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>å–œã¶</td>\n",
       "      <td>ã‚ˆã‚ã“ã¶</td>\n",
       "      <td>å‹•è©</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>è¤’ã‚ã‚‹</td>\n",
       "      <td>ã»ã‚ã‚‹</td>\n",
       "      <td>å‹•è©</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ã‚ã§ãŸã„</td>\n",
       "      <td>ã‚ã§ãŸã„</td>\n",
       "      <td>å½¢å®¹è©</td>\n",
       "      <td>0.999645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>è³¢ã„</td>\n",
       "      <td>ã‹ã—ã“ã„</td>\n",
       "      <td>å½¢å®¹è©</td>\n",
       "      <td>0.999486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55119</th>\n",
       "      <td>ãªã„</td>\n",
       "      <td>ãªã„</td>\n",
       "      <td>åŠ©å‹•è©</td>\n",
       "      <td>-0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55120</th>\n",
       "      <td>é…·ã„</td>\n",
       "      <td>ã²ã©ã„</td>\n",
       "      <td>å½¢å®¹è©</td>\n",
       "      <td>-0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55121</th>\n",
       "      <td>ç—…æ°—</td>\n",
       "      <td>ã³ã‚‡ã†ã</td>\n",
       "      <td>åè©</td>\n",
       "      <td>-0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55122</th>\n",
       "      <td>æ­»ã¬</td>\n",
       "      <td>ã—ã¬</td>\n",
       "      <td>å‹•è©</td>\n",
       "      <td>-0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55123</th>\n",
       "      <td>æ‚ªã„</td>\n",
       "      <td>ã‚ã‚‹ã„</td>\n",
       "      <td>å½¢å®¹è©</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55124 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        å„ªã‚Œã‚‹  ã™ãã‚Œã‚‹   å‹•è©         1\n",
       "0        è‰¯ã„    ã‚ˆã„  å½¢å®¹è©  0.999995\n",
       "1        å–œã¶  ã‚ˆã‚ã“ã¶   å‹•è©  0.999979\n",
       "2       è¤’ã‚ã‚‹   ã»ã‚ã‚‹   å‹•è©  0.999979\n",
       "3      ã‚ã§ãŸã„  ã‚ã§ãŸã„  å½¢å®¹è©  0.999645\n",
       "4        è³¢ã„  ã‹ã—ã“ã„  å½¢å®¹è©  0.999486\n",
       "...     ...   ...  ...       ...\n",
       "55119    ãªã„    ãªã„  åŠ©å‹•è© -0.999997\n",
       "55120    é…·ã„   ã²ã©ã„  å½¢å®¹è© -0.999997\n",
       "55121    ç—…æ°—  ã³ã‚‡ã†ã   åè© -0.999998\n",
       "55122    æ­»ã¬    ã—ã¬   å‹•è© -0.999999\n",
       "55123    æ‚ªã„   ã‚ã‚‹ã„  å½¢å®¹è© -1.000000\n",
       "\n",
       "[55124 rows x 4 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ”¹è¡Œè¨˜å·ãªã©ã®é™¤å»\n",
    "def linebreak(desstr):\n",
    "    return ''.join(desstr.replace('<br/>','').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed = [filter_emoji(i) for i in df['comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed = [linebreak(i) for i in texts_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«çµ±ä¸€ã™ã‚‹\n",
    "def uppertolower(desstr):\n",
    "    return desstr.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed = [uppertolower(i) for i in texts_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ­£è¦åŒ–\n",
    "def normalization(desstr):\n",
    "    return neologdn.normalize(desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neologdn\n",
    "texts_ed = [normalization(i) for i in texts_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è¨˜å·ç³»æ•°å­—ã®é™¤å»\n",
    "def number(desstr):\n",
    "    return re.sub('[â‘ -â‘¨]', '', desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed = [number(i) for i in texts_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urlã®é™¤å»\n",
    "def url(desstr):\n",
    "    return re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+','',desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed = [url(i) for i in texts_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed[504]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã”å‚è€ƒã¾ã§ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = browser.find_elements('css selector','.p-mark__review') \n",
    "print(type(elements))\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(element[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_page = browser.find_elements('css selector','.c-pagination__prev') \n",
    "print(type(elements_page))\n",
    "print(elements_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elements_page[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n",
    "    å¯¾å¿œæ–¹æ³•ï¼šhttps://blog.csdn.net/qq_38486203/article/details/82852240\n",
    "    ï¼œé‡å¯ä¹Ÿä¸è¡Œï¼ŒæŠŠchromedriver.exeæ”¾åˆ°pythonè„šæœ¬çš„æ–‡ä»¶å¤¹ä¸‹é¢ï¼Œå†æ‰§è¡Œè¯•è¯•ï¼â†’OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://filmarks.com/movies/86613/no_spoiler?page='\n",
    "# url_num = re.search('(.*?)page=[0-9]',url)\n",
    "html_data = []\n",
    "for i in range(1,175):\n",
    "#     url = get_page(url_basic)\n",
    "    resp = request.urlopen(url+str(i))\n",
    "    data = resp.read().decode('utf-8')\n",
    "    html_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å‚è€ƒ<https://www.cnblogs.com/cymwill/articles/7574479.html>\n",
    "pmark_review_lists = []\n",
    "pmark_user_lists = []\n",
    "pmark_score_lists = []\n",
    "for m in html_data:\n",
    "    soup = bs(m,'html.parser')\n",
    "    #ã‚³ãƒ¡ãƒ³ãƒˆï¼œsoupï¼\n",
    "    pmark_review_list = soup.find_all('div','p-mark__review')\n",
    "    \n",
    "    #ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼œsoupï¼\n",
    "    pmark_user_list = soup.find_all(lambda tag:tag.has_attr('alt') and tag.has_attr('loading'))\n",
    "    \n",
    "    #ã‚¹ã‚³ã‚¢ï¼œæ­£è¦ï¼\n",
    "    pmark_score_total = soup.find_all('div',class_='c-media__content')\n",
    "    pmark_score_total_list = []\n",
    "    for i in pmark_score_total:\n",
    "        pmark_score_total_list.append(re.findall('<div class=\"c-rating__score\">(.*)</div></div></div>',str(i)))\n",
    "\n",
    "    pmark_review_lists.append(pmark_review_list)\n",
    "    pmark_user_lists.append(pmark_user_list)\n",
    "    pmark_score_lists.append(pmark_score_total_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pmark_score_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å‚è€ƒã€Šhttps://blog.csdn.net/u012611644/article/details/82814377ã€‹\n",
    "import re\n",
    "\n",
    "comments = []\n",
    "for item in sum(pmark_review_lists,[]):\n",
    "    item_ed = re.findall('<div class=\"p-mark__review\">(.*)</div>',str(item))\n",
    "    comments.append(item_ed)\n",
    "\n",
    "users =[]\n",
    "for user in sum(pmark_user_lists,[]):\n",
    "    user_ed = re.findall('<img alt=\"(.*)\" height.*',str(user))\n",
    "    users.append(user_ed)\n",
    "    \n",
    "# scores = []\n",
    "# for score in sum(pmark_score_lists,[]):\n",
    "#     score_ed = re.findall('<div class=\"c-rating__score\">(.*)</div>',str(score))\n",
    "#     scores.append(score_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(users))\n",
    "print(len(comments))\n",
    "print(len(sum(pmark_score_lists,[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([sum(users,[]),sum(comments,[]),sum(sum(pmark_score_lists,[]),[])],\n",
    "                 index =['users','comments','scores']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Luoxiaohei.csv',encoding ='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scores_tmp = []\n",
    "for i in df['scores']:\n",
    "    if i != '-':\n",
    "        scores_tmp.append(float(i))\n",
    "<!-- #     else: -->\n",
    "        scores_tmp.append(i.replace('-','0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emojiã®é™¤å»\n",
    "def filter_emoji(desstr, restr=''):\n",
    "    \"\"\"\n",
    "    filter emoji\n",
    "    desstr: origin str\n",
    "    restr: replace str\n",
    "    \"\"\"\n",
    "    # filter emoji\n",
    "    try:\n",
    "        res = re.compile(u'[\\U00010000-\\U0010ffff]')\n",
    "    except re.error:\n",
    "        res = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "    return res.sub(restr, desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ed = [filter_emoji(i) for i in df['comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ”¹è¡Œè¨˜å·ãªã©ã®é™¤å»\n",
    "def linebreak(desstr):\n",
    "    return ''.join(desstr.replace('<br/>','').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#é¡”æ–‡å­—ã®é™¤å»\n",
    "def Emoticons(desstr):\n",
    "    return re.sub('amp|','',desstr)\n",
    "#     return re.sub('Ï†Ï‰Ï†|Ğ´|Ñ„|â„ƒ|â—‹|ã€‡|&amp|â“|â—','',desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_wakati = [Emoticons(i) for i in texts_wakati]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_wakati[754]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ•°å­—ã®é™¤å»\n",
    "def number(desstr):\n",
    "    return re.sub('\\d+', '', desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_wakati = [number(i) for i in texts_wakati]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_wakati[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords\n",
    "\n",
    "stopwords_list = ['ã‚ãã“','ã‚ãŸã‚Š','ã‚ã¡ã‚‰','ã‚ã£ã¡','ã‚ã¨','ã‚ãª','ã‚ãªãŸ','ã‚ã‚Œ','ã„ãã¤','ã„ã¤','ã„ã¾','ã„ã‚„','ã„ã‚ã„ã‚','ã†ã¡','ãŠãŠã¾ã‹','ãŠã¾ãˆ','ãŠã‚Œ','ãŒã„','ã‹ã','ã‹ãŸã¡','ã‹ã‚„ã®','ã‹ã‚‰','ãŒã‚‰','ããŸ','ãã›','ã“ã“','ã“ã£ã¡','ã“ã¨','ã”ã¨','ã“ã¡ã‚‰','ã”ã£ã¡ã‚ƒ','ã“ã‚Œ','ã“ã‚Œã‚‰','ã”ã‚','ã•ã¾ã–ã¾','ã•ã‚‰ã„','ã•ã‚“','ã—ã‹ãŸ','ã—ã‚ˆã†','ã™ã‹','ãšã¤','ã™ã­','ã™ã¹ã¦','ãœã‚“ã¶','ãã†','ãã“','ãã¡ã‚‰','ãã£ã¡','ãã§','ãã‚Œ','ãã‚Œãã‚Œ','ãã‚Œãªã‚Š','ãŸãã•ã‚“','ãŸã¡','ãŸã³','ãŸã‚','ã ã‚','ã¡ã‚ƒ','ã¡ã‚ƒã‚“','ã¦ã‚“','ã¨ãŠã‚Š','ã¨ã','ã©ã“','ã©ã“ã‹','ã¨ã“ã‚','ã©ã¡ã‚‰','ã©ã£ã‹','ã©ã£ã¡','ã©ã‚Œ','ãªã‹','ãªã‹ã°','ãªã«','ãªã©','ãªã‚“','ã¯ã˜ã‚','ã¯ãš','ã¯ã‚‹ã‹','ã²ã¨','ã²ã¨ã¤','ãµã','ã¶ã‚Š','ã¹ã¤','ã¸ã‚“','ãºã‚“','ã»ã†','ã»ã‹','ã¾ã•','ã¾ã—','ã¾ã¨ã‚‚','ã¾ã¾','ã¿ãŸã„','ã¿ã¤','ã¿ãªã•ã‚“','ã¿ã‚“ãª','ã‚‚ã¨','ã‚‚ã®','ã‚‚ã‚“','ã‚„ã¤','ã‚ˆã†','ã‚ˆã','ã‚ã‘','ã‚ãŸã—','','ãƒã‚¤','','','ä¸Š','ä¸­','ä¸‹','å­—','','','å¹´','æœˆ','æ—¥','æ™‚','åˆ†','ç§’','é€±','ç«','æ°´','æœ¨','é‡‘','åœŸ','å›½','éƒ½','é“','åºœ','çœŒ','å¸‚','åŒº','ç”º','æ‘','','','å„','ç¬¬','æ–¹','ä½•','çš„','åº¦','æ–‡','è€…','æ€§','ä½“','äºº','ä»–','ä»Š','éƒ¨','èª²','ä¿‚','å¤–','é¡','é”','æ°—','å®¤','å£','èª°','ç”¨','ç•Œ','ä¼š','é¦–','ç”·','å¥³','åˆ¥','è©±','ç§','å±‹','åº—','å®¶','å ´','ç­‰','è¦‹','éš›','è¦³','æ®µ','ç•¥','ä¾‹','ç³»','è«–','å½¢','é–“','åœ°','å“¡','ç·š','ç‚¹','æ›¸','å“','åŠ›','æ³•','æ„Ÿ','ä½œ','å…ƒ','æ‰‹','æ•°','å½¼','å½¼å¥³','å­','å†…','æ¥½','å–œ','æ€’','å“€','è¼ª','é ƒ','åŒ–','å¢ƒ','ä¿º','å¥´','é«˜','æ ¡','å©¦','ä¼¸','ç´€','èªŒ','ãƒ¬','è¡Œ','åˆ—','äº‹','å£«','å°','é›†','æ§˜','æ‰€','æ­´','å™¨','å','æƒ…','é€£','æ¯','å¼','ç°¿','','','','','å›','åŒ¹','å€‹','å¸­','æŸ','æ­³','ç›®','é€š','é¢','å††','ç‰','æš','','å‰','å¾Œ','å·¦','å³','æ¬¡','å…ˆ','','æ˜¥','å¤','ç§‹','å†¬','','','','ä¸€','äºŒ','ä¸‰','å››','äº”','å…­','ä¸ƒ','å…«','ä¹','å','ç™¾','åƒ','ä¸‡','å„„','å…†','','','ä¸‹è¨˜','ä¸Šè¨˜','æ™‚é–“','ä»Šå›','å‰å›','å ´åˆ','ä¸€ã¤','å¹´ç”Ÿ','è‡ªåˆ†','ãƒ¶æ‰€','ãƒµæ‰€','ã‚«æ‰€','ç®‡æ‰€','ãƒ¶æœˆ','ãƒµæœˆ','ã‚«æœˆ','ç®‡æœˆ','åå‰','æœ¬å½“','ç¢ºã‹','æ™‚ç‚¹','å…¨éƒ¨','é–¢ä¿‚','è¿‘ã','æ–¹æ³•','æˆ‘ã€…','é•ã„','å¤šã','æ‰±ã„','æ–°ãŸ','ãã®å¾Œ','åŠã°','çµå±€','æ§˜ã€…','ä»¥å‰','ä»¥å¾Œ','ä»¥é™','æœªæº€','ä»¥ä¸Š','ä»¥ä¸‹','å¹¾ã¤','æ¯æ—¥','è‡ªä½“','å‘ã“ã†','ä½•äºº','æ‰‹æ®µ','åŒã˜','æ„Ÿã˜']\n",
    "\n",
    "def stopwords(desstr,stopwords):\n",
    "    stoplist_removed_documents = []\n",
    "    for i in desstr.split():\n",
    "        if i not in stopwords:\n",
    "            stoplist_removed_documents.append(i)\n",
    "    return stoplist_removed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwordsã‚’é™¤å»\n",
    "texts_wakati_cleaned = [stopwords(i,stopwords_list) for i in texts_wakati]\n",
    "\n",
    "texts_wakati_cleaned_tmp = [' '.join(i) for i in texts_wakati_cleaned] \n",
    "\n",
    "#åˆ†ã‹ã¡æ›¸ãåˆ—ã‚’è¿½åŠ \n",
    "df['text_wakati'] = texts_wakati_cleaned_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "print(df.isnull().any())\n",
    "\n",
    "df_null = pd.isnull(df)\n",
    "df_null = df[df == True]\n",
    "print(df_null)\n",
    "\n",
    "df = df.fillna('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ç¾…å°é»’æˆ¦è¨˜.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: red; \">æ„Ÿæƒ…åˆ†æ</span>\n",
    "    ã‚¹ã‚³ã‚¢ã¨å†…å®¹ã®ä¹–é›¢ã¨ã®ã“ã¨ãŒã‹ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ãŸã‚\n",
    "    ç‚¹æ•°ãŒä½ã„ã¨ã„ã†ã‚ˆã‚Šã€ãã‚‚ãã‚‚ç‚¹æ•°ã¤ã‘ã¦ã„ãªã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå¤šã„\n",
    "    \n",
    "    â‘ æ¥µæ€§è¾æ›¸ã‚’å…ƒã«æ„Ÿæƒ…åˆ†æã‚’è¡Œã†\n",
    "    â‘¡æ©Ÿæ¢°å­¦ç¿’ã‚’å…ƒã«æ„Ÿæƒ…åˆ†æã‚’è¡Œã†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â‘ æ©Ÿæ¢°å­¦ç¿’ã‚’å…ƒã«æ„Ÿæƒ…åˆ†æã‚’è¡Œã†\n",
    "    å‚è€ƒï¼œhttps://blog.csdn.net/u011001084/article/details/78980299ï¼\n",
    "    <https://www.jianshu.com/p/29aa3ad63f9d>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelä»˜ä¸\n",
    "# def make_label(df):\n",
    "#     df['sentiment'] = df['scores'].apply(lambda x:1 if float(x)>3.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.scores == '-','sentiment'] = 2\n",
    "df.loc[(df.scores <= '3.5')&(df.scores != '-'),'sentiment'] = 0\n",
    "df.loc[df.scores > '3.5','sentiment'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].replace('0',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df.sentiment == 2]))\n",
    "print(len(df[df.sentiment == 1]))\n",
    "print(len(df[df.sentiment == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sentiment == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df.sentiment != 2]\n",
    "df_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[['text_wakati']]\n",
    "Y = df_train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=1/3, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,Y_train.shape,X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = 0.8\n",
    "min_df = 3\n",
    "\n",
    "vect = CountVectorizer(max_df= max_df,\n",
    "                       min_df=min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix = pd.DataFrame(vect.fit_transform(X_train.text_wakati).toarray(), columns=vect.get_feature_names())\n",
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB(alpha =1.0,class_prior = None,fit_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(vect, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(pipe,X_train.text_wakati, Y_train, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train.text_wakati, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pipe.predict(X_test.text_wakati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['pred'] = Y_pred.tolist()\n",
    "X_test[X_test['pred'] ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df[['sentiment','comments','scores']].loc[X_test.index.tolist()]\n",
    "X_test['true'] = df_tmp['sentiment']\n",
    "X_test['comments'] = df_tmp['comments']\n",
    "X_test['scores'] = df_tmp['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test[X_test['pred'] != X_test['true']]))\n",
    "X_test[X_test['pred'] != X_test['true']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "roc_auc = accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_pred)\n",
    "\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC(area = %0.2f)' % (roc_auc))\n",
    "plt.xlabel(\"FPR (False Positive Rate)\")\n",
    "plt.ylabel(\"TPR (True Positive Rate)\")\n",
    "plt.title(\"Receiver Operating Characteristic, ROC(AUC = %0.2f)\"% (roc_auc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(CM,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_michi = df.loc[df.scores == '-'][['text_wakati']]\n",
    "Y_pred_michi = pipe.predict(X_pred_michi.text_wakati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_michi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tmp_2 = df.loc[df.scores == '-']\n",
    "df_tmp_2['pred']=Y_pred_michi.tolist()\n",
    "df_tmp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_good= str(''.join(df[df.scores >'4'].text_wakati))\n",
    "str_bad= str(''.join(df[df.scores <'3'].text_wakati))\n",
    "str_normal= str(''.join(df[(df.scores <='4')&(df.scores>='3')].text_wakati))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "frequency_good = collections.Counter(str_good.split())\n",
    "frequency_good.most_common(200)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_bad = collections.Counter(str_bad.split())\n",
    "frequency_bad.most_common(200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_normal = collections.Counter(str_normal.split())\n",
    "frequency_normal.most_common(200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_good = []\n",
    "for k,v in frequency_good.most_common(200):\n",
    "    words_good.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "font = r'C:\\Users\\jorey\\python\\UDDigiKyokashoN-R.ttc'\n",
    "stopwords =set(['æ˜ ç”»','ã‚¢ãƒ‹ãƒ¡','ã‚·ãƒ£ã‚ªãƒ˜ã‚¤','ã—','ã„ã†','ã„','ã‚ã‚‹','ã„ã‚‹','ãªã„','ã•','ã„ã„','ã™ã‚‹'])\n",
    "\n",
    "wc = WordCloud(background_color = 'black',\n",
    "               font_path =font,\n",
    "               width = 1000,\n",
    "               height = 800,\n",
    "               stopwords=stopwords\n",
    "               ).generate(str(words_good))\n",
    "plt.imshow(wc)\n",
    "wc.to_file('good.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1)ã™ã¹ã¦ã®å˜èªã‚’ç‰¹å¾´ã«ã™ã‚‹\n",
    "def bag_of_words(words):  \n",
    "    return dict([(word, True) for word in words]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.scores <= '3.5') & (df.scores !='-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.scores > '3.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_words_good =bag_of_words(' '.join(df[df.scores > '3.5'].text_wakati.values).split())\n",
    "feature_words_bad =bag_of_words(' '.join(df[(df.scores <= '3.5') & (df.scores !='-')].text_wakati.values).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‘ã®è©•ä¾¡:\" + str(len(feature_words_good)))\n",
    "\n",
    "print(\"========================================================================\")\n",
    "\n",
    "print(\"ğŸ‘ã®è©•ä¾¡:\" + str(len(feature_words_bad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_words_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2)2ã¤ã®å˜èªã®ãƒšã‚¢ï¼ˆbigrams)ã‚’ç‰¹å¾´ã«ã™ã‚‹\n",
    "import nltk  \n",
    "from nltk.collocations import BigramCollocationFinder  \n",
    "from nltk.metrics import BigramAssocMeasures  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(words, score_fn=BigramAssocMeasures.chi_sq, n=1000):  \n",
    "    bigram_finder = BigramCollocationFinder.from_words(words) #å˜èªãƒšã‚¢ã®ä½œæˆ  \n",
    "    bigrams = bigram_finder.nbest(score_fn, n) #ã‚«ã‚¤äºŒä¹—ã€ã‚¹ã‚³ã‚¢ä¸Šä½1000ã®å˜èªãƒšã‚¢ã‚’ã‚»ãƒ¬ã‚¯ãƒˆ \n",
    "    newBigrams = [u+ ' ' + v for (u,v) in bigrams]\n",
    "    return bag_of_words(newBigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggram_words_good = bigram(feature_words_good)\n",
    "biggram_words_bad = bigram(feature_words_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‘ã®è©•ä¾¡:\" + str(biggram_words_good))\n",
    "\n",
    "print(\"========================================================================\")\n",
    "\n",
    "print(\"ğŸ‘ã®è©•ä¾¡:\" + str(biggram_words_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_words(words,score_fn=BigramAssocMeasures.chi_sq,n=1000):\n",
    "    bigram_finder=BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn,n)\n",
    "    newBigrams = [u + ' ' + v for (u,v) in bigrams]\n",
    "    a = bag_of_words(words)\n",
    "    b = bag_of_words(newBigrams)\n",
    "    a.update(b) \n",
    "    return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggram_good = bigram_words(feature_words_good)\n",
    "biggram_bad = bigram_words(feature_words_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‘ã®è©•ä¾¡:\" + str(biggram_good))\n",
    "\n",
    "print(\"========================================================================\")\n",
    "\n",
    "print(\"ğŸ‘ã®è©•ä¾¡:\" + str(biggram_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3)ã‚³ãƒ¼ãƒ‘ã‚¹å†…å˜èªã”ã¨ã®æƒ…å ±é‡ã‚’è¨ˆç®—ã™ã‚‹\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist  \n",
    "import itertools\n",
    "def create_word_scores():\n",
    "    #è¾æ›¸ã®å½¢ã‚’ãƒªã‚¹ãƒˆã«æ•´ç†ã™ã‚‹\n",
    "    poswords = list(itertools.chain(feature_words_good))\n",
    "    negwords = list(itertools.chain(feature_words_bad))\n",
    "    \n",
    "    #å˜èªã®é »åº¦ã‚’çµ±è¨ˆã™ã‚‹\n",
    "    word_fd = FreqDist()\n",
    "    cond_word_fd = ConditionalFreqDist()\n",
    "    for word in poswords:\n",
    "        word_fd[word] += 1  \n",
    "        cond_word_fd['pos'][word] += 1\n",
    "    for word in negwords:\n",
    "        word_fd[word] += 1\n",
    "        cond_word_fd['neg'][word] +=1  \n",
    "    \n",
    "    #ãƒã‚¸ã¨ãƒã‚¬å˜èªæ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "    pos_word_count = cond_word_fd['pos'].N()\n",
    "    neg_word_count = cond_word_fd['neg'].N() \n",
    "    total_word_count = pos_word_count + neg_word_count  \n",
    "    \n",
    "    word_scores = {}  \n",
    "    for word, freq in word_fd.items():  \n",
    "        pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word], (freq, pos_word_count), total_word_count) #è®¡ç®—ç§¯æè¯çš„å¡æ–¹ç»Ÿè®¡é‡ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥è®¡ç®—äº’ä¿¡æ¯ç­‰å…¶å®ƒç»Ÿè®¡é‡  \n",
    "        neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word], (freq, neg_word_count), total_word_count) #åŒç†  \n",
    "        word_scores[word] = pos_score + neg_score #ä¸€ä¸ªè¯çš„ä¿¡æ¯é‡ç­‰äºç§¯æå¡æ–¹ç»Ÿè®¡é‡åŠ ä¸Šæ¶ˆæå¡æ–¹ç»Ÿè®¡é‡  \n",
    "    \n",
    "    return word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_bigram_scores(): \n",
    "    posWords = list(itertools.chain(biggram_words_good))  \n",
    "    negWords = list(itertools.chain(biggram_words_bad))  \n",
    "\n",
    "    bigram_finder = BigramCollocationFinder.from_words(posWords)  \n",
    "    bigram_finder = BigramCollocationFinder.from_words(negWords)  \n",
    "    posBigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 5000)  \n",
    "    negBigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 5000)  \n",
    "\n",
    "    pos = posWords + posBigrams  \n",
    "    neg = negWords + negBigrams\n",
    "\n",
    "    word_fd = FreqDist()  \n",
    "    cond_word_fd = ConditionalFreqDist()  \n",
    "    for word in pos:  \n",
    "        word_fd[word] += 1  \n",
    "        cond_word_fd['pos'][word] += 1 \n",
    "    for word in neg:  \n",
    "        word_fd[word] += 1\n",
    "        cond_word_fd['neg'][word] += 1  \n",
    "\n",
    "    pos_word_count = cond_word_fd['pos'].N()  \n",
    "    neg_word_count = cond_word_fd['neg'].N()  \n",
    "    total_word_count = pos_word_count + neg_word_count  \n",
    "\n",
    "    word_scores = {}  \n",
    "    for word, freq in word_fd.items():  \n",
    "        pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word], (freq, pos_word_count), total_word_count)  \n",
    "        neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word], (freq, neg_word_count), total_word_count)  \n",
    "        word_scores[word] = pos_score + neg_score  \n",
    "\n",
    "    return word_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_word_bigram_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(words_score,index = ['words','socre']).T.to_csv('words_score_xiaohei.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_words(word_scores, number):\n",
    "    best_vals = sorted(word_scores.items(),key = lambda w_s:w_s[1],reverse = True)[:number]\n",
    "#     best_vals = sorted(word_scores.iteritems(), key=lambda (w, s):(w,s), reverse=True)[:number]\n",
    "#     best_vals = sorted(word_scores.items(), key=lambda item:item[1],  reverse=True)[:number]\n",
    "    best_words = set([w for w, s in best_vals])\n",
    "    return best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores_1 = create_word_scores()  \n",
    "word_scores_2 = create_word_bigram_scores() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_words = find_best_words(word_scores_1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_word_features(words):  \n",
    "    #load_data()\n",
    "    #word_scores = create_word_bigram_scores()\n",
    "    global best_words\n",
    "    #best_words = find_best_words(word_scores,7500)\n",
    "    return dict([(word, True) for word in words if word in best_words])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_word_scores_1=best_word_features(word_scores_1)\n",
    "best_word_scores_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_words = find_best_words(word_scores_2,100)\n",
    "best_word_scores_2=best_word_features(word_scores_2)\n",
    "best_word_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_feature(feature_extraction_method):\n",
    "    posFeatures =[]\n",
    "    for i in pos_review:\n",
    "        posWords = [feature_extraction_method(i),'pos']\n",
    "        posFeatures.append(posWords)\n",
    "    return posFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å»ƒæ­¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmark_dict = dict(zip(sum(users,[]),sum(comments,[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_tmp = pd.DataFrame(pmark_dict,index=['comments']).T\n",
    "\n",
    "df = df_tmp.reset_index().rename(columns={'index':'user'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(scores))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comments'][0] \n",
    "#çµµæ–‡å­—\n",
    "#æ”¹è¡Œãªã©\n",
    "#æ„Ÿæƒ…æº¢ã‚Œã™ãè¡¨ç¾\n",
    "\n",
    "texts_ed[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
