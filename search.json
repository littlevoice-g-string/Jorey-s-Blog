[{"title":"簡易な形態素解析器を構築","url":"/Jorey-s-Blog/2021/03/28/build-a-word-segumentation/","content":"\n### 1.説明\n### 2.new word discovery\n### 3.word segmentation\n\n ","tags":["自然言語処理","nlp"],"categories":["自然言語処理"]},{"title":"エラー集","url":"/Jorey-s-Blog/2021/03/28/others_エラー集/","content":"others\n\n1. you-get https://www.bilibili.com/video/BV1yK4y1E7n4?p=95\n\terror:  \n\t\tyou-get: [error] oops, something went wrong.\n\t\tyou-get: don't panic, c'est la vie. please try the following steps:\n\t\tyou-get:   (1) Rule out any network problem.\n\t\tyou-get:   (2) Make sure you-get is up-to-date.\n\t\tyou-get:   (3) Check if the issue is already known, on\n\t\tyou-get:         https://github.com/soimort/you-get/wiki/Known-Bugs\n\t\tyou-get:         https://github.com/soimort/you-get/issues\n\t\tyou-get:   (4) Run the command with '--debug' option,\n\t\tyou-get:       and report this issue with the full output.\n\n\tstep1:\n\t\tyou-get -i https://www.bilibili.com/video/BV1yK4y1E7n4?p=95\n\t\t\ttitle:               贪心 NLP 自然语言处理 (P95. 任务095：访问首页列表中的url)\n\t\tstreams:             # Available quality and codecs\n\t\t    [ DASH ] ____________________________________\n\t\t    - format:        dash-flv480\n\t\t      container:     mp4\n\t\t      quality:       清晰 480P\n\t\t      size:          32.0 MiB (33512282 bytes)\n\t\t    # download-with: you-get --format=dash-flv480 [URL]\n\t\t    ...\n\tsolved by:\n\t\tyou-get --format=dash-flv https://www.bilibili.com/video/BV1yK4y1E7n4?p=95\n\t\t\t- 説明:音声と画像が切り分けられてしまっている\n<!-- more -->\n","tags":["エラー"],"categories":["others"]},{"title":"NeuralNetwork実践_MNIST","url":"/Jorey-s-Blog/2021/03/16/neuralnetwork_MINIT/","content":"### 説明\nニューラルネットワークを理解するため、Numpy、keras、Pytorchを使ってMNISTデータセットの手書き数字を識別してみました。\n- 使用しているデータ\n\t- [Kaggle Digit Recognizer](https://www.kaggle.com/c/digit-recognizer)\n\t- MNISTデータセットは、機械学習でとても有名なデータセットであり、0から9までの手書き数字のグレースケール画像が入っています。各画像は、縦28X横28合計784ピクセルとなっており、各ピクセルには一つ0から255のピクセル値をついています。そのピクセル値が大きいければ大きいほど暗いことを意味しています。\n\t- Trainデータに関しては、先頭のlabel列はユーザーが描いた数字を表しており、残りの列は該当画像のピクセル値が格納されています。\n\t- testデータに関してはTrainデータと同様に、画像のピクセル値が格納していますが、label列がついていないので、それを識別するのは今回のタスクです。\n- training data,validate dataを用意します。\n<!-- more -->\n```\ndata = np.array(train)\nm,n = data.shape\nnp.random.shuffle(data)\n\ndata_val=data[0:1000].T\nY_val = data_val[0]\nX_val = data_val[1:n]\nX_val = X_val/255.\nprint(data_val.shape,Y_val.shape,X_val.shape)\n\ndata_train = data[1000:m].T\nY_train = data_train[0]\nX_train = data_train[1:n]\nX_train = X_train/255.\n_,m_train = X_train.shape\nprint(data_train.shape,Y_train.shape,X_train.shape)\n\n#(785, 1000) (1000,) (784, 1000)\n(785, 41000) (41000,) (784, 41000)\n```\n\n### 1.EDA\nまずkaggleからダウンロードしたtrain.csvとtest.csvを読み込んで、データの中身をいろいろ確認してみました。\n\n0. モジュールをインポートします。\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom collections import Counter\nfrom pyg2plot import Plot\nfrom pyecharts.charts import Bar,Line\nfrom pyecharts import options as opts\n``` \n1. データ件数\ntrain:42000件訓練データ、785列、testデータと違って、label列がついています。\ntest:28000件テストデータ、784列\n```\ntrain.shape,test.shape\n\t#((42000, 785), (28000, 784))\n\n\tfor i in train.columns:\n\t    if i not in test.columns:\n\t        print(i)\n\t#label\n```\n\n2. 実際にどういう数字なのか、画像で見てみます。\n```\nnum=train.iloc[10]\npic=np.array(num)\npic=pic[1:785]\npic=pic.reshape(28,28)\nplt.imshow(pic,cmap=\"magma\")\nplt.show()\n```\n<div style=\"width:50%;margin:left\">{% asset_img show_num.png show_num%}</div>\n3. 全体のvalue値の幅を見てみます。\n- value値は0から255の間に落ちているので、後ほどの前処理ではvalue値を標準化処理にします。\n```\nscatter = Plot(\"Scatter\")\ndf_scatter = pd.DataFrame(pic.reshape(392,2))\ndf_scatter_dict = df_scatter.to_dict(orient = 'dict')\nscatter.set_options(\n{\n    'appendPadding': 30,\n    'data': df_scatter.to_dict(orient='records'),\n    'xField': 0,\n    'yField': 1,\n    'size': [4, 30],\n    'shape': 'circle',\n    'pointStyle':{'fillOpacity': 0.8,'stroke': '#bbb'},\n    'xAxis':{'line':{'style':{'stroke': '#aaa'}},},\n    'yAxis':{'line':{'style':{'stroke': '#aaa'}},},\n    'quadrant':{\n        'xBaseline': 0,\n        'yBaseline': 0,\n    },\n})\nscatter.render()\n```\n<div style=\"width:100%;margin:auto\">{% asset_img show_values.png show_values%}</div>\n4. ラベルごとデータの件数を見てみます。\n- 各ラベルのデータ件数が大きいな差がなく、均等的であることがわかりました。\n```\nbar = (\n    Bar(init_opts=opts.InitOpts(width=\"620px\", height=\"420px\"))\n    .add_xaxis(list(train_count.keys()))\n    .add_yaxis('count', list(train_count.values()))\n    .set_global_opts(title_opts=opts.TitleOpts(title=\"count\", subtitle=None))\n    .set_global_opts(title_opts={\"text\": \"count\", \"subtext\": None})\n)\n\nbar.render_notebook()\n```\n<div style=\"width:80%;margin:auto\">{% asset_img show_labels.png show_labels%}</div>\n\n### 2.Numpyでニューラルネットワークを実装してみました\n1. 今回2層ニューラルネットワークを実装しようと思います。\n\t- 活性化関数:sigmoid関数(h1)とsoftmax関数(h2)\n\t- 損失関数:交差エントロピー誤差\n<div style=\"width:80%;margin:left\">{% asset_img 2層ニューラルネットワーク.png 2層ニューラルネットワーク%}</div>\n2. 事前に活性化関数、損失関数、labelのone-hot処理の関数を用意します。\n```\n#h1 sigmoid関数\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\n#back propagation用sigmoid関数の微分関数\ndef sigmoid_(x):\n    return sigmoid(x)*(1-sigmoid(x))\n\n#h2 softmax関数\ndef softmax(z):\n    A = np.exp(z)/sum(np.exp(z))\n    return A\n\n#one-hot 処理関数\ndef one_hot(Y):\n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n    one_hot_Y[np.arange(Y.size), Y] = 1\n    one_hot_Y = one_hot_Y.T\n    return one_hot_Y\n\n#損失関数\ndef cross_entropy(a, y):\n    return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n```\n3. w1,b1,w2,b2の初期化\n```\ninput_, hidden, output = 784, 100, 10\n\nw1 = np.random.randn(hidden,input_)\nb1 = np.random.randn(hidden,1)\nw2 = np.random.randn(output,hidden)\nb2 = np.random.randn(output,1)\nprint(w1.shape,b1.shape,w2.shape,b2.shape)\n#(100, 784) (100, 1) (10, 100) (10, 1)\n```\n4. forwardの実現\n```\ndef forward(x,w1,w2,b1,b2):\n    a1 = np.dot(w1,x)+b1\n    z1 = sigmoid(a1)\n    a2 = np.dot(w2,z1)+b2\n    z2 = softmax(a2)\n    y_pred = z2\n    return a1,a2,z1,y_pred\na1,a2,z1,y_pred = forward(X_train,w1,w2,b1,b2)\nprint(a1.shape,a2.shape,z1.shape,y_pred.shape)\n#(100, 41000) (10, 41000) (100, 41000) (10, 41000)\n```\n5. back propagatationの実現\n```\ndef backward(z1,a1,y_pred,a2,x,y,w1,w2):\n    dz2 = y_pred - one_hot(y)\n    dw2 = (np.dot(dz2,z1.T))/m\n    db2 = (1/m)*np.sum(dz2,axis=1,keepdims=True)\n    da1 = np.dot(w2.T,dz2)\n    dz1 = da1*sigmoid_(a1)\n\n    dw1 =(1/m)*np.dot(dz1,x.T)\n    db1 = (1/m)*np.sum(dz1,axis=1,keepdims=True)\n    return dw1,db1,dw2,db2\n\ndef update(w1,b1,w2,b2,dw1,db1,dw2,db2,lr):  \n    w2 -= lr*dw2\n    b2 -= lr*db2\n    w1 -= lr*dw1\n    b1 -= lr*db1\n    return w1,w2,b1,b2\n```\n6. パラメータ最適化\n```\n# accuracy値の関数\ndef get_accuracy(predictions,y):\n    return np.sum(predictions == y)/y.size\ndef gradient_descent(x,y,w1,w2,b1,b2,iterations,lr):\n    accuracy_ = list()\n    for i in range(iterations):\n        a1,a2,z1,y_pred = forward(x,w1,w2,b1,b2)\n        dw1,db1,dw2,db2 = backward(z1,a1,y_pred,a2,x,y,w1,w2)\n        w1,w2,b1,b2=update(w1,b1,w2,b2,dw1,db1,dw2,db2,lr)\n        if i % 10 == 0:\n            print(f'第{(i/10)}回',i)\n            predictions = np.argmax(a2,0)\n            accuracy = round(get_accuracy(predictions,y),3)\n            accuracy_.append(accuracy)\n            print(accuracy)\n    return w2,b2,w1,b1,accuracy,accuracy_\nw2,b2,w1,b1,accuracy,accuracy_ = gradient_descent(X_train,Y_train,w1,w2,b1,b2,5000,0.01)\n\n#第0.0回 0\n0.09409756097560976\n第1.0回 10\n0.1038780487804878\n第2.0回 20\n0.11382926829268293\n第3.0回 30\n0.1218780487804878\n....\n```\n7. accuracyの推移値\n```\nx_axis = [i for i in range(len(accuracy_))]\ny_axis = accuracy_\nline1=(\n    Line()\n    .add_xaxis(x_axis)  \n    .add_yaxis('accuracy',y_axis)\n    .set_global_opts(title_opts=opts.TitleOpts(title='accuracy'),\n                     xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(font_size=20)))\n)\nline1.render_notebook()\n```\n<div style=\"width:80%;margin:auto\">{% asset_img numpy_acc_line.png numpy_acc_line%}</div>\n\n### 3.Kerasでニューラルネットワークを構築てみました\n1. 考え方は基本Numpyでの構築と同様ですが、Sequentialモデルをもとに、.add()でレイヤーを積み重ねて、.compile()で訓練プロセスを設定します。\n```\nmodel=Sequential() #\nmodel.add(Dense(100,activation = 'relu'))\nmodel.add(Dense(10,activation = 'softmax'))\n\nmodel.compile(optimizer='adam',\n             loss ='sparse_categorical_crossentropy',\n             metrics ='acc')\nmodel.fit(X_train.T,Y_train,epochs=60,batch_size=512)\n```\n<div style=\"width:100%;margin:auto\">{% asset_img keras_result.png keras_result%}</div>\n2. testデータの予測し、Kaggleに提出します。\n```\nresult = model.predict(np.array(test))\nresult_=pd.DataFrame(results,index=list(range(1,len(results)+1))).reset_index().rename(columns = {'index':'ImageId',0:'Label'})\nresult_.to_csv('Digit Recognizer_version3(keras).csv',index=False)\n```\n<div style=\"width:100%;margin:auto\">{% asset_img keras_kaggle.png keras_kaggle%}</div>\n\n### 4.Pytorchでニューラルネットワークを構築してみました\n1. データ集をPytorch専用のTensorに変換する\n```\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn,optim\nimport torch.nn.functional as F\nimport torch.utils.data\nX_train_torch = torch.from_numpy(X_train).float()\nY_train_torch = torch.from_numpy(Y_train).long()\nX_val_torch = torch.from_numpy(X_val).float()\nY_val_torch = torch.from_numpy(Y_val).long()\nX_train_torch.T.size(0),Y_train_torch.T.size(0)\n#(41000, 41000)\n```\n2. Batchごとを行うため、TensorDatasetに格納します。\n```\ntrain_dataset=torch.utils.data.TensorDataset(X_train_torch.T, Y_train_torch)\nval_dataset=torch.utils.data.TensorDataset(X_val_torch.T, Y_val_torch)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset,batch_size=1000,shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset,batch_size=1000,shuffle=True)\n```\n3. Pytorchのフォーマットに沿って、ニューラルネットワークの構築を行います。過学習を防ぐため、Dropoutを利用します。\n```\nclass NN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(784,100)\n        self.layer2 = nn.Linear(100,10)\n        \n        self.dropout = nn.Dropout(p=0.2)\n        \n    def forward(self,x):\n        x = x.view(x.shape[0],-1)\n        \n        x = self.dropout(F.relu(self.layer1(x)))\n        x = F.softmax(self.layer2(x),dim=1)\n        \n        return x\n```\n4.　損失関数と最適化手法を設定し、訓練プロセスを設定します。\n```\nmodel = NN()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(),lr=0.001)\n\nepochs = 100\ntrain_losses = []\ntest_losses = []\naccuracy_list = []\nprint('start')\nmodel = model.float()\nfor e in range(epochs):\n    running_loss = 0\n    for images,labels in train_loader:\n        optimizer.zero_grad()\n        \n        log_ps = model(images)\n        loss = criterion(log_ps,labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    else:\n        test_loss = 0\n        accuracy = 0\n        with torch.no_grad():\n            model.eval()\n            \n            for images,labels in val_loader:\n                log_ps =model(images)\n                test_loss += criterion(log_ps,labels)\n                ps = torch.exp(log_ps)\n                top_p,top_class = ps.max(dim=1, keepdim=True)\n\n                equals = top_class == labels.view(*top_class.shape)\n                \n                accuracy += torch.mean(equals.type(torch.FloatTensor))\n                \n        model.train()\n        \n        train_losses.append(running_loss/len(train_loader))\n        test_losses.append(test_loss/len(val_loader))\n        accuracy_list.append(accuracy)\n                \n        print(\"epoch: {}/{}.. \".format(e+1, epochs),\n              \"train_loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n              \"test_loss: {:.3f}.. \".format(test_loss/len(val_loader)),\n              \"accuracy: {:.3f}\".format(accuracy/len(val_loader)))      \n```\n<div style=\"width:100%;margin:auto\">{% asset_img pytorch_result.png pytorch_result%}</div>\n5.可視化によって、損失と精度の推移を確認します。\n```\nplt.plot(train_losses,label='Training loss')\nplt.plot(test_losses,label='Validation loss')\nplt.plot(accuracy_list,label='Accuracy')\nplt.legend()\n```\n<div style=\"width:100%;margin:auto\">{% asset_img pytorch_graph.png pytorch_graph%}</div>\n\n### 5.Next\n- Pytorchでテキストの分類を行ってみる\n\n### 6.参考文献\n\n1. [keras documents](https://keras.io/ja/)\n2. [pytorch documents](https://pytorch.org/docs/stable/index.html)\n3. [ゼロから作るDeepLearning](https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-%E2%80%95Python%E3%81%A7%E5%AD%A6%E3%81%B6%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E7%90%86%E8%AB%96%E3%81%A8%E5%AE%9F%E8%A3%85-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873117585/ref=sr_1_1?adgrpid=103020866343&dchild=1&gclid=Cj0KCQjw0oCDBhCPARIsAII3C_EwbmbxiFhXpRe6SPmoc7pFv4BMTFtbL0VNqLMla0wervsh-BHtO-UaAkmaEALw_wcB&hvadid=448651961923&hvdev=c&hvlocphy=1009333&hvnetw=g&hvqmt=e&hvrand=13330263149548873495&hvtargid=kwd-335457286238&hydadcr=27294_11561544&jp-ad-ap=0&keywords=%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8Bdeep+learning&qid=1616920581&sr=8-1)\n","tags":["機械学習","ニューラルネットワーク","Pytorch","Keras","Numpy"],"categories":["機械学習"]},{"title":"パワポを自動翻訳する","url":"/Jorey-s-Blog/2021/02/10/パワポを自動通訳する/","content":"<!-- toc -->\n### 課題紹介\n業務でよく報告書を翻訳するので、いつもDeepLを使っていますが、DeepLは最近翻訳したパワポをダウンロードしたら、修復エラー出てしまうので、Google translate apiを呼び出して、パワポの自動翻訳を実現したいと思います。\n\n### 参考文献\n1. [Pythonでパワポスライドを自動翻訳する](https://qiita.com/code_440/items/9998d97b480db82ef738)\n2. [3 分で作る無料の翻訳 API with Google Apps Script](https://qiita.com/tanabee/items/c79c5c28ba0537112922)\n3. [deep-translator_github](https://github.com/nidhaloff/deep-translator)\n4. [python-pptx](https://python-pptx.readthedocs.io/en/latest/)\n\n### ① Google Apps Scriptによる翻訳APIの利用\n1. 簡単にapiを作成\n\n>こちらの記事[`3 分で作る無料の翻訳 API with Google Apps Script`](https://qiita.com/tanabee/items/c79c5c28ba0537112922)を参考し、無料の翻訳apiを作成しました。\n>下記のコードをGoogle Apps Scriptにて入力し、`ウェブアプリケーションとして導入`を選択したら\n`https://script.google.com/macros/s/[api_key]/exec`のようなURLが生成してくれる。こちらを使って翻訳を行います。\n>`https://script.google.com/macros/s/[api_key]/exec?text=こんにちは&source=ja&target=en`で結果があったら、apiの作成が成功できたといえます。\n\n<!-- more -->\n\n<div style=\"width:70%;margin:auto\">{% asset_img api_result.png api_result%}</div>\n\n```\nfunction doGet(e) {\n  var p = e.parameter;\n  var translatedText = LanguageApp.translate(p.text, p.source, p.target);\n  return ContentService.createTextOutput(translatedText);\n}\n```\n\n2. PythonからAPIにアクセス\n`requests`を使ってAPIのURLにアクセスし、英訳を試してみました。\n```\ndef translate(str_in, source='ja', target= 'en'):\n    url=\"https://script.google.com/macros/s/\"\n    url += \"[api_key]\"\n    url += \"/exec?text=\" + str_in\n    url += \"&source=\" + source\n    url += \"&target=\" + target\n    rr = requests.get(url)\n\n    return rr.text\n```\n<div style=\"width:70%;margin:auto\">{% asset_img api_test.png api_test%}</div>\n\n### ② deep-translatorを使って複数の翻訳ツールを利用する\n1. deep-translatorはgoogle_translator,Linguee translator,DeepL等複数翻訳ツールを対応してくれる複合型モジュールです。いくつかの翻訳ツールを比較したい場合、かなり便利なモジュールですね。本記事では、google translate、Linguee translator、DeepLを使いたいと思います。\n* DeepL apiは現在アクセスできないため、とりあえずやめました。\n* google_translatorを使用する場合、5000文字の制限があります。\n\t- text must be a valid text with maximum 5000 character,otherwise it cannot be translated\n<div style=\"width:70%;margin:auto\">{% asset_img deep_translator.png deep_translator%}</div>\n```\n#google translate\ndef Google_Translator(str_in):\n    from_language = json_inputs['source_Google']\n    to_language = json_inputs['target_Google']\n    translated = GoogleTranslator(source = from_language,target = to_language).translate(text=str_in)\n    return translated\n#Linguee translator\ndef Linguee_Translator(str_in):\n    from_language = json_inputs['source_Linguee']\n    to_language = json_inputs['target_Linguee']\n    translated = LingueeTranslator(source = from_language,target = to_language).translate(str_in)\n    return translated\n#DeepL\n#DeepLは今有料ユーザーにしかapiを提供しないため、auth_keyは必要です。\nwith open(os.getcwd() + '/config.json') as j:\n    config = json.load(j)\ndef DeepL_Translator(str_in):\n    from_language = json_inputs['source_deepl']\n    to_language = json_inputs['target_deepl']\n    translated = DeepL(config['auth_token']).translate(source=from_language,target=to_language,text=str_in)\n    return translated\n```\n2. 各翻訳ツール対応している言語、及び言語の省略をdeep-translatorでそのまま調べます。\n```\nlangs_list = GoogleTranslator.get_supported_languages(as_dict=True) \ngoogle_lang = pd.DataFrame(langs_list.keys(),langs_list.values()\n            ).reset_index().rename(columns= {'index':'言語省略',0:'言語'})\nprint(google_lang[google_lang['言語']=='english'])\nprint(google_lang[google_lang['言語']=='japanese'])\nprint(google_lang[google_lang['言語']=='chinese (simplified)'])\nprint(google_lang[google_lang['言語']=='chinese (traditional)'])\n#   言語省略       言語\n21   en  english\n   言語省略        言語\n45   ja  japanese\n     言語省略                    言語\n14  zh-cn  chinese (simplified)\n     言語省略                     言語\n15  zh-tw  chinese (traditional)\n```\n<div style=\"width:60%;margin:auto\">{% asset_img google_support.png google_support%}</div>\n\n### ③ パワポを翻訳\n1. パワポの解析はslide→shape→paragraph→runに基づいて行いました。\n- 本記事では最小単位であるrun.textごとのテキスト翻訳とrun一個上のparagraph.textごとのテキスト翻訳を行いました。\n<div style=\"width:100%;margin:auto\">{% asset_img ppt.png ppt%}</div>\n- notesの部分は`slide.notes_slide.notes_text_frame.text`でテキスト本文を指定できます。\n- テーブルの部分は`shape.shape_type == MSO_SHAPE_TYPE.TABLE`を使ってテーブルを絞って、row→cellという構造に基づいて`cell.text_frame.text`を用いて本文を指定できます。\n- Google_Translatorは空白とNoneTypeに対応しないため、`if paragraph.text.isspace() != True and paragraph.text != ''`を使ってそれらを除外しました。\n- `#`が入ってしまうと、通信エラーが生じてしまうらしいので、`str_in = title.text.replace('#','')`を使って翻訳に投げるテキストに入っている`#`を除外しました。\n```\njson_inputs = {\n    'file_path':ファイルのパスをここに記入,\n    'Google': False, #deep-translator\n    'translate': True, #api with Google Apps Script\n    'Linguee':False, #deep-translator\n    'DeepL':False, #deep-translator\n    'notes':True,\n    'source_Google':'ja',\n    'target_Google':'en',\n    'source_translate':'ja',\n    'target_translate':'en',\n    'source_Linguee':'ja',\n    'target_Linguee':'en',    \n    'source_deepl':'JA',\n    'target_deepl':'EN'\n}\n\npath_to_presentation = json_inputs['file_path']\nprs = Presentation(path_to_presentation)\n\ndef ppt_translate(prs):\n    for ns,slide in enumerate(prs.slides): #slide\n        title = slide.shapes.title\n        if title:\n            if json_inputs['Google']:\n                str_in = title.text.replace('#','')\n                title_out = Google_Translator(str_in)\n                sleep(1.5)\n                title.text = title_out\n            elif json_inputs['translate']:\n                str_in = title.text.replace('#','')\n                title_out = translate(str_in)\n                sleep(1.5)\n                title.text = title_out\n            elif json_inputs['deepL']:\n                str_in = title.text.replace('#','')\n                title_out = deepl_translate(str_in)\n                sleep(1.5)\n                title.text = title_out\n        if json_inputs['notes']: #notes\n            if slide.has_notes_slide: \n                if json_inputs['Google']:\n                    str_in = slide.notes_slide.notes_text_frame.text.replace('#','')\n                    notes_out = Google_Translator(str_in)\n                    sleep(1.5)\n                    slide.notes_slide.notes_text_frame.text = notes_out\n                elif json_inputs['translate']:\n                    str_in = slide.notes_slide.notes_text_frame.text.replace('#','')\n                    notes_out = translate(str_in)\n                    sleep(1.5)\n                    slide.notes_slide.notes_text_frame.text = notes_out \n                elif json_inputs['deepL']:\n                    str_in = slide.notes_slide.notes_text_frame.text.replace('#','')\n                    notes_out = deepl_translate(str_in)\n                    sleep(1.5)\n                    slide.notes_slide.notes_text_frame.text = notes_out\n        for nsh,shape in enumerate(slide.shapes):\n            if shape.shape_type == MSO_SHAPE_TYPE.TABLE: #table\n                for row in shape.table.rows:\n                    for cell in row.cells:\n                        if json_inputs['Google']:\n                            str_in = cell.text_frame.text.replace('#','')\n                            cell_out = Google_Translator(str_in)\n                            sleep(1.5)\n                            cell.text_frame.text = cell_out\n                        elif json_inputs['translate']:\n                            str_in = cell.text_frame.text.replace('#','')\n                            cell_out = translate(str_in)\n                            sleep(1.5)\n                            cell.text_frame.text = cell_out\n                        elif json_inputs['deepL']:\n                            str_in = cell.text_frame.text.replace('#','')\n                            cell_out = deepl_translate(str_in)\n                            sleep(1.5)\n                            cell.text_frame.text = cell_out\n#paragraph.textごとのテキスト翻訳\n            if shape.has_text_frame:\n                for np,paragraph in enumerate(shape.text_frame.paragraphs):\n                    if json_inputs['Google']:\n                    # Google_Translatorは空白とNoneTypeに対応しないため、それらを除外する\n                        if paragraph.text.isspace() != True and paragraph.text != '':\n                            str_in = paragraph.text.replace('#','')\n                            str_out = Google_Translator(str_in)\n                            paragraph.text = str_out\n                    elif json_inputs['translate']:\n                        str_in = paragraph.text.replace('#','')\n                        str_out = translate(str_in)\n                        paragraph.text = str_out\n                    elif json_inputs['deepL']:\n                        str_in = paragraph.text.replace('#','')\n                        str_out = deepl_translate(str_in)\n                        paragraph.text = str_out\n#run.textごとのテキスト翻訳\n#                     for rs,run in enumerate(paragraph.runs):\n#                         if json_inputs['Google']:\n#                             str_in = run.text.replace('#','')\n#                             str_out = Google_Translator(str_in)\n#                             run.text = str_out\n#                         elif json_inputs['translate']:\n#                             str_in = run.text.replace('#','')\n#                             str_out = translate(str_in)\n#                             sleep(1.5)\n#                             run.text = str_out\n#                         elif json_inputs['deepL']:\n#                             str_in = run.text.replace('#','')\n#                             str_out = deepl_translate(str_in)\n#                             sleep(1.5)\n#                             run.text = str_out\n        prs.save(os.path.split(path_to_presentation)[1].split('.pptx')[0]+'_en.pptx')\n        print(f'第{ns}スライドは完了')\n```\n- run.textを単位とparagraph.textを単位に翻訳した結果は以下の通りです。\n- paragraph.textのほうが若干ナチュラルと気がしますが、run.textの場合はちゃんとフォントサイトや、太字等を原本のままで保持できるので、場合によって取捨選択できたらと思います。\n\n|run.text|paragraph.text|\n|-|-|\n|<div style=\"width:80%;margin:auto\">{% asset_img run_text.png run_text%}</div>|<div style=\"width:100%;margin:auto\">{% asset_img paragraph_text.png paragraph_text%}</div>|\n\n### ④ Excel,csv,txtファイルを翻訳\n- Excel,csv,txtの場合だとずっと簡単ですが、一応練習するため、以下の関数を書いてみました。\n```\ndef excel_excel_txt(path):\n    if os.path.splitext(path)[1] == '.csv':\n        df = pd.read_csv(path,index_col=0)\n    elif os.path.splitext(path)[1] == '.xlsx':\n        df = pd.read_excel(path,header=None,index_col=0)\n    elif os.path.splitext(path)[1] == '.txt':\n        df = pd.read_table(path,header=None)\n    for i in df.columns:\n        if json_inputs['Google']:\n            i_en = [Google_Translator(str(m)) for m in df[i]]\n            sleep(1.5)\n            df[f'{i}_en'] = i_en\n        elif json_inputs['translate']:\n            i_en = [translate(str(m)) for m in df[i]]\n            sleep(1.5)\n            df[f'{i}_en'] = i_en\n        elif json_inputs['DeepL']:\n            i_en = [DeepL_Translator(str(m)) for m in df[i]]\n            sleep(1.5)\n            df[f'{i}_en'] = i_en\n    df.to_csv(os.path.splitext(json_inputs['file_path'])[0]+'.csv',encoding='utf-8-sig')\n    return df\n```\n\n### ⑤ Next Action\n- 上記コードで実現できたことをWeb ツールにて実現できたら良いかと思います。\n- 現状パワポ内の画像テキストの翻訳はまだできていないので、今後画像の部分もクリアしたいです。\n- word、pdf系のファイルもこちらに取り入れたいと思います。\n\n### Appendix\n1. 中国繁體語⇔簡体語\n- deep-translatorを利用する場合\n```\ntext = '统计学入门'\n\nresult = GoogleTranslator(source = 'auto',target = 'zh-tw').translate(text=text)\nresult\n#'統計學入門'\n```\n- openccを利用する場合\n```\npip install opencc-python-reimplemented\nfrom opencc import OpenCC\ncc = OpenCC('S2t')\nto_convert = '统计学入门'\nconverted = cc.convert(to_convert)\nconverted\n#'統計學入門'\n```\n2. 単語の同義語を取得\n- deep-translatorのLingueeTranslatorを使って、単語の同義語を取得できる\n- LingueeTranslator内の`return_all = True`にすると、該当単語の翻訳した言語の同義語がいくつか出てくる\n```\nword='面白い'\ntranslated_linguee = LingueeTranslator(source = 'ja',target = 'en').translate(word,return_all = True)\ntranslated_linguee \n#['interesting', 'interesting', 'amusing']\n```\n\n\n\n\n\n\n\n","tags":["自動化","ファイル操作","翻訳","パワポ"],"categories":["pythonによる自動化"]},{"title":"find the different files between 2 folders","url":"/Jorey-s-Blog/2021/02/07/find-the-different-files-between-2-files-md/","content":"<!-- toc -->\n---------------\n### 1.課題\n- 一つの圧縮ファイルを解凍して、2つのフォルダを得た。\n\t- A_fullフォルダ:エラーなく解凍でき、ファイル紛失のないフォルダ\n\t- B_errorフォルダ:途中エラーが出てしまい、一部ファイルが紛失してしまったフォルダ\n- この2つのフォルダにそれぞれ８つのサブフォルダがあり、サブフォルダ名は以下の対応関係である。\n\t- A_full:20210201 ~ B_error:a1\n\t- A_full:20210202 ~ B_error:a10\n\t- A_full:20210203 ~ B_error:a11\n\t- A_full:20210204 ~ B_error:a2\n\t- A_full:20210205 ~ B_error:a\n\t- A_full:20210206 ~ B_error:A1\n\t- A_full:20210207 ~ B_error:A19\n\t- A_full:20210208 ~ B_error:S\n\t|A_full|B_error|\n\t|-|-|\n\t|<div style=\"width:70%;margin:auto\">{% asset_img A_full.png A_full%}</div>|<div style=\"width:70%;margin:auto\">{% asset_img B_error.png B_error%}</div>|\n<!-- more -->\n- フォルダ内のファイル名はそれぞれ一致している。\n\t- 20210201\n\t<div style=\"width:50%;margin:auto\">{% asset_img 20210201.png 20210201%}</div>\n\t- a1\n\t<div style=\"width:70%;margin:auto\">{% asset_img a1.png a1%}</div>\n- 探しだしたファイルをフォルダごとに格納する、以下の通りになってほしい。\n\t|フォルダ構造|20210201-a1後の結果|\n\t|-|-|\n\t|<div style=\"width:70%;margin:auto\">{% asset_img new.png new%}</div>|<div style=\"width:100%;margin:auto\">{% asset_img new_20210201.png new_20210201%}</div>|\n\n### 2.A_fullとB_error内のファイル名をマッチする\n<div id=\"kk_full\">\n\n>まずA_fullとB_errorのパスを用意する\n\n</div>\n\n~~~\nkk_full=[k for k,w,t in os.walk('./A_full')]\nww_full=[w for k,w,t in os.walk('./A_full')]\ntt_full=[t for k,w,t in os.walk('./A_full')]\n\nkk_error=[k for k,w,t in os.walk('./B_error')]\nww_error=[w for k,w,t in os.walk('./B_error')]\ntt_error=[t for k,w,t in os.walk('./B_error')]\n\nkk_full\n#['./A_full',\n './A_full\\\\20210201',\n './A_full\\\\20210202',\n './A_full\\\\20210203',\n './A_full\\\\20210204',\n './A_full\\\\20210205',\n './A_full\\\\20210206',\n './A_full\\\\20210207',\n './A_full\\\\20210208']\n\nfile_name_full = [kk_full[i].split('/')[-1].split('\\\\')[-1] for i in range(1,len(kk_full))]\nfile_name_error = [kk_error[i].split('/')[-1].split('\\\\')[-1] for i in range(1,len(kk_error))]\n\nfile_name_error\n#['a', 'A1', 'a1', 'a10', 'a11', 'A19', 'a2', 'S']\n~~~\n>並び替えルールを定義する(`＠ながた`さんに感謝！)\n>考え方としては、A,a,SをすべてUnicode値に変換することによって、順位を人為的に付けさせる。\n~~~\ndef rank_filename(value):\n    if len(value) <= 1:\n        for i in count():\n            try:\n                _ = chr(i)\n            except ValueError:\n                max_char_number = i-1\n                break\n        left,right = value.lower(),chr(max_char_number)    \n    else:\n        left,right = value[0],''.join(list(value)[1:])\n        #A-Zのほうがa-zより小さいので、それを入れ替える\n        left_after = ''\n        for c in left:\n            if 'a' <= c <= 'z':\n                left_after += chr(ord('A')+(ord(c)-(ord('a')))+5)\n            elif 'A' <= c <= 'Z':\n                left_after += chr(ord('a')+(ord(c)-(ord('A')))+5)\n            else:\n                left_after += c\n        left = left_after      \n    return left,right\n~~~\n>定義したrank_filename関数をsortのkeyに応用する。\n~~~\nfile_name_sort = sorted(file_name_error,key=rank_filename)\nfile_name_sort\n#['a1', 'a10', 'a11', 'a2', 'a', 'A1', 'A19', 'S']\n~~~\n>参考①A,a,S,-などのUnicode値は以下の通りである。\n```\nord('A'),ord('S'),ord('a'),ord('s'),ord('-')\n#(65, 83, 97, 115, 45)\n```\n### 3.すべてのファイルのフルパスを取得する\n```\nfolder_path_sorted = ['./B_error/'+i for i in file_name_sort]\nfolder_path_full = ['./A_full/'+i for i in file_name_full]\n\ndef make_full_path(list_):\n    full_path = []\n    for i in list_:\n        for v,k,m in os.walk(i):\n            path_tmp = [os.path.join(v,x) for x in m]\n            full_path.append(path_tmp)\n    return full_path\n\nfull_path_sorted = make_full_path(folder_path_sorted)\nfull_path_all = make_full_path(folder_path_full)\n\nfull_path_sorted[7][0]\n#'./B_error/S\\\\全員に１レッスンプレゼント！仙台校移転記念♪トライアルキャンペーン開催！！.eml'\n```\n>念の為、BフォルダとAフォルダの対応関係を一つの辞書に格納した。\n```\nfile_path_dict=dict(zip(folder_path_sorted,folder_path_full))\nfile_path_dict\n#{'./B_error/a1': './A_full/20210201',\n './B_error/a10': './A_full/20210202',\n './B_error/a11': './A_full/20210203',\n './B_error/a2': './A_full/20210204',\n './B_error/a': './A_full/20210205',\n './B_error/A1': './A_full/20210206',\n './B_error/A19': './A_full/20210207',\n './B_error/S': './A_full/20210208'}\n```\n### 4-1.ファイル名をもとに比較する\n>一度サブフォルダの順序を変えたので、Bフォルダのパス情報を新たに格納する\n```\nkk_error_sorted=[k for i in folder_path_sorted for k,w,t in os.walk(i)]\n# ww_sorted=[w for i in folder_path_sorted for k,w,t in os.walk(i)]\ntt_error_sorted=[t for i in folder_path_sorted for k,w,t in os.walk(i)]\n```\n[すでに処理済み👇](#kk_full)\n```\nkk_full=[k for k,w,t in os.walk('./A_full')]\nww_full=[w for k,w,t in os.walk('./A_full')]\ntt_full=[t for k,w,t in os.walk('./A_full')]\n\n#フォルダごとの差分を取り出す\nlist_ = [[]]*8\nfor i in range(len(tt_error_sorted)):\n    list_.append(list(set(tt_full[i+1])-set(tt_error_sorted[i])))\nlist_\n[[],\n [],\n [],\n [],\n [],\n [],\n [],\n [],\n ['Nintendo Switch『モンスターハンターライズ』の最新映像が公開。 _ トピックス _ Nintendo.html',\n  'The savings are clear.eml',\n  '【Nintendo Switch】期間限定セール・新作ほか、ソフトのご紹介.eml'],\n ['Your question of the week from Vocabulary.com.eml',\n  '東京2020大会：オリンピック＆パラリンピックの世界へ.eml'],\n ['新年のご挨拶.eml', 'もっちもっちの弾力に、脈打つ熟成香。魅惑のブランデーあんぱん.eml'],\n ['ソフトウェアを安全に保護するためのより良い方法.eml',\n  'ゴーストライター騒動から7年 新垣隆の「覚悟」.eml',\n  'Communication trends of 2020.eml'],\n ['東京2020大会：オリンピック＆パラリンピックの世界を知り尽くそう.eml', '宇賀なつみ、思わず息をのむ.eml'],\n ['深度学习之路（一）：用LSTM网络做时间序列数据预测 - 简书.html'],\n ['Our cool-productivity-killer newsletter.eml'],\n ['明智光秀、最後の夜は…….eml']]\n\n dict_path = dict(zip(kk_full[1:],list_[8:]))\n```\n### 4-2.差分のファイルを新しくフォルダごとに保存する\n```\nsave_dir='./new/'\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\n\nfor i in kk_full[1:]:\n#     print(i.split('/')[1].split('\\\\')[1])\n    for m in dict_path[i]:\n        from_path = os.path.join(i,m)\n#         print(from_path)\n        to_path = os.path.join(save_dir,i.split('/')[1].split('\\\\')[1])\n        if not os.path.isdir(to_path):\n            os.makedirs(to_path)\n        copy(from_path,to_path)\n```\n### 5-1.内容ともとに比較する\n>今回のファイルにはeml,html(サンプルにはないけど、ほんとはmsgファイルも)が入っているため、以下の内容抽出するための関数を作成した。\n~~~\ndef extract_text(list_):\n    text = []\n    file_id = []\n    for i in list_:\n        if i.split('.')[-1] == 'eml':\n            file_id.append(i)\n            with open(i,'r',encoding = 'utf-8-sig') as f:\n                text_ = f.read()\n                text.append(text_)\n#                 print('extracted',i)\n        elif i.split('.')[-1] == 'msg':\n            file_id.append(i)\n            subject = extract_msg.Message(i).subject\n            date = extract_msg.Message(i).date\n            body = extract_msg.Message(i).body\n            text_ = subject+date+body\n            text.append(text_)\n#             print('extracted',i)\n        elif i.split('.')[-1] == 'html':\n            file_id.append(i)\n            htmlfile = open(i, 'r', encoding='utf-8-sig')\n            htmlhandle = htmlfile.read()\n            soup = BeautifulSoup(htmlhandle, 'html.parser')\n            text.append(soup.text)\n#             print('extracted',i)\n            \n#         dict_ = dict(zip(file_id,text))\n    return file_id,text         \n~~~\n>上記関数を使って、フォルダごとの内容を比較し、フルパスを取得する\n~~~\ndiffer_ids=[]\nfor i in range(len(full_path_all)):\n    file_id,text_sorted =extract_text(full_path_all[i])\n    file_id_error,text_error =extract_text(full_path_sorted[i])\n    dict_ = dict(zip(text_sorted,file_id))\n    differ_id = []\n    for m in text_sorted:\n        if m not in text_error:\n            differ_id.append(dict_[m])\n    differ_ids.append(differ_id)\n\ndiffer_ids[1]\n#['./A_full/20210202\\\\Your question of the week from Vocabulary.com.eml',\n './A_full/20210202\\\\東京2020大会：オリンピック＆パラリンピックの世界へ.eml']\n~~~\n### 5-2.差分を保存する\n~~~\nsave_dir='./new'\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nfor i in differ_ids:\n    for m in i:\n#         print(m.split('/')[2].split('\\\\')[0])\n        to_path = os.path.join(save_dir,m.split('/')[2].split('\\\\')[0])\n        if not os.path.isdir(to_path):\n            os.makedirs(to_path)\n        copy(m,to_path)\n~~~\n### appendix フォルダ名を変更する\n>投げたフォルダ(pathで指定)内のサブフォルダ名をname_listの順序通りに名前を変更させる。\n~~~\ndef rename(path):\n    i=0\n    name_list=['kaggle','tableau','pytorch'] #自由設定できる\n    FileList = os.listdir(path)\n    for files in FileList:\n        oldDirPath = os.path.join(path, files)\n        if os.path.isdir(oldDirPath):\n            rename(oldDirPath)            \n            newDirPath=os.path.join(path,name_list[i])\n            os.rename(oldDirPath, newDirPath)\n            print(files +' has changed as '+newDirPath)\n            i += 1\n~~~\n### Next\n- すでに同じ機能を果たしている無料ツールがあるらしいので、ユーザーが使いやすいため、webツールを作れるといいなと思います。\n- 今回は単にeml,msg,htmlを読み込んだだけなので、今度細かくテキスト文書を抽出したいと思います。\n\n21","tags":["自動化","ファイル操作"],"categories":["pythonによる自動化"]},{"title":"markdown記法","url":"/Jorey-s-Blog/2021/02/03/markdown記法/","content":"<!-- toc -->\n---------------\n<div id=\"title1\">\n\n### 1.インラインでコードを表示する\n</div>\n\n\nこのように`code`をバッククオートを表現したいコードの間に入れるとできる\n~~~\nこのように`code`をバッククオートを表現したいコードの間に入れるとできる\n~~~\n\n### 2.ページ内リンク\n- 方法1:id を指定して移動させる(Markdown/html両方ともOK\n<font size=2.5>**id名は英語ではないと、位置ずれてしまう恐れがあるので、要注意**</font>\n\n~~~\n#出発箇所：\n[contents](#id名)\n\n#目的箇所：\n<div id=\"id名(任意)\"></div>\n~~~\n- 方法2:ヘッダーの名前を指定して移動する(Markdown記法のみ)\n<font size=2.5>**この方法は本記事で実現できなかった**</font>\n\n~~~\n#出発箇所\n[contents](#1.インラインでコードを表示する)\n#目的箇所\n指定されたヘッダー\n~~~\n[Sample:方法1を使って**1.インラインでコードを表示する**へ移動](#title1)\n\n","tags":["markdown","記法","日々の積み上げ"],"categories":["日々の積み上げ"]},{"title":"Pythonによる自動化","url":"/Jorey-s-Blog/2021/01/25/Pythonによる自動化/","content":"<!-- toc -->\n---------------\n## 一、複数のcsvをExcelのSheetに保存する\n### 1.目的\n<font size=3>\n\n>同じフォルダ内のcsvファイルに情報を追加し、合併する\n>フォルダごとの情報を一枚のExcel表にシートごとにまとめる\n\n</font>\n\n### 2.事前説明\n>①使ったモジュール\n~~~\n\timport os\n\timport pandas as pd\n\timport datetime\n\timport py7zr\n\timport openpyxl\n~~~\n>②データ構造\n>>1_毎日同じ構造のデータが生成すると想定する\n~~~\n20200701.7z\n▲フォルダ1\n\t■csvファイル1\n\t■csvファイル2\n▲フォルダ2\n\t■csvファイル1\n\t■csvファイル2\n▲フォルダ3\n\t■csvファイル1\n\t■csvファイル2\n~~~\n>>2_元データの中身\n\n<!-- more -->\n---------------\n\n<table>\n<tr><th>20200701.csv</th><th>log_20200701.csv</th></tr>\n<tr><td>\n\n|開始時間|サイズ(Byte)|\n|-|-|\n|2020/7/1  0:02:00|0|\n|2020/7/1  0:06:00|32567889| 　　\n|2020/7/1  0:08:00|17869027|\n|2020/7/1  0:10:00|3170165999|\n...\n\n</td><td>\n\n|開始|処理時間|件数\n|-|-|-|\n|2020/7/1  0:02:00|0:00:00|0|\n|2020/7/1  0:06:00|02:27.1|32|\n|2020/7/1  0:08:00|02:04.0|0|\n|2020/7/1  0:10:00|0:00:00|-20|\n\n</td></tr> </table>\n\n>>3_希望結果\n\n<div style=\"width:70%;margin:auto\">{% asset_img result.png result%}</div>\n\n### 3.解凍\n\n<font size=3>\n\n>①圧縮されたデータを解凍する\n\n</font>\n\n~~~\nzipfilename = './20200701.7z'\nfiles = py7zr.SevenZipFile(zipfilename,'r')\nfiles.extractall(path = ｒ'') #ここ解凍後フォルダのpathを空白にすれば、余計なディレクトリを生成しなくても済む\nfiles.close()\n~~~\n<font size=3>\n\n>②解凍後フォルダ内のパスをリストに格納する\n\n</font>\n\n~~~\npath = zipfilename[2:10]\ndef read_file(path):\n    mm = [m for m,k,w in os.walk(path)]\n    kk = [k for m,k,w in os.walk(path)]\n    ww = [w for m,k,w in os.walk(path)]\n    file_path_1 = []\n    file_path_2 = []\n    for i in range(1,len(mm)):\n        path_1 = mm[i] + '/' + ww[i][0]\n        path_2 = mm[i] + '/' + ww[i][1]\n        file_path_1.append(path_1)\n        file_path_2.append(path_2)\n    return file_path_1,file_path_2\n~~~\n\n### 4.csvの操作\n<font size=3>\n\n>①ファイルを読み込み、「件数」列は-20に等しい行を削除\n\n</font>\n\n~~~\ndef make_dataframe(file_path_1,file_path_2):\n    df_1 = [pd.read_csv(file_path_1[k],encoding='cp932') for k in range(len(file_path_1))]\n    df_2 = [pd.read_csv(file_path_2[k],encoding='cp932') for k in range(len(file_path_2))]\n    df_2 = [k[k['件数'] != -20].reset_index() for k in df_2]\n    df_2 = [k.loc[:,['開始','処理時間','件数']] for k in df_2]\n    return df_1,df_2\n~~~\n<font size=3>\n\n>②読み込んだ２つのdataframeを合併する\n>③サイズ(Byte)列の値をサイズ(MB)に換算する\n>④文字列である「処理時間」列をdatetime.time型(hour,minute,second)のデータに変える\n>⑤second単位に統一し、sumを求めたら、datetime型に戻らせる\n>⑥諸々の集計結果を最後の一行に追加しておく\n\n</font>\n\n~~~\ndef make_file(df_1,df_2):\n    df = pd.concat([df_2,df_1],axis=1)\n    df['サイズ(MB)'] = df['サイズ(Byte)']/(1024**2)\n    \n    ii = [pd.to_datetime(i).time() for i in df_2['処理時間']]\n    time_to_second = []\n    for i in ii:\n        if i.microsecond: #i.microsecond:int型\n            s = '%06d' % i.microsecond　#int型を文字列に変更する際、頭の0をなくさないようにする\n            if int(s[0]) >= 5:\n                seconds = i.hour*3600 + i.minute*60+ i.second +1\n                time_to_second.append(seconds)\n            else:\n                seconds = i.hour*3600 + i.minute*60+ i.second\n                time_to_second.append(seconds)\n        else:\n            seconds = i.hour*3600 + i.minute*60+ i.second\n            time_to_second.append(seconds)\n    m,s = divmod(sum(time_to_second),60)\n    h,m = divmod(m,60)\n    sum_time='{0}:{1:02d}:{2:02d}'.format(h,m,s)\n    \n    sum_num = sum(df['件数'])\n    sum_size = sum(df['サイズ(Byte)'])\n    sum_size_MB = sum_size/(1024**2)\n    \n    df.loc['sum'] = ['',sum_time,sum_num,sum_size/1024**3,sum_size,sum_size_MB]\n    \n    return df\n~~~\n### 5.excelに書き込む\n~~~\ndef combine_to_excel(list_file_1,list_file_2,list_file_3):\n    list_name = ['list_file_1','list_file_2','list_file_3']\n    writer = pd.ExcelWriter('20200701.xlsx')\n    for i in list_name:\n        eval(i).to_excel(excel_writer = writer,sheet_name = i,index=False)\n    writer.save()\n    writer.close()\n~~~\n\n### 6.残り課題\n>pyファイルに変えておきたい。\n\n## 二、2つのフォルダに差になったファイルを新しいフォルダにコピーする\n### 1.差になったファイルを探し出す\n~~~\nimport os\nfrom shutil import copy\n\nll=[l for l,m,n in os.walk('./')]\nnn = [n for l,m,n in os.walk('./')]\n\n#多めにあるフォルダを判定\nlen(nn[2]) > len(nn[3])\n#差になったファイルを判定\nfile = [i for i in nn[2] if i not in nn[3]]\n#差になったファイルのパスをつける\npath=[ll[2]+'/'+i for i in file]\n~~~\n### 2.新しいフォルダにコピー\n~~~\nsave_dir = './data_new'\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nfor i in range(len(path)):\n    copy(path[i],save_dir) \n~~~\n## 三、フォルダ内のファイルを一括に読み込み、特定のファイルのみ結合する\n### 1.ファイルを一括に読み込み\n~~~\nimport pandas as pd\nimport os\nimport numpy as np\n\ndatas = []\nfor info in os.listdir(\"フォルダのパス\"):\n    domain = os.path.abspath(r\"フォルダのパス\")\n    info = os.path.join(domain,info)\n    data = pd.read_csv(info,encoding ='cp932')\n    datas.append(data)\n~~~\n### 2.特定のファイルのみ結合\n>indexで指定する\n~~~\ndatas_tmp = []\nfor i in range(len(datas)):\n    datas_tmp.append(datas[:i] + datas[i+1:])\n~~~\n\n<div style=\"width:80%;margin:auto\">{% asset_img datas_tmp.png datas_tmp%}</div>\n\n## 四、フラグつけ\n>あるデータ群に対してランダムに10件HOTフラグに付け、残りは全部NOTフラグにつける\n>上記のdatas_tmpをもとに実行する\n~~~\ndf=pd.concat(datas_tmp[2])\ndata2=np.random.randint(3,100,(100,3))\ndf_data2=pd.DataFrame(data2,columns=['挿入','公式','データ'])\ndf_data2\nddf=pd.concat([df.reset_index()[['挿入','公式','データ']],df_data2],axis=0)\n~~~\n>ddfを今回フラグつけてあげたいデータとする\n\n<div style=\"width:80%;margin:auto\">{% asset_img 四-1.png 四-1%}</div>\n\n~~~\nimport random\nnum=92\nlist_not=[0]*num\nnum_hot=10\nlist_hot=[1]*num_hot\n\nlist_all=list_not+list_hot\nrandom.shuffle(list_all)\n\nddf['HOT']=list_all\nddf.loc[ddf['HOT']==1,'NOT']=0\nddf.loc[ddf['HOT']==0,'NOT']=1\n~~~\n<div style=\"width:80%;margin:auto\">{% asset_img 四-2.png 四-2%}</div>\n\n## 五、辞書によるリスト要素の重複削除\n<div style=\"width:80%;margin:auto\">{% asset_img 五_1.png 五_1%}</div>\nsetを使用するには、hashableの対象ではないと効かないので、そのままsetを使用することで、エラーが出てしまいます。\n\n1. 回答①\n    - dict内すべての要素を一度重複があるかどうかを判断してリストに格納する方法です。\n\n```\nresult = []\ndef unduplicate(result,data):\n    if data not in result:\n        result = result+[data]\n    return result\nfor i in test:\n    result = unduplicate(result,i)\nresult\n#[{'a': 1}, {'a': 3}, {'b': 2}]\n```\n2. 回答②\n    - 回答①と本質的に変わらないのですが、reduceを使うことで、やや効率よくなる方法です。\n    -[reduceの基本構文](https://techacademy.jp/magazine/18844)\n        - reduce(function, iterable, initializer)\n        - 第1引数には2つの引数を取るfunctionを指定する必要があります。(必須)\n        - 第2引数にはiterableを指定する必要があります。(必須)\n        - 第3引数にはオプションですが、指定された場合、iterableの先頭に置かれ、iterableが空の場合のデフォルト値になります。また第1引数にはfunctionの代わりにラムダ式を使用する事もできます。\n\n```\nfrom functools import reduce \ndef delete_duplicate(data):\n    func = lambda x,y :x +[y] if y not in x else x\n    data = reduce(func,[[],] +data)\n    return data\ndelete_duplicate(test)\n#[{'a': 1}, {'a': 3}, {'b': 2}]\n```\n3. 回答③\n    - dictを無理矢理に文字列に変更し、set関数を使って重複削除を行ったら、eval関数を使ってdictに戻させる方法です。\n    - [eval関数の基本構文](https://techacademy.jp/magazine/40662#1)\n        - eval(\"式\", globals=None, locals=None)\n        - 第1引数の”式”には、ダブルクオーテーションで囲った式を代入します。\n        - 第2引数と第3引数には、任意でそれぞれグローバル、ローカルの「名前空間」を指定しますが、これらはeval関数の基本的な使い方については理解するにはそれほど重要ではないので、詳しい説明はここでは省略します。\n        - eval関数の返り値としては、第1引数で指定した式の値が返されます。\n\n```\ndef delete_duplicate(data):\n    dict_ed = set([str(i) for i in data])\n    data = [eval(i) for i in dict_ed]\n    return data\ndelete_duplicate(test)\n#[{'b': 2}, {'a': 3}, {'a': 1}]\n```\n4. 回答④ ★★★☆☆\n    - 要素であるdictをtuple形式に変更し、setを使って重複削除し、dict関数を使ってtuple形式をdict形式に戻させる方法です。\n    - この方法は2重dictの要素が入ってあれば適用しないので、その場合回答③を使えればと思います。\n\n```\ndata = [dict(t) for t in set([tuple(i.items()) for i in test])]\ndata\n#[{'b': 2}, {'a': 1}, {'a': 3}]\n```\n\n","tags":["Excel","自動化","ファイル操作"],"categories":["pythonによる自動化"]},{"title":"Windows環境上のAnacondaにMecabのユーザー辞書とNEologd辞書のインストール","url":"/Jorey-s-Blog/2021/01/22/Mecab in Windows/","content":"<font size=2>**本記事では、MeCabとは、公式サイトからダウンロードしてきたMeCabのことを言う、Anaconda上のMeCabはAnaconda上でダウンロードしてきたMeCabを指します。**</font>\n\n<font size=3>\n\n>1)ネット上色々記事を参考してましたが、ほぼBoW環境をもとに紹介されています。\n>2)Windows環境+Anaconda(Python3.7)上では、下記記事の記載されている通り実行したらうまく行けましたので、メモとして残したいと思います。\n\n</font>\n<font size=4.5>\n\n[**WindowsでMeCab+NEologdをインストールする**](http://qd-suriken.com/2020/04/22/widows%E3%81%A7mecabneologd/)\n\n</font>\n\n<!-- more -->\n---------------\n<!-- toc -->\n---------------\n\n### 1.MeCabをインストールする\n>1)Jupyter上でpip install mecab-python3ではなく、公式サイトからMeCabをダウンロード、インストールすることです。\n>2)公式サイト：[Mecab/download](http://taku910.github.io/mecab/#download)\n>3)自分はBinary package for MS-Windowsをダウンロードしております。\n\n<div style=\"width:70%;margin:auto\">{% asset_img mecab_download.png mecab_download%}</div>\n\n>4)インストーラの案内通りでインストールを行えばいいですが、辞書の文字コードの選び(自分は**utf-8**)とインストーラ先の指定(自分は**C:\\Program Files (x86)\\MeCab**)は後ほど参考になるので、一旦覚えといたほうがいいと思います。\n>5)インストールしたら、C:\\Program Files (x86)\\MeCab\\dic\\ipadicにchar.bin、dicrc、matrix.bin、sys.dic、unk.dicというファイルが有れば問題がないかと思います。\n\n### 2.環境変数のPATHに追加\n>C:\\Program Files (x86)\\MeCab\\binを環境変数のPATHに追加します。\n\n### 3.Anaconda jupyter上のmecabをインストールする\n以下いくつかの方法どれでも行けると思います。\n>pip install mecab\n>pip install mecab-python-windows(python3.7&以前のバージョン)\n>pip install mecab-python3(python3.8のバージョン)\n\n### 4.libmecab.dll（必須ではないかも）\n>1)MeCabのインストール先のlibmecab.dll(自分は**C:\\Program Files (x86)\\MeCab\\bin\\libmecab.dll**)を(Anacondaのインストール先)/Lib/site-packages(自分のは**D:\\anaconda\\envs\\mecab\\Lib\\site-packages**)にコピーする\n>2)ただ偶然にsite-packages内のlibmecab.dllを削除したとしても、特にエラー生じてなかったので、このステップいらないかもしれないですね。\n\n### 5.NEologd辞書の登録\n#### A.NEologdのダウンロード\n>1)[NEologd](https://github.com/neologd/mecab-ipadic-neologd)\n>2)ダウンロードできたNEologdに(\\mecab-ipadic-neologd\\seed)いくつかのcsv圧縮ファイルがあります。\n>3)csvの圧縮ファイルを解凍します。\n\n<div style=\"width:70%;margin:auto\">{% asset_img NEologd_xz.png NEologd_xz%}</div>\n\n>csvファイルの解凍は色々方法がありますが、自分は以下の方法で行いました。\n\n\timport lzma\n\timport shutil\n\timport os\n\n\tfiles = []\n\tfor root,dirs,file in os.walk(r'〇〇\\MeCab\\mecab-ipadic-neologd\\seed'): #csv.xz所在しているファイルのディレクトリ\n\t    files.append(file)\n\tfor i in files[0]:\n    input_file = './mecab-ipadic-neologd/seed/'+i\n    destination_dir = 'C:/Users/jorey/python/'\n    with lzma.open(input_file,'rb') as readfile:\n        with open(destination_dir+i.split('.xz')[0],'wb') as writefile:\n            data = readfile.read()\n            while data:\n                writefile.write(data)\n                data = readfile.read()\n\n<div style=\"width:40%;margin:auto\">{% asset_img NEologd_xz_csv.png NEologd_xz_csv%}</div>\n\n#### B.NEologd辞書の登録\n>1)Anaconda Promptを開き、＞activate mecab（mecabはAnaconda上で使用している環境の名前です。）を打って、Anaconda環境に入る\n>2)csvファイル所在しているフォルダに移動（自分のは**C:\\Users\\〇〇Desktop\\MeCab\\dic\\mecab-ipadic-neologd\\seed**です）\n>3)以下のコードを打つと、csvファイルをdic辞書に作成する\n>>1_:C:\\Program Files (x86)\\MeCab\\bin\\mecab-dict-index.exeはdic辞書を作成するためのtool\n>>2_:C:\\Program Files (x86)\\MeCab\\dic\\ipadicはMecab上インストールしたらデフォルトあるファルダ\n>>3_:〇〇.dic 〇〇.csvはどういうcsvファイルをdic辞書に作成するかを指定する\n>>4_:-f utf-8 はcsvファイルの文字コード　(NEologd辞書のcsvを解凍したらutf-8になるので、ここもutf-8に指定しました)\n>>5_:-t utf-8 は辞書ファイルの文字コード（Mecabインストールするとき自分はutf-8を選んだので、ここutf-8に指定しました）\n>>6_:簡単にまとめていうと、【mecab-dict-index -f <csvファイルの文字コード> -t <辞書ファイルの文字コード>】のイメージですね\n\n\t\"C:\\Program Files (x86)\\MeCab\\bin\\mecab-dict-index.exe\" -d “C:\\Program Files (x86)\\MeCab\\dic\\ipadic” -u 〇〇.dic -f utf-8 -t utf-8 〇〇.csv\n>4)C:\\Program Files (x86)\\MeCab\\dicの下にNEologdのフォルダを作成する。\n>上記作成できた〇〇.dic辞書(例：mecab-user-dict-seed.20200910.dic)をC:\\Program Files (x86)\\MeCab\\dic\\NEologdに入れる。\n\n<div style=\"width:70%;margin:auto\">{% asset_img NEologd_dic.png NEologd_dic%}</div>\n\n####　C.mecabrcを修正\n>1)C:\\Program Files (x86)\\MeCab\\etc内のmecabrcを開き、以下2行を追加します。(;と空白なし)\n>2)デフォルトのあるdicdirとuserdicの2行を消すか、行頭に;を消せればいいと思います。\n>3)userdic複数追加したい場合、','を使って複数追加すれば大丈夫です。\t\n\n\t; dicdir =  $(rcpath)\\..\\dic\\ipadic\n\t; userdic = /home/foo/bar/user.dic\n\tdicdir = C:\\Program Files (x86)\\MeCab\\dic\\ipadic　\n\tuserdic = C:\\Program Files (x86)\\MeCab\\dic\\NEologd\\mecab-user-dict-seed.20200910.dic #生成したdicファイルのディレクトリ\n\t#複数追加したい場合\n\tuserdic = C:\\Program Files (x86)\\MeCab\\dic\\NEologd\\mecab-user-dict-seed.20200910.dic,C:\\Program Files (x86)\\MeCab\\dic\\NEologd\\neologd-adjective-exp-dict-seed.20151126.dic\n\n####　D.AnacondaでJupyter Notebookを開き、確認\n>MeCab.Tagger()に\"-r C:\\Program Files (x86)\\MeCab\\etc\\mecabrc\"そのまま入力したら機能しません。\n>>①\\はエスケープ文字に認識されてしまうらしいので、\\を/または\\\\\\に変更する必要あります。\n>>②MeCabはディレクトリ内空白が入ってしまうと認識しません.Program Files (x86)Ni空白入ってしまってるので、ここmecabrcをdesktopに移動したら、機能しました。\n\n\timport MeCab\n\ttagger = MeCab.Tagger('-r C:/Users/jorey/Desktop/mecabrc')\n\tprint(tagger.parse('中島裕翔'))\n\t##中島裕翔\t名詞,固有名詞,人名,一般,*,*,中島裕翔,ナカジマユウト,ナカジマユウト\n\tEOS\n\n### 6.ユーザー辞書の登録\n#### A.ユーザー辞書の作成\n>ユーザー辞書の作成に関しては、こちらの記事では細かく記載しましたので、結構参考になりました。\n>>[エントリのフォーマット (活用しない語)](http://taku910.github.io/mecab/dic.html)\n>>[Windows環境でMeCabのユーザ辞書に単語を追加する](https://ameblo.jp/lucifep2525/entry-12406376509.html)\n\n\t表層形,左文脈ID,右文脈ID,コスト,品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音\n\t#自分の:\n\t何度も,*,*,20,名詞,固有名詞,名詞,*,*,*,何度も,ナンドモ,ナンドモ\n\t風息,*,*,20,名詞,固有名詞,人名,一般,*,*,風息,フーシー,フーシー\n>保存するとき、文字コードの選びは後ほどdict作成するとき必要があるので、メモしといたほうがいいと思います。\n\n<div style=\"width:50%;margin:auto\">{% asset_img user_foo.png user_foo%}</div>\n\n#### B.ユーザー辞書csvからユーザー辞書dicに作成する\n>やり方は**5.NEologd辞書の登録>B.NEologd辞書の登録**と一緒です。\n\n\t\"C:\\Program Files (x86)\\MeCab\\bin\\mecab-dict-index.exe\" -d “C:\\Program Files (x86)\\MeCab\\dic\\ipadic” -u foo.dic -f utf-8 -t utf-8 foo.csv\n<div style=\"width:50%;margin:auto\">{% asset_img foo_dic.png foo_dic%}</div>\n\n#### C.mecabrcにパス追加\n>やり方は**5.NEologd辞書の登録>C.mecabrcを修正**と一緒です。\n\n\t; dicdir =  $(rcpath)\\..\\dic\\ipadic\n\t; userdic = /home/foo/bar/user.dic\n\n\tdicdir = C:\\Program Files (x86)\\MeCab\\dic\\ipadic-utf8\n\tuserdic = C:\\Program Files (x86)\\MeCab\\dic\\NEologd\\mecab-user-dict-seed.20200910.dic,C:\\Program Files (x86)\\MeCab\\dic\\userdic\\foo.dic\n\n### 7.Anacondaで確認\n>こうしてAnaconda上でも登録したNEologd辞書とユーザー辞書が反映されたと確認できました。\n>こちらを用いて、色々の自然言語処理にて活用したいと思います。\n\n<div style=\"width:60%;margin:auto\">{% asset_img jupyter_end.png jupyter_end%}</div>\n\n\n\n\n\n\n\n\n","tags":["自然言語処理","MeCab","他人記事参照"],"categories":["日々の積み上げ"]},{"title":"paddlepaddleを使って写真の背景を切り捨てる","url":"/Jorey-s-Blog/2021/01/16/05_paddlehub/","content":"\n\n<!-- toc -->\n---------------\n## 1.deeplabv3p_xception65_humanseg\n<font size=3>\n\n>写真の主体だけを切り抜き、ワードクラウドの背景にしたいと思い、Baiduのディープラーニングフレームワークを使ってみました。\n>今回はdeeplabv3p_xception65_humansegというモデルのみ使ってみましたが、paddlepaddleにはNLPモデル、画像処理モデル、音声認識モデル等様々入っているので、今後色々試したいと思います。\n\n>今回の処理対象写真は以下の5枚を用意しました。\n>>動物系の黒猫である羅小黒、人間姿になった羅小黒、人間ではあるが、バイバイしている後ろ姿しか見せていないシャオヘイの師匠であるムゲン、錦戸亮の進化した成人状態と王道のちび亮時期\n\n</font>\n\n<div style=\"width:100%;margin:auto\">{% asset_img object.png object%}</div>\n<!-- more -->\n<!-- --------------- -->\n<font size=3>まず必要なものをダウンロードする</font>\n\n\tpython -m pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple\n\t#paddlepaddle 1.8.5 requires opencv-python<=4.2.0.32\n\n\tpip install -i https://mirror.baidu.com/pypi/simple paddlehub\n\n<font size=3>以上無事にインストールできたら、paddlepaddleのpaddlehubツールが使えるようになる</font>\n\t\n\timport os, paddlehub as hub\n\n\t#モデルをロードする\n\thuseg = hub.Module(name = 'deeplabv3p_xception65_humanseg')\n\n\t#用意した写真をimgというフォルダに格納している\n\tpath = r'./img/'\n\tfiles = [path +i for i in os.listdir(path)]\n\n\t#visualization=Falseになる場合、画像が生成してくれないので、要注意\n\tresult = huseg.segmentation(data = {'image':files},visualization=True,output_dir=path)\n\n<font size=3>\n\n>結果は以下の通りです。\n>人間の顔があると、精度がまあまあ良いですが、猫の顔が全然無理でしたね。\n\n</font>\n<div style=\"width:100%;margin:auto\">{% asset_img result_array.png result_array%}</div>\n<div style=\"width:100%;margin:auto\">{% asset_img object_ed.png object_ed%}</div>\n\n## github\n[paddlepaddle model github](https://github.com/PaddlePaddle/models)\n[paddlepaddle github](https://github.com/PaddlePaddle/Paddle)\n\n\n","tags":["画像処理"],"categories":["画像処理"]},{"title":"2021年1月SQL練習課題回答","url":"/Jorey-s-Blog/2021/01/12/2021年1月SQL練習課題回答/","content":"\n説明：SQLの操作はpsycopg2を使って、Python上での操作を行いました。\n\n<!-- toc -->\n---------------\n### 1.2021年1月12日\n>勉強したSQL構文:\n>>OVER(PARTITION BY column1 ORDER BY column2)\n\n<font size=3>①データ挿入</font>\n\n\t%%sql\n\tCREATE TABLE T0104(\n\t\tID INT,\n\t\tNAME VARCHAR(10),\n\t\tNUM INT)\n\tINSERT INTO T0104 VALUES(1,'A',1);\n\tINSERT INTO T0104 VALUES(2,'A',2);\n\tINSERT INTO T0104 VALUES(3,'A',6);\n\tINSERT INTO T0104 VALUES(4,'A',4);\n\tINSERT INTO T0104 VALUES(5,'A',3);\n\tINSERT INTO T0104 VALUES(6,'B',2);\n\tINSERT INTO T0104 VALUES(7,'B',8);\n\tINSERT INTO T0104 VALUES(8,'B',2);\n<div style=\"width:70%;margin:auto\">{% asset_img 20200104_1.png 20200104_1%}</div>\n<!-- more -->\n<font size=3>②条件を設定し、データを絞り込む</font>\n\n\t%%sql\n\n\tselect A.id,A.name,A.num from(\n    select T.ID,T.NAME,T.NUM,\n    sum(NUM) OVER(PARTITION BY NAME ORDER BY ID)*1.0 a,\n    sum(NUM) OVER(PARTITION BY NAME ORDER BY ID) *1.0 b,\n    sum(NUM) OVER(PARTITION BY NAME)*1.0 c from T0104 as T\n    ) A\n    where A.a/A.c > 0.6;\n<div style=\"width:70%;margin:auto\">{% asset_img 20200104_2.png 20200104_2%}</div>\n\n### 2.2021年1月13日\n>勉強したSQL構文:\n>>1.column1をもとに分割し、column2をもとにそれぞれ順番そろえて、順位を新しい列につけて行く\n>>>row_number() over(partion column1 order by column2)\n\n>>2.coalesce関数\n>>>coalesce(A,B,C...):()の中に一つの非NULLの値が出力される\n>>>参考記事:[CASE式で条件分岐をSQL文に任せる](https://qiita.com/sfp_waterwalker/items/acc7f95f6ab5aa5412f3)\n\n>>3.条件分岐構文:検索case式\n\n\tcase \n\t\twhen column1 = '' then 結果1\n\t\twhen column1 = '' then 結果2\n\t\telse ●●\n\tEND\n\n<font size=3>①まず条件分岐で正負の検索結果をつけていき、コードごとに順番並ぶ</font>\n\n\t%%sql\n\tSELECT *,\n\tcase when 金額相違値 >= 0 then '正'\n\t    when 金額相違値 < 0 then '負'\n\t        else '' end as 増長方向,\n\t        row_number() over(partition by コード order by 日付) rn from T0113\n<div style=\"width:70%;margin:auto\">{% asset_img 20210113_1.png 20210113_1%}</div>\n　\n<font size=3>②上記結果をまるごとuにし、a,bという名をつけて、同じコード & bのrn列はaのrnより値が低い & 増長方向が変更した節点という三つ条件が満足した行を絞り込み、降順した第一行目を抽出する</font>\n\n\t%%sql\n\tselect b.rn from u b\n\twhere b.コード = a.コード\n\tand b.rn<a.rn\n\tand b.増長方向 != a.増長方向\n\torder by b.rn desc limit 1\n\n<font size=3>③上記条件を満足されない行に関しては、null値になるので、coalesce関数でnull値を0に変更し、aのrn行と引き算を行い、連続増長日数という新しい一列にする。</font>\n\n\tselect 日付,コード,金額,金額相違値,増長方向,\n    a.rn - coalesce((select b.rn from u b\n                    where b.コード = a.コード\n                    and b.rn<a.rn\n                    and b.増長方向 != a.増長方向\n                    order by b.rn desc limit 1),0) \"連続増長日数\" from u a                   \n\n<font size=3>④完成版と結果は以下の通り</font>           \n\n\t%%sql\n\twith u as\n\t(SELECT *,\n\tcase when 金額相違値 >= 0 then '正'\n\t    when 金額相違値 < 0 then '負'\n\t        else '' end as 増長方向,\n\t        row_number() over(partition by コード order by 日付) rn \n\t from T0113)\n\tselect 日付,コード,金額,金額相違値,増長方向,\n\t    a.rn - coalesce((select b.rn from u b\n\t                    where b.コード = a.コード\n\t                    and b.rn<a.rn\n\t                    and b.増長方向 != a.増長方向\n\t                    order by b.rn desc limit 1),0) \"連続増長日数\" from u a\n\n<div style=\"width:70%;margin:auto\">{% asset_img 20210113_2.png 20210113_2%}</div>\t\n\n### 3.2021年1月16日\n\n\tselect 商品,日付,sum(単価*個数)/sum(個数) as 平均単価 \n\tfrom (select *, \n\tdense_rank() OVER(PARTITION BY 商品 ORDER BY 日付) from T0116)as a\n\twhere dense_rank = 1\n\tgroup by 商品,日付;\n\n<div style=\"width:70%;margin:auto\">{% asset_img 20210116.png 20210116%}</div>\n\n### 4.2021年1月17日\n>勉強したSQL構文:\n>>CONCATを使って、数字型と文字型の結合異なるデータ型を結合したい\n>>>参考記事：[SQL 文字列結合メモ](https://qiita.com/Sammy_U/items/5498b93ff77d52c7e8fb)\n\n<font size=3>①まずover()関数を使ってコードごとにグループ分けを行い、順位をつける</font> \n\n\tselect *,row_number() over(PARTITION by L order by x) as m from T0117\n<div style=\"width:100%;margin:auto\">{% asset_img 20210117_1.png 20210117_1%}</div>\n<font size=3>②つけた順位をもとに、さらにグループ分けを行う</font>\n\n\tselect *,x-row_number() over(PARTITION by L order by x) as m from T0117 \n<div style=\"width:100%;margin:auto\">{% asset_img 20210117_2.png 20210117_2%}</div>\n<font size=3>③お題の求めた通り、グループごとに最小値、最大値の間に'ー'をつける</font> \n\n\tconcat((min(x),'-',max(x)))\n<font size=3>④まとめる</font>\n\n\tselect l,concat(min(x),'-',max(x)) as y from \n\t(select *,x-row_number() over(PARTITION by L order by x) as m from T0117\n\t) as a\n\tgroup by l,m; \n<div style=\"width:100%;margin:auto\">{% asset_img 20210117_3.png 20210117_3%}</div>\n\n### 5.2021年1月21日\n>勉強したSQL構文:\n>>DATEから年月のみを取得\n>>>参考記事[日付データを任意のフィールドで切り捨てる関数(DATE_TRUNC関数)](http://vertica-tech.ashisuto.co.jp/date_trunc/)\n\n<font size=3>①後ほど条件判定を行えるため、DATEから年月のみを取得</font>\n\t\n\tselect *,to_char(mon,'YYYY-MM') as MONTH from T0121;\n<div style=\"width:80%;margin:auto\">{% asset_img 20210121_1.png 20210121_1%}</div>\n<font size=3>②同じユーザー、同じ月の行を選び出し、金額/カウントで結果得る</font>\n\n\twith u as\n\t(select *,to_char(mon,'YYYY-MM') as MONTH from T0121)\n\tselect id,name,month,state,amount/(select count(*) from u as b \n\t                                 where a.month=b.month\n\t                                 and b.name = a.name) as number\n\tfrom u as a;\n<div style=\"width:80%;margin:auto\">{% asset_img 20210121_2.png 20210121_2%}</div>\n\n### 6.2021年1月25日\n>①[データ型書式設定関数](https://www.postgresql.jp/document/9.4/html/functions-formatting.html)\n>>postgresql:to_char([元列名], 'YYYY-MM-DD HH24:MI')\n>>mysqlなら:date_format([元列名], '%Y-%m-%d %H:%i') \n\n>②join[結合テーブル](https://www.postgresql.jp/document/9.1/html/queries-table-expressions.html)\n\n>**③with as使いすぎるとメモリが食われやすいので、要注意**\n\n<font size=3>①データの型をお題通りに揃える</font>\n\n\tselect *,to_char(TransTime,'YYYY-MM-DD HH24:MI') as TransTimed from T0125;\t\n<div style=\"width:80%;margin:auto\">{% asset_img 20210125_1.png 20210125_1%}</div>\n<font size=3>②後ほどのグループ分けのためopcodeごとに順位を付けていく</font>\n\n\twith u as\n\t(select *,to_char(TransTime,'YYYY-MM-DD HH24:MI') as TransTimed from T0125)\n\tselect *,row_number() over(PARTITION BY opcode order by transtimed) as rn from u;\n<div style=\"width:80%;margin:auto\">{% asset_img 20210125_2.png 20210125_2%}</div>\n<font size=3>③次の行を一個上に持ち上げいき、それぞれのtranstimeをstarttimeとendtimeに設定する。transtypeは後ほど文字列の結合をするため、新しい列を作る。</font>\n\n\twith uu as\n\t(with u as\n\t(select *,to_char(TransTime,'YYYY-MM-DD HH24:MI') as TransTimed from T0125)\n\tselect *,row_number() over(PARTITION BY opcode order by transtimed) as rn from u)\n\tselect a.transtype as transtype_1,b.transtype as transtype_2,a.oprseq,a.opcode,a.rn,\n\ta.transtimed as starttime,b.transtimed as endtime from uu as a\n\t    left join uu as b\n\t        on b.opcode=a.opcode\n\t            and b.rn = a.rn+1;\n<div style=\"width:80%;margin:auto\">{% asset_img 20210125_3.png 20210125_3%}</div>\n<font size=3>④奇数列のみ残り(where a.rn %2!=0)、transtypeの２列を結合し、お題が求める列のみを抽出する</font>\n\n\twith result as\n\n\t(with uu as\n\t \n\t(with u as\n\t(select *,to_char(TransTime,'YYYY-MM-DD HH24:MI') as TransTimed from T0125)\n\t \n\tselect *,row_number() over(PARTITION BY opcode order by transtimed) as rn from u)\n\t \n\tselect a.transtype as transtype_1,b.transtype as transtype_2,a.oprseq,a.opcode,a.rn,\n\t a.transtimed as starttime,b.transtimed as endtime from uu as a\n\t     left join uu as b\n\t         on b.opcode=a.opcode\n\t         and b.rn = a.rn+1\n\twhere a.rn %2!=0)\n\n\tselect concat(result.transtype_1,'-',result.transtype_2) as transtype,result.oprseq,result.opcode,result.starttime,result.endtime \n\tfrom result;\n<div style=\"width:80%;margin:auto\">{% asset_img 20210125_4.png 20210125_4%}</div>\n\n### 7.2021年1月26日\n<font size=3>①テーブルを作成し、データを挿入する</font>\n~~~\ncreate table T0126A\n(id serial,\nspmc VARCHAR(100),\n分配量 NUMERIC(18,4));\n\ninsert into T0126A(spmc,分配量) values\n('A',80),\n('B',100);\n\ncreate table T0126B\n(id serial,\nspmc VARCHAR(100),\n購入量 NUMERIC(18,4));\n\ninsert into T0126B(spmc,購入量) values\n('A',20),\n('A',50),\n('A',40),\n('A',30),\n('B',120),\n('B',80),\n('B',100);\n~~~\n>ポイント1:**serial**\n- テーブルの列に一意の識別子を設定する簡便な表記法\n- integer列(整数列)が作成され、その列のデフォルト値が連番整数を振り当てる\n- PostgreSQL7.3及び以降のバージョンでは、UNIQUEまたはPRIMARY KEYにしたい場合、他のすべてのデータ型と同様に指定する必要がある\n- <u>**mysqlならAUTO_INCREMENT,OracleならSEQUENCE(PostgreSQLも)**</u>で連番を採番できる\n- 書き方\n\n\n\tCREATE TABLE tablename(\n\t\tcolumnname SERIAL);\n>>参考記事:[シリアルデータ型](https://www.postgresql.jp/document/7.3/user/datatype.html#DATATYPE-SERIAL)\n\n<font size=3>②spmcごとに累計購入量を計算する</font>\n~~~\nselect *, (select sum(購入量) from T0126B as B\n    where B.spmc=A.spmc\n          and B.id<= A.id) as sum_\n    from T0126B as A\n~~~\n<div style=\"width:80%;margin:auto\">{% asset_img 20210126-1.png 20210126-1%}</div>\n<font size=3>③T0126Bの累計購入量がT0126Aの総分配量より少ない場合、それまで毎回の購入量が全部分配されたと考えられる。それに反した場合、累計購入量とその回の購入量の差が総分配量より少ない場合、その回では一部の購入量が分配されたと考えられる。そうではない場合、余った部分は在庫量になる</font>\n\n~~~\nwith T as(\n    select *, (select sum(購入量) from T0126B as B\n    where B.spmc=A.spmc\n          and B.id<= A.id) as sum_\n    from T0126B as A)\nselect t.spmc,t.購入量,\n(case when T.sum_ <= U.分配量 then T.購入量 else \n     (case when (T.sum_-T.購入量)<U.分配量 then U.分配量-(T.sum_-T.購入量) else 0 end)\nend) as CW\nfrom T\ninner join T0126A as U\n    on t.spmc=U.spmc\n~~~\n<font size=3>④列名をお題通りに変更する</font>\n~~~\nwith T as\n(select *, (select sum(購入量) from T0126B as B\n    where B.spmc=A.spmc\n          and B.id<= A.id) as sum_\n    from T0126B as A),\nV as\n(select t.spmc,t.購入量,\n(case when T.sum_ <= U.分配量 then T.購入量 else \n     (case when (T.sum_-T.購入量)<U.分配量 then U.分配量-(T.sum_-T.購入量) else 0 end)\nend) as CW\nfrom T\ninner join T0126A as U\n    on t.spmc=U.spmc)\nselect spmc,V.購入量,V.cw as 今回使用量,(V.購入量-cw) as 在庫量 from V;\n~~~\n<div style=\"width:80%;margin:auto\">{% asset_img 20210126-2.png 20210126-2%}</div>\n","tags":["sql","回答"],"categories":["sql"]},{"title":"02.羅小黒戦記の映画コメントを分析してみよう","url":"/Jorey-s-Blog/2021/01/08/02_luoxiaohei/","content":"\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/QmfU2NMCw8k\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n<font size=3>自分の好きなシリーズ動画である羅小黒戦記のアニメ映画版は日本で上映されたので、\n日本人の方からこの動画をどう見ているのかを知りたく、ネット上のコメントを取得して分析してみました。\n良い分析結果得られなかったですが、今回とりあえずいろいろ手法の練習にし、今後は今回の結果をもとに、チューニングと精度向上のほうに進みたいと思います。</font>\n\n<font size=3>**該当映画コメントサイトはこちらです↓↓↓**</font>\n[<font size=3>Filmarks羅小黒戦記 ぼくが選ぶ未来の映画情報・感想・評価</font>>](https://filmarks.com/movies/86613)\n\n<!-- more -->\n---------------\n<!-- toc -->\n---------------\n### 1.データクロージング\n<font size=3.5>\n\n>まずサイトからデータを取得してきました。コメント自体は**ネタバレあり**、**ネタバレなし**という2種類に分けられたので、それぞれ取ってきました。\n\n</font>\n\n<font size=3>**①ネタバレなしデータの取得**</font>\n\t\n\tfrom selenium import webdriver\n\timport time\n\tbrowser = webdriver.Chrome()\n<font size=3>\n\n>サイトのコメントを見ていくと、2021年1月時点、およそ300ページほどあり、一ページつづ10本コメントがあります。そこで自動的にページを繰って全部のデータを取ってもらえると嬉しいですね.\n>色々記事を参考してましたが、難しいそうなコードを書かれて、いまいち理解できず・・・\n自分なりに地味に考えたのは：最終ページ数を取ってきて、Range関数を使ってコツコツで集めるとのやり方でした。\n>サイトの要素を確認してみると、最終ページを示している **【>|】** という印は \nhref=\"/movies/86613?page=338\"で指定されています。要するに、ここの338という数字は求めている最終ページ数ですね！\n</font>>\n\n{% asset_img lastpage_href.png lastpage_href %}\n\n\tbrowser.get('https://filmarks.com/movies/86613/no_spoiler?page=1')\n\ttime.sleep(3)\n\n\tlinks = []\n\tfor link in browser.find_elements_by_xpath(\"//*[@href]\"):\n\t    if \"page\" in str(link.get_attribute('href')):\n\t        links.append(link.get_attribute('href'))\n<font size=3>\n\n>取ってきたリンクいくつがありました、それぞれはページ2ー5、next page行く、最終ページ行くと見れますね。\n>今回はまずネタバレなしのデータを取得するので、page289はネタバレなしの合計ページ数ですね。\n>サイト上の状況と一致していることが確認しました。\n\n</font>\n{% asset_img links.png links %}\n{% asset_img no_spoiler_allpage.png no_spoiler_allpage %}\n\n\t#ページ数のみを取得します。\n\ttimes_no_spoiler = links[-1].split(\"page=\")[-1]\n\ttimes_no_spoiler\n\t#289\n\n<font size=3>\n\n>ほか、ユーザー名、ユーザーがつけたスコア、ユーザーの登録頻度の情報も取ってきたいので、一つの関数にまとめてみました。\n\n>ただ最終ページは必ずコメント10本とは言えないので、最終ページ寸前のページまでしかこの関数を使ってません。\n\n>最終ページは別途で処理してます。\n\n</font>\n\n\tdef find_texts(times,url):\n\t    texts = [] #コメント\n\t    scores = []　#スコア\n\t    users = []　#ユーザー名\n\t    conditions = [] #ユーザー登録頻度\n\t    for i in range(1,int(times)):\n\t        browser.get(url+str(i))\n\t        element_text = browser.find_elements('css selector','.p-mark__review')\n\t        element_score = browser.find_elements('css selector','.c-rating__score')\n\t        text = [m.text for m in element_text]\n\t        score = [l.text for l in element_score]\n\t        user = [n.get_attribute('alt') for n in browser.find_elements_by_tag_name('img')]\n\t        condition = [c.get_attribute('loading') for c in browser.find_elements_by_tag_name('img')]\n\t        texts.append(text)\n\t        scores.append(score[1:11])\n\t        users.append(user[3:13])\n\t        conditions.append(condition[3:13])\n\t    return texts,scores,users,conditions\n\n<font size=3>\n\n>早速ネタバレなしのデータを取ってきました。\n(いえ、嘘です、早速ではなく、結構時間かかりました。)\n\n</font>\n\n\tresults_no_spoiler = find_texts(times_no_spoiler)\n\ttext_no_spoiler,score_no_spoiler,user_no_spoiler,condition_no_spoiler = results_no_spoiler\n\n\tprint(len(sum(results_no_spoiler[0],[])))\n\tprint(len(sum(results_no_spoiler[1],[])))\n\tprint(len(sum(results_no_spoiler[2],[])))\n\tprint(len(sum(results_no_spoiler[3],[])))\n\t#2880\n\t#2880\n\t#2880\n\t#2880\n\n<font size=3>\n\n>問題がなさそうですね！引き続きネタバレなしの最終ページを！\n（やり方地味過ぎて申し訳ないです・・・）\n\n</font>\n\n\tbrowser = webdriver.Chrome()\n\n\turl_no_spoiler ='https://filmarks.com/movies/86613/no_spoiler?page='\n\n\tbrowser.get(url_no_spoiler+str(times_no_spoiler))\n\telement_text_last_no_spoiler = browser.find_elements('css selector','.p-mark__review')\n\telement_score_last_no_spoiler = browser.find_elements('css selector','.c-rating__score')\n\n\ttext_last_no_spoiler = [m.text for m in element_text_last_no_spoiler]\n\tscore_last_no_spoiler = [l.text for l in element_score_last_no_spoiler]\n\n\tuser_last_no_spoiler = [n.get_attribute('alt') for n in browser.find_elements_by_tag_name('img')]\n\tcondition_last_no_spoiler = [c.get_attribute('loading') for c in browser.find_elements_by_tag_name('img')]\n\n\tprint(len(text_last_no_spoiler))\n\t#7\n<font size=3>\n\n>最終ページ7本のコメントしかないようですね。実際にサイトの方で確認しましたら、一致でした。\n\n</font>\n\n\ttexts_no_spoiler = sum(text_no_spoiler,[])+text_last_no_spoiler\n\tscores_no_spoiler = sum(score_no_spoiler,[])+score_last_no_spoiler[1:8]\n\tusers_no_spoiler = sum(user_no_spoiler,[])+user_last_no_spoiler[3:10]\n\tconditions_no_spoiler = sum(condition_no_spoiler,[])+condition_last_no_spoiler[3:10]\n\n\timport pandas as pd\n\tdf_no_spoiler = pd.DataFrame({'テキスト':texts_no_spoiler,\n\t                           'スコア':scores_no_spoiler,\n\t                           'ユーザー':users_no_spoiler,\n\t                           '登録頻度':conditions_no_spoiler})\n\tdf_no_spoiler['ネタバレ'] = 'なし'\n<font size=3>\n\n>これでネタバレなしのデータ取得できました。\n\n</font>\n{% asset_img df_no_spoiler.png df_no_spoiler %}\n\n<font size=3>\n\n>これでネタバレありのデータも同様な方法で取得し、最後できた全件データは3374件でした。\n\n</font>\n{% asset_img df_all_data.png df_all_data %}\n\n### 2.前処理と分かち書き\n<font size=3>\n\n>前処理では、絵文字、iphone系絵文字、改行記号、省略記号(…)、英語を小文字に統一、正規化、url、数字、各種記号の処理を行いました。\n\n</font>\n\n\timport neologdn\n\timport re\n\timport emoji\n\timport string\n\tdef filter(desstr, restr=''):\n\t\t\n\t\t#emoji\n\t    res = re.compile(u'[\\U00010000-\\U0010ffff]')\n\t    res_emoji = res.sub(restr,desstr)\n\n\t    #iPhoneのemoji \n\t    res_iphone_emoji = ''.join(c for c in desstr if c not in emoji.UNICODE_EMOJI) \n\n\t    #改行記号\n\t    res_linebreak = ''.join(res_iphone_emoji.replace('<br/>','').split())\n\n\t    #省略記号\n\t    res_ellipsis = ''.join(res_linebreak.replace('…','').split())\n\n\t    #大文字→小文字\n\t    res_uppertolower = res_ellipsis.lower() \n\n\t    #正規化\n\t    res_normalization = neologdn.normalize(res_uppertolower)\n\n\t    #url\n\t    res_url = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+','',res_normalization)\n\n\t    #記号\n\t    res_punctuation = res_url.translate(str.maketrans( '', '',string.punctuation)) \n\n\t    #数字\n\t    res_num = ''.join([i for i in res_punctuation if not i.isdigit()])\n\t        \n\t    return res_num\n\ttext_ed = [filter(str(i)) for i in df['text']]\n\tdf['texts_ed'] = text_ed\n<font size=3>\n\n>今回Mecabを使って分かち書きを行いました。\n\n</font>\n\n\tdef wakati_by_mecab(text):\n\t    tagger = MeCab.Tagger(r'C:\\Users\\※※\\Anaconda3\\envs\\enviroment\\Lib\\site-packages\\MeCab\\mecab-ipadic-neologd')\n\t    tagger.parse('') \n\t    node = tagger.parseToNode(text)\n\t    word_list = []\n\t    while node:\n\t        pos = node.feature.split(\",\")[0]\n\t        if pos in [\"名詞\", \"動詞\", \"形容詞\"]:\n\t            word = node.surface\n\t            word_list.append(word)\n\t        node = node.next\n\t    return \" \".join(word_list)\n\n\ttexts_wakati = [wakati_by_mecab(i) for i in text_ed]\n\n\t#stopwordsの処理\n\ttexts_wakati_ed =[]\n\tfor m in texts_wakati:\n\ttext_wakati_ed = [i for i in m.split() if i not in df_stopwords.values.tolist()]\n    texts_wakati_ed.append(text_wakati_ed)\n    df['texts_wakati'] = [' '.join(i) for i in texts_wakati_ed]\n<font size=3>\n\n>分かち書き行ったデータはこういう感じです\n\n</font> \n{% asset_img wakati.png wakati%}\n\n### 3.登場人物と関連テーマ\n#### A.ワードクラウドでコメントの概要を把握\n<font size=3>\n\n>まずこの映画の内容について、コメント内一番話題となっている人物とテーマを探りたいので、\n>とりあえず一番よく出る単語の上位200位のワードクラウドで確認してみました。\n\n</font> \n\n\tfrom collections import Counter\n\tfrom wordcloud import WordCloud\n\timport matplotlib.pyplot as plt\n\tfrom pyecharts.charts import *\n\tfrom pyecharts import options as opts\n\tfrom pyecharts.globals import ThemeType\n\tfrom IPython.display import Image\n\n\tword_count_1 = Counter(sum(texts_wakati_ed,[]))\n\twc_1 = WordCloud(\n    width=2000,\n    height=1200,\n    font_path=r'C:\\Users\\★★\\python\\UDDigiKyokashoN-R.ttc',\n    margin=2,\n    relative_scaling=0.2,\n    prefer_horizontal=1,\n    colormap='tab20')\n\twc2_1 = wc_1.generate_from_frequencies(word_count_1,200) \n\tfig = plt.figure(figsize=(20,12))\n\tplt.imshow(wc2_1, interpolation=\"bilinear\")\n\tplt.axis(\"off\")\n\tplt.show()\n\n<div style=\"width:70%;margin:auto\">{% asset_img wc_1.png wc_1%}</div>\n<font size=3>\n\n>ワードクラウドを確認して、以下のいくつのテーマについて気になりましたので、それぞれの内容を抽出してみました。\n>**['シャオヘイ','ムゲン','フーシー','人間','妖精','日本','中国','ジブリ']**\n\n</font> \n\n\tl1=[]\n\tl2=[]\n\tfor i in range(0,len(df['text'])):\n\t    if 'シャオヘイ' in df['text'].iloc[i]:\n\t        l1.append(df['texts_wakati'].iloc[i])\n\t    if '小黒' in df['text'].iloc[i]:\n\t        l2.append(df['texts_wakati'].iloc[i])\n\t...\n\n\tlist_=[len(list(set(l1+l2+l13))),len(list(set(l6+l7+l8))),len(list(set(l3+l4+l5+l12))),len(l9),len(l10),len(l11),len(l14),len(l15)]\n\tdf_kw = pd.DataFrame(list_,index=['シャオヘイ','ムゲン','フーシー','人間','妖精','ジブリ','日本','中国'])\n\tdf_kw=df_kw.reset_index().rename(columns = {'index':'名前',0:'回数'})\n\n\txaxis = df_kw['回数'].to_list()\n\tyaxis = df_kw['名前'].to_list()\n{% asset_img xyaxis.png xyaxis%}\n\n\t#横棒グラフで集計結果を確認しました。\n\tc = (\n\t    Bar(init_opts = opts.InitOpts(theme = ThemeType.CHALK))\n\t    .add_xaxis(yaxis)\n\t    .add_yaxis('',xaxis).reversal_axis()\n\t    .set_global_opts(title_opts = opts.TitleOpts(title = 'テーマ言及回数',pos_left = 'top'),\n\t                     yaxis_opts = opts.AxisOpts(axislabel_opts = opts.LabelOpts(font_size=13)),\n\t                     xaxis_opts = opts.AxisOpts(axislabel_opts = opts.LabelOpts(font_size=13))\n\t                    )\n\t    .set_series_opts(label_opts = opts.LabelOpts(font_size = 16,position ='right'))\n\t)\n\tc.render_notebook()\n<div style=\"width:70%;margin:auto\">{% asset_img テーマごとの結果_1.png テーマごとの結果_1%}</div>\n<font size=3>\n\n>テーマごとの詳細はワードクラウドで表示してみました。\n>5つのテーマの結果しか記載していないが、全然違いがないこと一目瞭然ですね。。。\n>おそらくユーザーのコメントテーマはかなり近くて、文章レベルじゃあわかりにくいと思うので、Sentenceレベルで行ってみました。\n\n</font>\n<div style=\"width:100%;margin:auto\">{% asset_img wc_1_all.png wc_1_all%}</div>\n<font size=3>\n\n>まず文章を一つずつのSentenceに分解しました。\n\n</font>\n\t\n\ttext_kw=' '.join(' '.join(''.join(df['text'].to_list()).split('、')).split('。')).split()\n\n<div style=\"width:70%;margin:auto\">{% asset_img sentence_cut.png sentence_cut%}</div>\n\n\tkw_list = ['シャオヘイ','小黒','フーシー','フーシ','風息','ムゲン','無限','師匠','人間','妖精','ジブリ','フウシー','黒猫','日本','中国']\n\tlist_kw_text_sh = []\n\tlist_kw_text_fs = []\n\t...\n\tlist_kw = []\n\tfor i in text_kw:\n\t    if kw_list[0] in i:\n\t        list_kw_text_sh.append(i)\n\t        list_kw.append(kw_list[0])\n\t    elif kw_list[1] in i:\n\t        list_kw_text_sh.append(i)\n\t        list_kw.append(kw_list[0])\n\t    elif kw_list[2] in i:\n\t        list_kw_text_fs.append(i)\n\t        list_kw.append(kw_list[2])\n\t    elif kw_list[3] in i:\n\t        list_kw_text_fs.append(i)\n\t        list_kw.append(kw_list[2])\n\t ...\n\tdf_count = pd.DataFrame(Counter(list_kw).keys(),Counter(list_kw).values(),columns =['KW'] )\n\tdf_count=df_count.reset_index().rename(columns = {'index':'回数'})\n\tdf_count\n\tyaxis = df_count['KW'].values.tolist()\n\txaxis = df_count['回数'].values.tolist()\n\n<font size=3>\n\n>上記得た統計結果をもとにした横棒グラフ結果とワードクラウドは以下の通りです。\n\n>これらの結果からテーマごとの違いがわかりましたね。例えば、シャオヘイきっととにかく可愛いですよね、ムゲンはかっこ良く最強イメージの師匠ですね、フーシーの声優さんである櫻井孝宏さんは結構評価されたらろうね、仲間に関する、人間と妖精の共存、居場所が破壊されたりされたシーンがありましたね、この映画に日本のジブリや、ドラゴン、もののけ等の作品に影響され、かなり日本アニメ要素多く感じられましたよね。そこから中国文化に関して語られたユーザーもいたなどなどの情報が見られましたね。\n\n</font>\n<div style=\"width:70%;margin:auto\">{% asset_img テーマごとの結果_2.png テーマごとの結果_2%}</div>\n<div style=\"width:100%;margin:auto\">{% asset_img wc_2_all_1.png wc_2_all_1%}</div>\n<div style=\"width:100%;margin:auto\">{% asset_img wc_2_all_2.png wc_2_all_2%}</div>\n\n#### B.ワードクラウドを自動生成する簡易版GUI\n<font size=3>\n\n>この結果をもとに、任意のキーワードを入力すると、自動的にワードクラウドが出てくる簡易版GUIツールを作ってみました。\n>こんな感じです！(地味不可避・・・)\n>今後の課題としてどんどん改善していきたいと思います！\n\n<font size=3>\n\n\tfrom tkinter import *\n\timport tkinter as tk\n\n\t#ワードクラウド生成用の関数\n\tdef get_data():\n\t    theme_data = _input.get()\n\t    theme_texts =[i for i in text_kw if theme_data in i]\n\t    theme_wakati = [wakati_by_mecab(i) for i in theme_texts]\n\t    theme_count = Counter(' '.join(theme_wakati).split())\n\t    wc = WordCloud(\n\t        width=1800,\n\t        height=1200,\n\t        font_path=r'C:\\Users\\※※\\python\\UDDigiKyokashoN-R.ttc',\n\t        margin=2,\n\t        relative_scaling=0.3,\n\t        prefer_horizontal=1,\n\t        colormap='tab20')\n\t    wc2 = wc.generate_from_frequencies(theme_count,300)\n\t    fig = plt.figure(figsize=(18,12))\n\t    plt.imshow(wc2, interpolation=\"bilinear\")\n\t    plt.axis(\"off\")\n\t    plt.show()\n\n    #簡易版GUI作成\n    app = Tk()\n\t_input=tk.Entry(app,show=None) #入力画面の設定\n\t_input.pack()　\n\tapp.title('キーワード入力')\n\tscreenwidth = app.winfo_screenwidth()\n\tscreenheight = app.winfo_screenheight()\n\tdialog_width = 400\n\tdialog_height = 170\n\tapp.geometry(\n\t    \"%dx%d+%d+%d\" % (dialog_width, dialog_height, (screenwidth - dialog_width) / 2, (screenheight - dialog_height) / 2))\n\tbtn = Button(text ='search',command = get_data,width = 10)\n\tbtn.place(x=155,y = 80)\n\tbtn.pack()\n\tapp.mainloop()\n<div style=\"width:100%;margin:auto\">{% asset_img GUI.png GUI%}</div>\n\n### 4.極性辞書による感情分析\n#### A.単語感情極性対応表でスコアを付く\n<font size=3>\n\n>極性辞書による感情分析は今回メインに以下の記事を参考して行いました。\n>>[感情分析でニュース記事のネガポジ度合いをスコア化する](https://qiita.com/g-k/items/e49f68d7e2fed6e300ea)\n\n>まず極性辞書 ([単語感情極性対応表](http://www.lr.pi.titech.ac.jp/~takamura/pndic_ja.html))を読み込み\n>>極性辞書と結合するため、分かち書きからデータの品詞情報を色々処理を行いました。\n\n</font>\n\n\t#極性辞書を読み込み\n\tdict_= pd.read_table('極性辞書.txt',sep =':',header = None)\n\tdff = dict_.rename(columns ={0:'基本形',1:'読み方',2:'品詞',3:'極性スコア'})\n<div style=\"width:70%;margin:auto\">{% asset_img dff_dict.png dff_dict%}</div>\n\n\t#分かち書き\n\timport MeCab\n\ttagger = MeCab.Tagger(r'C:\\Users\\※※\\Anaconda3\\envs\\enviroment\\Lib\\site-packages\\MeCab\\mecab-ipadic-neologd')\n\ttexts = [tagger.parse(i) for i in df.texts_ed]\n<div style=\"width:70%;margin:auto\">{% asset_img texts_wakati.png texts_wakati%}</div>\n\n\ttmps = []\n\tfor t in texts:\n\t        tmp = [k.split('\\t') for k in t.split('\\n')]\n\t        tmp_ = tmp[:-2] #文末のEOS部分を切り捨て\n\t        tmps.append(tmp_)\n\tsurfs = [pd.DataFrame(m).rename(columns = {0:'単語',2:'読み方_old',3:'基本形',4:'品詞_old'}) for m in tmps]\n\t\n\t#形容詞-非自立可能、名詞-普通名詞-助数詞可能のような品詞情報を形容詞、名詞に修正\n\tllist= []\n\tfor i in surfs:\n\t    list_ = []\n\t    for m in range(0,len(i)):\n\t        if i['品詞_old'][m] !=None:\n\t            list_.append(i['品詞_old'].str.split('-')[m][0])\n\t        else:\n\t            list_.append(i['品詞_old'][m])\n\t    llist.append(list_) \n\t\n\t#読み方をカタカナからひらがなに変更\n\timport jaconv\n\tllists= []\n\tfor i in surfs:\n\t    list_y = []\n\t    for m in range(0,len(i)):\n\t        if i['読み方_old'][m] != None:\n\t            list_y.append(jaconv.kata2hira(i['読み方_old'][m]))\n\t        else:\n\t            list_y.append(i['読み方_old'][m])\n\t    llists.append(list_y)\n\n\t#分かち書きされたデータと極性辞書を結合することによって、単語ごとにスコア値を付けていく\n\tscore_results = []\n\tfor i in range(0,len(surfs)):\n\t    surfs[i]['品詞'] = llist[i]\n\t    surfs[i]['読み方'] = llists[i]\n\t    if surfs[i].columns.size == 10:\n\t        score_result= pd.merge(surfs[i][['単語','基本形','読み方','品詞']],dff,on=['基本形','読み方','品詞'],how = 'left')\n\t        score_results.append(score_result)\n\t    else:\n\t        score_results.append(surfs[i])\n<font size=3>\n\n>すべての文書は単語ごとにスコアが付けられました。\n>極性辞書にない単語のスコアはNaN値。\n>文書ごとにすべて単語のスコアを加算すると、文書ごとのスコアが得られる\n\n</font>\n<div style=\"width:70%;margin:auto\">{% asset_img socre_singlearticle.png socre_singlearticle%}</div>\n\n\tresults = []\n\tfor i in range(0,len(score_results)):\n\t    if score_results[i].columns.size == 5:\n\t        text = ''.join(list(score_results[i]['単語']))\n\t        score = score_results[i]['極性スコア'].astype(float).sum()\n\t        score_r = score/score_results[i]['極性スコア'].astype(float).count()\n\t        result = [text,score,score_r]\n\t        results.append(result)\n\t    else:\n\t        results.append(score_results[i])\n\tdf_score = []\n\tfor i in range(0,len(results)):\n\t    if len(results[i]) == 3: \n\t        dfff = pd.DataFrame({\n\t            'テキスト':results[i][0],\n\t            '累計スコア':results[i][1],\n\t            '累計標準化スコア':results[i][2],\n\t            'user':df.iloc[i]['user'],\n\t            'score':df.iloc[i]['score']},index =range(1))\n\t        df_score.append(dfff.iloc[[0]])\n\tddf_score = pd.concat(df_score).reset_index().sort_values('累計標準化スコア',ascending=False)\n<font size=3>\n\n>一発で行った結果、上位10位と下位10位の内容は以下の通りでした。\n>分かち書きによるデータの品詞や読み方等極性辞書と一致しない単語があることで、スコア付けがズレが生じたことがわかりました。\n>>例えば2678行の「好きです」の「好き」は分かち書きした品詞が形状詞ですが、極性辞書内では名詞とのことで、スコアが付けられませんでした。\n\n</font>\n\t\n\tddf_score[:10:]['テキスト'].to_list()\n\t#['フーシーにもムゲンにもそうだよねぇっていう気持ち。シャオヘイがかーわいい。',\n\t 'とっても可愛かったー猫好きなら見るべきストーリーも良かった',\n\t '昭和の東映まんが祭りを彷彿とさせる力強くまっすぐなアニメーション。素晴らしかった。',\n\t 'やさしいタッチのキャラデザから繰り広げられるゴリッゴリのエフェクトアニメーション作画に圧巻ファンタジーとアクションのミックスシャオヘイかわゆい',\n\t '『ドラゴンボールブロリー』並みのアクションバトルシーンに圧巻',\n\t 'おもろかったストーリーわかりやすいし、作画が神デパートでのバトルシーンが一番好き',\n\t 'ムゲン様バトルシーンが凄かったこれはdxでみればよかったな',\n\t '可愛い絵が綺麗可愛いアクションシーンがすごい可愛い面白い可愛い可愛い可愛い',\n\t 'これは満足ですまずシャオヘイが可愛いし、ムゲン様がカッコいい',\n\t '良いリアルファンタジーで、良い師弟ものでした。わかりやすい面白さがあります。']\n\n\t ddf_score[-10::]['テキスト'].to_list()\n\t #['ナルト×ドラゴンボール×ジブリみたいで、そら大好きになりますわといった感じです。',\n\t 'ドラゴンボールと千と千尋となんかをミックスした感じ。シャオヘイとてもきゃわわ。',\n\t '子供と観たんだけれども、ストーリーがよくわからなかったみたい。',\n\t 'アニメで見たいのはこういうのなんだよ', →score=0\n\t 'ムゲン様フーシームゲン様フーシーウッ',→score=0\n\t 'え好き',→score=0\n\t 'アアアァァァァクションンンンンンンだったこれはサイドストーリーひたすら観たいな',→score=0\n\t 'livezound×rgbレーザーチネチッタチネm',→score=0\n\t '中国語版ユジク日本語吹き替え版バルト',→score=0\n\t '好きです'→score=0]\n#### B.極性辞書を生成した上でスコアを付く\n<font size=3>\n\n>既存の極性辞書は今回のデータと相性が良くないようなので、独自の極性辞書を生成する手を考えました。\n>>メインはこちらの記事[感情分析に用いる極性辞書を自動生成する](https://qiita.com/g-k/items/1b7c765fa6520297ca7c)を参考しました。\n>>model_neologd.vecの取得はこちらの記事[fastTextの学習済みモデルを公開しました](https://qiita.com/Hironsan/items/513b9f93752ecee9e670)をご参考ください。\n>>posi_listとnega_listはデータ内容をみて適宜修正あり\n\n</font>\n\n\timport gensim\n\tmodel = gensim.models.KeyedVectors.load_word2vec_format('model_neologd.vec', binary=False)\n\n\tposi_list = ['優れる', '良い','喜ぶ','褒める', 'めでたい','賢い','善い', '適す','天晴','祝う', '功績','賞',\n\t'嬉しい','喜び','才知','徳', '才能','素晴らしい','芳しい','称える','適切','崇める','助ける','抜きんでる','清水',\n\t'雄雄しい','仕合せ','幸い','吉兆','秀でる','かわいい','すすめ','dvd','可愛','涙腺','泣き','豪華','うまい',\n\t'緻密','おもしろかっ','加点','最高','すごい','魅力','王道','綺麗','感動','楽しめ','好き','一番','やすい','いえー',\n\t'酔いしれ','微笑み']\n\tnega_list = ['悪い', '死ぬ', '病気', '酷い', '罵る', '浸ける', '卑しい','下手', '苦しむ', '苦しい', '付く', \n\t'厳しい', '難しい', '殺す', '難い', '荒荒しい','惨い', '責める', '敵', '背く', '嘲る', '苦しめる', '辛い', \n\t'物寂しい', '罰', '不貞腐る','寒い', '下らない','残念','疲れ','複雑','真似','不足','わから','違う','BLM','デモ',\n\t'過激','対立','ステレオタイプ','古い','寝','謎','足し','オリジナリティ','苦痛','後進','昭和','稚拙','なさ','退屈',\n\t'高かっ','残ら','説明','しんど','減点','違和感','ぽんぽこ','粗','仕方ない','合わ','掘り下げ','薄い','未熟','苦手',\n\t'ダメ','ごめん','ガッカリ','眠','抑圧','にくい']\n\n\tdef posi_nega_score(x):\n\t    #ポジティブ度合いの判定\n\t    posi = []\n\t    for i in posi_list:\n\t        try:\n\t            n = model.similarity(i, x)\n\t            posi.append(n)\n\t        except:\n\t            continue\n\t    try:\n\t        posi_mean = sum(posi)/len(posi)\n\t    except:\n\t        posi_mean = 0\n\t    #ネガティブ度合いの判定\n\t    nega = []\n\t    for i in nega_list:\n\t        try:\n\t            n = model.similarity(i, x)\n\t            nega.append(n)\n\t        except:\n\t            continue\n\t    try:\n\t        nega_mean = sum(nega)/len(nega)\n\t    except:\n\t        nega_mean = 0\n\t    if posi_mean > nega_mean:\n\t        return posi_mean\n\t    if nega_mean > posi_mean:\n\t        return -nega_mean\n\t    else:\n\t        return 0\n\n    for i in score_results:\n\t    if i.columns.size !=2:\n\t        i['極性スコア_new'] = i['単語'].apply(lambda x : posi_nega_score(x))\n<div style=\"width:70%;margin:auto\">{% asset_img score_result_2.png score_result_2%}</div>\n<font size=3>\n\n>新しく得た極性スコアを用いて文書ごとのスコアを算出しましょう\n\n</font>\n\n\t#スコアを標準化\n\timport numpy as np\n\tfor i in score_results:\n\t    if i.columns.size!=2:\n\t        sscore = np.array(i['極性スコア_new'])\n\t        sscore_std = (sscore - sscore.min())/(sscore.max() - sscore.min())\n\t        sscore_scaled = sscore_std * (1 - (-1)) + (-1)\n\t        i['標準化スコア_new'] = sscore_scaled\n\t\n\t#文書のスコアを算出\n\tresults_new = []\n\tfor i in score_results:\n\t    if i.columns.size != 2:\n\t        text_new = ''.join(list(i['単語']))\n\t        score_new = i['標準化スコア_new'].astype(float).sum()\n\t        score_r_new = score_new/i['標準化スコア_new'].astype(float).count()\n\t        result_new = [text_new,score_new,score_r_new]\n\t        results_new.append(result_new)\n\t    else:\n\t        results_new.append(i)\n\n\t#上位10位と下位10位\n\tddf_score_new[:10:]['テキスト'].to_list()\n\t#['戦目新宿バルト戦目t・ジョイプリンス品川戦目新宿バルト戦目新宿バルト戦目チネチッタ川崎戦目チネチッタ川崎最新字幕版戦目チネチッタ川崎最新字幕版',\n\t '久しぶりにちゃんとしたアニメーション映画を観たなっていう感じだった',\n\t '期待どおり最高。ストーリー、キャラ、アクション、テンポ感、会話の掛け合い、声優来場者特典イラストカード。',\n\t '可愛い絵が綺麗可愛いアクションシーンがすごい可愛い面白い可愛い可愛い可愛い',\n\t '『ドラゴンボールブロリー』並みのアクションバトルシーンに圧巻',\n\t '大変大変大変大変大変大変大変大変大変大変大変大変大変大変大変大変大変大変大変大変大変に、好みの映画でしたパンフ売り切れだったので、アニプレックスさんの通販で買いました。買えてよかった',\n\t '涙腺ガバだから泣いためちゃくちゃ王道だしクオリティ高い中国アニメいいです',\n\t 'バトルかっこよす。櫻井vs宮野、木属性vs金属性、自然vs人工。',\n\t '超神作画、、ムゲンさま、、宮野さま、、共に生きること',\n\t 'もう一度見たい★★★★★期待以上☆☆☆☆期待通り☆☆☆期待外れ☆☆時間返せ☆dynamic・・・・・・・・・・・★・・・・・・realisticーーfantastic・・・・・・・・・・・・・・・・・・static日時場所バルト客入☆☆★☆☆']\n\n\tddf_score_new[-10::]['テキスト'].to_list()\n\t['線も造形もシンプルだけどとにかく動き回りまくってて楽しいキャラクターや世界観に想像の余地があってもっと見たくなるからまで説明する鬼滅とはある意味反対でそこは好み',\n\t 'しゅき正直話として飲み込めないとこも多々あったけど、しゅきという感情にねじ伏せられる',\n\t 'かわいいしかない映画。ロシャオとムゲンのやり取りが好きすぎる',\n\t '変化を受けいられない人たちは切り捨てるしかなかったのかと思ったら、悲しくなってきた',\n\t '「《共存》コソコソ生きろって」構図と動きの表現に勢いがある。話の内容は製作国の最近のことを考えるとちょっと複雑。',\n\t 'ソフト化未定ってどういうことやアニプレックスなんとかしてくれや',\n\t 'アニメーションはすごく迫力がありキャラクターも可愛らしく大変楽しめたがblm運動や香港のデモのことなどが頭によぎりマイノリティの過激派と穏健派の対立という物語はもう古いというかマジョリティによるステレオタイプの再生産じゃ無いかと思ってしまった',\n\t 'ひたすらかわいい、しかもアクションが楽しい字幕のスピード、大きさ、色がいつもの映画のそれとは違ってて追えない箇所が多かった',\n\t '森になる森になるしかなかった森になるしかなかった人はどうすればいいのか',\n\t 'とにかくシャオヘイがかわいい話が進むにつれて変化するムゲンとの関係も見ていて楽しかった']\n<font size=3>\n\n>うん、ちょっとまずいですね・・・\n>'〇〇良かったですが、〇〇はダメですね'というようなコメントはちらほら見られますね\n>すごい人から、中盤スコアを除くと、悪い評価と良い評価の差が付けられるかもしれないというアドバイスを頂きましたため、実施してみましょう。\n\n</font>\n\n\tresults_new = []\n\tfor i in score_results:\n    if i.columns.size != 2:\n        text_new = ''.join(list(i['単語']))\n        score_new = i[abs(i['標準化スコア_new'])>0.5]['標準化スコア_new'].sum() #極性スコアの絶対値は0.5以上しか加算しないと指定\n        score_r_new = score_new/i['標準化スコア_new'].astype(float).count()\n        result_new = [text_new,score_new,score_r_new]\n        results_new.append(result_new)\n    else:\n        results_new.append(i)\n\n    #上位10位と下位10位\n<div style=\"width:100%;margin:auto\">{% asset_img score_result_4.png score_result_4%}</div>\n<div style=\"width:100 %;margin:auto\">{% asset_img score_result_5.png score_result_5%}</div>\n\n<font size=3>\n\n>若干改善?でもやはり一部良い評価なのに下位に来てしまった文書がありますね\n>またそもそも各単語のスコアを確認してみると、シャオヘイやムゲンというキャラクタ名のスコアもマイナスになっていて、更に「話」や「が」、「つれ」、「の」等も無駄にマイナスになっていますね。やはり今回生成した辞書そのものが怪しいですね。。\n\n</font>\n<div style=\"width:100 %;margin:auto\">{% asset_img score_result_6.png score_result_6%}</div>\n\n<font size=3>\n\n>それに応じて得たスコアも怪しいかもしれないですが、こんな感じです。\n\n</font>\n\n\tdata = ddf_score_new[ddf_score_new['スコア']!='-']['累計標準化スコア'].to_list()\n\tfig = plt.figure(figsize=(12,8))\n\tplt.hist(data,bins=40, density=True,stacked=True, facecolor=\"blue\", edgecolor=\"black\", alpha=0.7)\n\tplt.show()\n<div style=\"width:70%;margin:auto\">{% asset_img score_plot.png score_plot%}</div>\n\n\tscore_new = [round((3+i/0.5),1) for i in ddf_score_new['累計標準化スコア']]\n\tddf_score_new['score_test'] = score_new\n<div style=\"width:100%;margin:auto\">{% asset_img score_test.png score_test%}</div>\n\n#### C.まとめ\n<font size=3>\n\n>極性辞書を使って、容易にすべてのデータに対してスコアをつけられることが使いやすいですね\n>しかし、生成した辞書の正確性をどのように保証していくのか、今後の課題として探求していきたいと思います。\n>中盤のスコアを無視し、両端側(今回は絶対値0.5以上のスコアを加算)スコアの加算は有効な手段と考えられます。\n\n</font>\n\n### 5.機械学習による感情分析_LSTM(今後データ量を増加する予定)\n#### A.ラベル付け\n<font size=3>\n\n>まずユーザーのスコアによるポジフラグとネガフラグを付けていきたいと思いますが、今回スコア2以下のデータはわずか7件しかないため、スコア3.5以下のデータのうち、ネガ情報入っているデータをネガフラグつけることにしました。(ここはあくまで人工知能の人工の部分です。)\n\n</font>\n\n\t#今回のテストデータ\n\tdf.loc[df['label'] == '-','sentiment'] = 3\n\t#ネガフラグデータ\n\tdf.loc[df['label'] == '0','sentiment'] =0\n\t#ポジフラグデータ\n\tdf.loc[(df['label'] == '1'),'sentiment'] =1\n\t#ネガではなく、ポジでもないデータ\n\tdf.loc[df['label'] == '2','sentiment'] =2\n\tprint(len(df[df.sentiment == 1]),len(df[df.sentiment == 0]),len(df[df.sentiment == 2]),len(df[df.sentiment == 3]))\n\t#2302 180 587 305\n<div style=\"width:100%;margin:auto\">{% asset_img sentiment_plot_1.png sentiment_plot_1%}</div>\n\n#### B.ベクトル化\n<font size=3>\n\n>前処理と分かち書きを行いましたら、gensimのword2vecモデルを使ってデータのベクトル化を行いました\n\n</font>\n\t\n\timport gensim\n\tmodel = gensim.models.KeyedVectors.load_word2vec_format('model_neologd.vec', binary=False)\n\t#Wikiモデル内格納している各単語\n\tvocab_list = list(model.wv.vocab.keys())\n\t#Wikiモデル内格納している各単語のベクトル\n\tword_vectors = model.wv.syn0\n\t#Wikiモデル内格納している各単語及び単語のインデックス情報\n\tword_index = {word: index for index, word in enumerate(vocab_list)}\n\n\tlen_list =[len(i.split()) for i in df['texts_wakati']]\n\tprint('ファイル全件',len(len_list),'件です')\n\tprint('単語全部',sum(len_list),'個です')\n\tprint('単語数の最大値は',max(len_list),'です')\n\t#ファイル全件 3374 件です\n\t#単語全部 206770 個です\n\t#単語数の最大値は 2087 です\n\n\timport random\n\tpos_data = df[df['sentiment'] == 1]\n\tneg_data = df[df['sentiment'] == 0]\n\t#ポジ・ネガデータを17:1で分割する\n\tpos_index_train = random.sample(list(pos_data.index),2174)\n\tneg_index_train = random.sample(list(neg_data.index),170)\n\t#インデックスを取得\n\tpos_index_validation = [x for x in pos_data.index if x not in pos_index_train]\n\tneg_index_validation = [x for x in neg_data.index if x not in neg_index_train]\n\t#テキストデータ等を取得\n\tpos_sentence_train = pos_data.loc[pos_index_train]\n\tneg_sentence_train = neg_data.loc[neg_index_train]\n\tpos_sentence_validation = pos_data.loc[pos_index_validation]\n\tneg_sentence_validation = neg_data.loc[neg_index_validation]\n\n\t#trainデータとvalidデータを結合する\n\tdf_train = pd.concat([pos_sentence_train,neg_sentence_train],axis =0)\n\tdf_validation = pd.concat([pos_sentence_validation,neg_sentence_validation],axis =0)\n\tdf_all = pd.concat([df_train,df_validation],axis = 0)\n\tprint(len(df_train),len(df_validation),len(df_all))\n\t#2344 138 2482\n\tX_all = df_all.texts_wakati.tolist()\n\tX_train = df_train.texts_wakati.tolist()\n\tX_validation = df_validation.texts_wakati.tolist()\n\ty_train = df_train['sentiment'].values.astype('int8')\n\n\tfrom keras.preprocessing.text import Tokenizer\n\tfrom keras.preprocessing.sequence import pad_sequences\n\t#まずTokenizerを使ってベクトルを作成してみました\n\ttokenizer = Tokenizer(\n\t    nb_words = 2000,\n\t    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n\t    lower=True,\n\t    split=' ')\n\ttokenizer.fit_on_texts(X_all)\n\tX = tokenizer.texts_to_sequences(X_all)\n\tX = pad_sequences(X)\n\tX.shape\n\t#(2482,1715)\n\n#### C.LSTMモデルを使って実施\n\tembed_dim = 128\n\tlstm_out = 256\n\tbatch_size = 32\n\n\tfrom keras.models import Sequential\n\tfrom keras.layers import Embedding\n\tfrom tensorflow.keras.layers import Dropout, Dense, LSTM,Flatten\n\tmodel_1 =  Sequential()\n\tmodel_1.add(Embedding(2000,embed_dim,input_length = X.shape[1]))\n\tmodel_1.add(LSTM(lstm_out,dropout=0.2,return_sequences=True))\n\tmodel_1.add(Flatten())\n\tmodel_1.add(Dense(2,activation='softmax'))\n\tmodel_1.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n\tprint(model_1.summary())\n<div style=\"width:100%;margin:auto\">{% asset_img LSTM.png LSTM%}</div>\n\n\tfrom sklearn.model_selection import train_test_split\n\tfrom sklearn.metrics import roc_auc_score\n\tfrom keras.callbacks import Callback\n\n\tx_train5,y_train5,x_label5,y_label5 = train_test_split(X_train,y_binary, train_size=0.8, random_state=234)\n\n\thistory =  model_1.fit(x_train5,x_label5,batch_size=batch_size,epochs= 2,validation_data=(y_train5, y_label5),verbose=2)\n<div style=\"width:100%;margin:auto\">{% asset_img LSTM_2.png LSTM_2%}</div>\n\n\ty_lstm = model_1.predict(X_validation,batch_size=batch_size)\n\tdf_validation['score_new'] = [round((i/0.2),1) for i in y_lstm[:,1].tolist()]\n\n\t#出た結果とユーザーの評価と比較してみると\n\txaxis = df_validation['user'].tolist()\n\tyaxis1 = df_validation['score'].astype(float).tolist()\n\tyaxis2 = df_validation['score_new'].astype(float).tolist()\n\tplt.figure(figsize = (12,8))\n\tplt.title('Result Analysis')\n\tplt.plot(yaxis1,color='red',linewidth=2.0,linestyle='--',label = 'user')\n\tplt.plot(yaxis2,color='green',linewidth=2.0,linestyle='solid',label = 'LSTM')\n\tplt.legend()\n\tplt.show()\n<div style=\"width:100%;margin:auto\">{% asset_img LSTM_1.png LSTM_1%}</div>\n\n#### まとめ\n>epochs5回と30回も実行してみましたが、どうやらトレーニングデータのaccuracyがひたすらよくなって、val_accuracyは維持する結果しかならないですね。\n>そもそも3000件しかないデータだとディプラーニングで過学習しやすいと後ほど知人にからかわれました・・・\n>とりあえずデータを増加してから再度チャレンジしたいと思います！！！\n\n### 6.残り課題\n>極性辞書のチューニング\n>LSTMのパラメータファインチューニング\n>絵文字といった感情表現を含めて考慮したら？\n>異常検知\n>人物のネットワーク分析\n\n### 参考文献\n[1.selenium Web element](https://www.selenium.dev/documentation/en/webdriver/web_element/)\n[2.pythonで絵文字を駆逐する](https://qiita.com/yoshimo123/items/85331d881aed9ad41020)\n[3.感情分析に用いる極性辞書を自動生成する](https://qiita.com/g-k/items/1b7c765fa6520297ca7c)\n[4.感情分析でニュース記事のネガポジ度合いをスコア化する](https://qiita.com/g-k/items/e49f68d7e2fed6e300ea)\n[5.Python实现输入电影名字自动生成豆瓣评论词云图（带GUI界面）小程序](https://blog.csdn.net/q_u_a_r_t_e_r/article/details/111286094)\n[6.fastTextの学習済みモデルを公開しました](https://qiita.com/Hironsan/items/513b9f93752ecee9e670)","tags":["自然言語処理","クロージング","感情分析","LSTM","GUI"],"categories":["自然言語処理"]},{"title":"04_bilibili三国志_中国ファンの自作動画の弾幕を取得してみた","url":"/Jorey-s-Blog/2021/01/05/04_bilibili/","content":"\n\n<iframe width=\"560\" height=\"315\" src=\"//player.bilibili.com/player.html?aid=50865908&bvid=BV12441147tV&cid=89046676&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n弾幕を閉じる方法：メッセージボックスのようなボダンを押せば弾幕を閉じれます。\n<div style=\"width:70%;margin:auto\">{% asset_img danmaku_close.jpg danmaku_close%}</div>\n\n最近新解釈・三国志上映されたので、上映初日観に行きました！\n福田監督の作品なので、三国時代の話を面白く語っていて、つかみ合いのシーンは個人的にとても良かったと思いました。また、城田優さんが演じた呂布はイメージ通りでしたが、岩田さんが演じた趙雲子龍のキャラはちょっと待ってって感じでした。笑\n\n同時期に中国のbilibili動画サイト（ニコニコの中国版サイト）で中国のファンが三国をテーマに、複数のドラマ、映画から素材を集まって自作した動画を見つけました。77万回も視聴され、弾幕コメントは2000本ほどありました。とても気に入った動画なので、弾幕コメントを取得してみました。\n<!-- more -->\n---------------\n<!-- toc -->\n---------------\n### 1.動画のCIDを取得する\n{% asset_img cid取得.png cid取得 %}\n\n**cid = 89046676**\n\n### 2.動画弾幕コメントのxmlにアクセス\n\n\tⅠ.https://comment.bilibili.com/ + {cid}.xml\n\thttps://comment.bilibili.com/89046676.xml　　\n\t\t→1000件コメントしか取得できない\n\t\t→場合によって8000件、500件だったるすて取得可能な件数が不安定らしいです。\n\n\tⅡ.全件弾幕コメントを取得するには、\n\thttps://api.bilibili.com/x/v2/dm/history?type=1&oid={cid}&date={date}　\n\t\t→本動画は2019-04-29からアップロードされたので、date = 2019-04-29 ~ 2021-01-05の弾幕を取得していきたいと思います。\n\n### 3.最新コメントを取得してみよう\n\n{% asset_img 1_xml.png 1_xml %}\n\n\trequestモジュールを使って取得してみた\n\n\timport requests\n    from bs4 import BeautifulSoup\n\n    url = 'https://comment.bilibili.com/89046676.xml'\n    request = requests.get(url)\n    request.text\n{% asset_img 1_encoding.png 1_encoding %}\n\n\t#一度文字コードを設定する\n\trequest.encoding='utf-8-sig'\n{% asset_img 1_encoded.png 1_encoded %}\n\n\t#取得してきたテキスト文書をBeautifulSoupで分析する\n\tsoup = BeautifulSoup(request.text,'lxml')\n\n\t#テキスト文書の中身を確認すると、弾幕コメントはすべて以下のフォーマットに格納していることがわかった\n\t#<d p=\"18.29700,1,25,16777215,1609775751,0,8d1aab76,43323792511467527\">テキスト</d>\n\n\t#したがって、<d>を抽出する\n\tresults = soup.find_all('d')\n{% asset_img d取得.png d取得 %}\n\n\t#コメント部分だけを取得\n\tcomments = [comment.text for comment in results]\n{% asset_img comment.png comment %}\n\n\tlen(comments)\n\t＃1000\n\t＃このやり方で1000件コメントしか取得できない。\n\n### 4.古いコメントを取得してみよう\n\n\t#bilibiliの古い弾幕コメントは1日分ごとにxmlに格納されている。上記リンクで日付を指定することで、該当日の弾幕コメントを取得できる。\n\t　しかし、こちらのxmlデータをアクセスするには、ログインする必要がある。\n\t　本記事はseleniumを使ってログインを行いたいと思う\n\n\tfrom bs4 import BeautifulSoup as bs\n\timport pandas as pd\n\tfrom selenium import webdriver\n\timport time, datetime\n\n\turl_login = 'https://passport.bilibili.com/login'\n\tdriver = webdriver.Chrome()\n\tdriver.get(url_login)\n\t#ここでログイン情報を入力↓↓↓\n{% asset_img login.png login %}\n\n\tcid = 89046676\n\tbegin = '2019-04-29'\n\tend = '2021-01-05'\n\tfmt = '%Y-%m-%d'\n\n\t＃ある期間中の日付を一日単位で取得する\n\tdef day_range(bgn,end):\n\t    fmt = '%Y-%m-%d'\n\t    begin=datetime.datetime.strptime(bgn,fmt)\n\t    end=datetime.datetime.strptime(end,fmt)\n\t    delta=datetime.timedelta(days=1)\n\t    interval=int((end-begin).days) +1\n\t   \n\t    return [begin+delta*i for i in range(0,interval,1)]\n\n\ttime_list = [m.strftime(fmt) for m in day_range(begin,end)]\n{% asset_img timelist.png timelist %}\n\n\t#対象となるすべてのxmlをリストに格納\n\turls = ['https://api.bilibili.com/x/v2/dm/history?type=1&oid='+str(cid)+'&date='+str(i) for i in time_list]\n{% asset_img urls.png urls %}\n\t\n\thtmls = []\n\tfor i in urls:\n\t    driver.get(i)\n\t    time.sleep(3)\n\t    #get_attribute('innerHTML'):seleniumでWebElementからhtmlを取得する\n\t    htmls.append(driver.find_element_by_tag_name('i').get_attribute('innerHTML'))\n\n\tsoups = [bs(h,'lxml') for h in htmls]\n\tresults = [soup.find_all('d') for soup in soups]\n{% asset_img results.png results %}\n\n\tresultss = sum(results,[])\n\tcomments = [comment.text for comment in resultss]\n{% asset_img comments.png comments %}\n\n\tlen(comments)\n\t#608931\n\n\t#重複削除\n\tcommentss = list(set(comments))\n\tlen(commentss)\n\t#1625 \n\n### 5.残る課題\n>1_弾幕コメントだけではなく、日付情報も取得したい\n>2_違うユーザーは同じ弾幕コメントを出すことが可能なので、ユーザー情報取得可能か\n>3_短い文書の解析\n>4_モジュールにしたい\n\n### 参考文献\n\n[[Python][Bilibili]B站历史弹幕爬虫](https://blog.csdn.net/sinat_18665801/article/details/104519838)","tags":["bilibili","自然言語処理","nlp","クロージング"],"categories":["自然言語処理"]},{"title":"2021年1月SQL練習課題","url":"/Jorey-s-Blog/2021/01/05/2021年1月SQL練習課題/","content":"<!-- toc -->\n---------------\n\n### 1.2021年1月12日\n\n<font size=3>以下のデータに対して、NAME列内種類ごとの和と各種類の総数の比率が0.6より大きいIDとNAMEを抽出せよ。</font>\n\n|ID|NAME|NUM|\n|-|-|-|\n|1|A|1|\n|2|A|2|\n|3|A|6|\n|4|A|4|\n|5|A|3|\n|6|B|2|\n|7|B|8|\n|8|B|2|\n<!-- more -->\n### 2.2021年1月13日\n<font size=3>テーブルT0113は以下の通りです。</font>\n\n|日付|コード|金額|金額相違値|\n|-|-|-|-|\n|2020-07-01|A|2.00|.00|\n|2020-07-06|A|4.00|2.00|\n|2020-07-12|A|3.00|-1.00|\n|2020-07-13|A|4.00|1.00|\n|2020-07-13|B|2.00|.00|\n|2020-07-19|B|4.00|2.00|\n\n<font size=3>同じコードでも日によって、金額違うので、その連続増長の日数と連続減少の日数を統計し、以下の結果を出してみよう</font>\n\n|日付|コード|金額|金額相違値|増長方向|連続増長日数｜\n|-|-|-|-|-|-|\n|2020-07-01|A|2.00|.00|正|1|\n|2020-07-06|A|4.00|2.00|正|2|\n|2020-07-12|A|3.00|-1.00|負|1|\n|2020-07-13|A|4.00|1.00|正|1|\n|2020-07-13|B|2.00|.00|正|1|\n|2020-07-19|B|4.00|2.00|正|2|\n\n### 3.2021年1月16日\n<font size=3>T0116というテーブルがあって、商品ごとに最新日の日平均単価を求めよう</font>\n\n|商品|日付|単価|個数|\n|-|-|-|-|\n|デスク|2016-01-01|5000|1|\n|イス|2016-01-01|3000|4|\n|デスク|2016-01-02|5000|2|\n|デスク|2016-01-01|5000|3|\n|イス|2016-01-02|3000|2|\n|デスク|2016-01-02|4800|1|\n\n### 4.2021年1月17日\n<font size=3>T0117というテーブルがあって、希望の形(右)に整おう</font>\n\n<table>\n<tr><th>T0117</th><th>集計後</th></tr>\n<tr><td>\n\n|X|L|\n|-|-|\n|1|A|\n|2|A| 　　\n|3|A|\n|4|A|\n|5|B|\n|6|B|\n|7|B|\n|8|A|\n|9|A|\n\n</td><td>\n\n|L|R|\n|-|-|\n|A|1-4|\n|A|8-9|\n|B|5-7|\n\n</td></tr> </table>\n\n### 5.2021年1月21日\n<font size=3>T0121というテーブルがあって、Amountは一人ひとりが同じ月に違うステータスしたの金額総和である。その総和を該当月の平均金額に換算しよう</font>\n\n|ID|NAME|MON|STATE|AMOUNT|\n|-|-|-|-|-|\n|1|Aさん|201901|A|9000|\n|2|Aさん|201901|B|9000|\n|3|Aさん|201901|E|9000|\n|4|Bさん|201902|A|1800|\n|5|Bさん|201902|C|1800|\n|6|Cさん|201902|C|30000|\n|7|Cさん|201902|F|30000|\n\n<font size=3>希望:</font>\n\n|ID|NAME|MON|STATE|AMOUNT|\n|-|-|-|-|-|\n|1|Aさん|201901|A|3000|\n|2|Aさん|201901|B|3000|\n|3|Aさん|201901|E|3000|\n|4|Bさん|201902|A|900|\n|5|Bさん|201902|C|900|\n|6|Cさん|201902|C|15000|\n|7|Cさん|201902|F|15000|\n|8|Aさん|201902|D|2000|\n\n### 6.2021年1月25日\n<font size=3>T0125というテーブルがあります。</font>\n\n|TransType|OprSeq|OpCode|TransTime|\n|-|-|-|-|\n|開始|10|NF21|2019-11-30 14:06|\n|シフト|10|NF21|2019-11-30 14:09|\n|開始|10|NF21|2019-11-30 14:10|\n|結束|10|NF21|2019-11-30 14:13|\n|開始|20|NF22|2019-11-30 14:15|\n|結束|20|NF22|2019-11-30 14:16|\n|開始|30|NF24|2019-11-30 14:17|\n|結束|30|NF24|2019-11-30 14:20|\n\n<font size=3>以下のように整理しましょう</font>\n\n|TransType|OprSeq|OpCode|StartTime|EndTime|\n|-|-|-|-|-|\n|開始-シフト|10|NF21|2019-11-30 14:06|2019-11-30 14:09|\n|開始ー結束|10|NF21|2019-11-30 14:10|2019-11-30 14:13|\n|開始ー結束|20|NF22|2019-11-30 14:15|2019-11-30 14:16|\n|開始ー結束|30|NF24|2019-11-30 14:17|2019-11-30 14:20|\n\n### 7.2021年1月26日\n<font size=3>T0126Aというテーブルがあります。薬AとBの配り状況を表しています。T0126Bは薬AとBの購買記録です。</font>\n\n<table>\n<tr><th>T0126A</th><th>T0126B</th></tr>\n<tr><td>\n\n|id|spmc|分配量|\n|-|-|-|\n|1|A|80.0000|\n|2|B|100.0000|\n\n</td><td>\n\n|id|spmc|購入量|\n|-|-|-|\n|1|A|20.0000|\n|2|A|50.0000|\n|3|A|40.0000|\n|4|A|30.0000|\n|5|B|120.0000|\n|6|B|80.0000|\n|7|B|100.0000|\n\n</td></tr> </table>\n<font size=3>上記2つのテーブルの情報をまとめて、下記のテーブルを出しましょう。</font>\n\n|spmc|購入量|今回使用量|在庫量|\n|-|-|-|-|\n|A|20.0000|20.0000|.0000|\n|A|50.0000|50.0000|.0000|\n|A|40.0000|10.0000|30.0000|\n|A|30.0000|.0000|30.0000|\n|B|120.0000|100.0000|20.0000|\n|B|80.0000|.0000|80.0000|\n|B|100.0000|.0000|100.0000|\n\n\n\n","tags":["sql"],"categories":["sql"]},{"title":"01_1_index.js","url":"/Jorey-s-Blog/2021/01/04/01_1_index/","content":"\n----------\n\t'use strict';\n\tvar cheerio = require('cheerio');\n\n\t// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string\n\tfunction getPosition(str, m, i) {\n\t  return str.split(m, i).join(m).length;\n\t}\n<!-- more -->\n\tvar version = String(hexo.version).split('.');\n\thexo.extend.filter.register('after_post_render', function(data){\n\t  var config = hexo.config;\n\t  if(config.post_asset_folder){\n\t    \tvar link = data.permalink;\n\t\tif(version.length > 0 && Number(version[0]) == 3)\n\t\t   var beginPos = getPosition(link, '/', 1) + 1;\n\t\telse\n\t\t   var beginPos = getPosition(link, '/', 3) + 1;\n\t\t// In hexo 3.1.1, the permalink of \"about\" page is like \".../about/index.html\".\n\t\tvar endPos = link.lastIndexOf('/') + 1;\n\t    link = link.substring(beginPos, endPos);\n\n\n\t    var toprocess = ['excerpt', 'more', 'content'];\n\t    for(var i = 0; i < toprocess.length; i++){\n\t      var key = toprocess[i];\n\t \n\t      var $ = cheerio.load(data[key], {\n\t        ignoreWhitespace: false,\n\t        xmlMode: false,\n\t        lowerCaseTags: false,\n\t        decodeEntities: false\n\t      });\n\n\t      $('img').each(function(){\n\t\t\tif ($(this).attr('src')){\n\t\t\t\t// For windows style path, we replace '\\' to '/'.\n\t\t\t\tvar src = $(this).attr('src').replace('\\\\', '/');\n\t\t\t\tif(!/http[s]*.*|\\/\\/.*/.test(src) &&\n\t\t\t\t   !/^\\s*\\//.test(src)) {\n\t\t\t\t  // For \"about\" page, the first part of \"src\" can't be removed.\n\t\t\t\t  // In addition, to support multi-level local directory.\n\t\t\t\t  var linkArray = link.split('/').filter(function(elem){\n\t\t\t\t\treturn elem != '';\n\t\t\t\t  });\n\t\t\t\t  var srcArray = src.split('/').filter(function(elem){\n\t\t\t\t\treturn elem != '' && elem != '.';\n\t\t\t\t  });\n\t\t\t\t  if(srcArray.length > 1)\n\t\t\t\t\tsrcArray.shift();\n\t\t\t\t  src = srcArray.join('/');\n\t\t\t\t  $(this).attr('src', config.root + link + src);\n\t\t\t\t  console.info&&console.info(\"update link as:-->\"+config.root + link + src);\n\t\t\t\t}\n\t\t\t}else{\n\t\t\t\tconsole.info&&console.info(\"no src attr, skipped...\");\n\t\t\t\tconsole.info&&console.info($(this));\n\t\t\t}\n\t      });\n\t      data[key] = $.html();\n\t    }\n\t  }\n\t});\n\n","tags":["hexo-asset-image"],"categories":["サイト構築"]},{"title":"01_Hexo上でサイトの構築について","url":"/Jorey-s-Blog/2021/01/03/01_Hexo上でサイトの構築について/","content":"<!-- toc -->\n### １.写真の挿入：\n\nHexoをgithubに搭載する場合、以下のやり方でうまくできました\n\n\t1.npm install https://github.com/CodeFalling/hexo-asset-image --save\n\t\t*このバージョンをインストール必要があります。\n\n\t2.以下の内容をコピーし、index.jsに保存する\n\t　>node_modules>hexo-asset-image>index.jsを交換する\n\n[index.js](http://localhost:4000/Jorey-s-Blog/2021/01/04/index/)\n<!-- more -->\n\t3.hexoの_config.ymlにpost_asset_folder: trueに変更する\\n\n\n{% asset_img post_asset_folder.png post_asset_folder %}\n\t\n\t4.cd hexo(Jorey-s-Blog)\n\t hexo new photo \n\t\t*新しいphoto.mdというファイルが作成される\n\t\t*そしてsource/_posts ファルダにphotoというファルダも生成される\n{% asset_img mdとフォルダ同時生成.png mdとフォルダ同時生成 %}\n\n\t5.{% asset_img 写真の名前.png 注釈(記入しなくてもいい) %}\n\t\t*githubに搭載する場合、この絶対パスの引用が効きます\n\t\t*![]()の記入方法だと、![](github上のパス)を記入する必要がある\n\n\t6.hexo clean && hexo g -dで更新する\n\n---\n### 2.Music Playerの挿入\n\nspotifyのPlaylistを挿入する\n\t\n\t1.npm install hexo-tag-spotify\n\n\t2.hexoの_config.ymlに以下の内容を追加\n\n\t\tplugins:\n\t\t - hexo-tag-spotify\n\t\tspotify:\n\t\t  size: 'large'\n\t\t  theme: 'light'\n\t\t  view: 'list'\n\n\t3.\n\tシングルソングの挿入：\n\t{% spotify spotify:track:0ZoOOxoA8o0lY590ivyM %}\n\tシングルソング、アルバム、playlistの挿入：\n\t{% spotify https://play.spotify.com/user/12132493/playlist/1kBmZZ1rW2Mq2jQdS4QclQ %}\n{% spotify spotify:playlist:37i9dQZF1EphGUoLq3zryN%}\n\n---\n### 3.文章の字数統計と文章を読むにはかかる時間を表示する\n[hexo-symbols-count-time](https://github.com/theme-next/hexo-symbols-count-time)\n- hexo-symbols-count-timeをインストール\n```\nnpm install hexo-symbols-count-time\n```\n- hexo_config.ymlの修正\n```\nsymbols_count_time:\n  symbols: true # 該当文章の字数統計量を表示する\n  time: true # 該当文章を読むにはかかる時間を表示する\n  total_symbols: true # すべての文章の字数統計量を表示する\n  total_time: true # すべて文章を読むにはかかる時間を表示する\n```\n- next_config.ymlの修正\n```\nsymbols_count_time:\n  separated_meta: true  # 改行の表示\n  item_text_post: true  # 文章内の表示\n  item_text_total: false # ブログのボトムの表示\n```\n\n\n","tags":["hexo","構築","エラー","html"],"categories":["サイト構築"]}]